# Earnings Call LLM â€” Demo Project (Custom Tokenizer, Mini GPT, RAG)

This repository is a **demo implementation** of building a tiny LLM from scratch, it is tiny because of my hardware specs, only run on CPU and low VRAM.  
Even though the compute is limited, this project still demonstrates the **core engineering steps required to build and run a real LLM pipeline**, including:

#### Custom tokenizer (SentencePiece BPE)  
#### Mini GPT-style language model  
#### End-to-end training loop (from scratch)  
#### RAG (Retrieval-Augmented Generation) over earnings call transcripts  
#### CLI tool for training, retrieval, and inference  

The goal:  
> **Learn all components of LLM development by building a completeâ€”though smallâ€”working system. It can work like other LLMs like ChatGPT, Gemini, ect., if can be trained through large data**

---

# ğŸ“‚ Project Structure

```
LLM-Earnings-Call-Analyst/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # CLI entrypoint
â”‚   â”œâ”€â”€ data_utils.py           # Data preprocessing utilities
â”‚   â”œâ”€â”€ tokenizer_train.py      # Tokenizer training script
â”‚   â”œâ”€â”€ tokenizer_utils.py      # Tokenizer helper functions
â”‚   â”œâ”€â”€ train_lm.py             # Language model training
â”‚   â”œâ”€â”€ model.py                # Model architecture
â”‚   â”œâ”€â”€ inference_demo.py       # Inference demonstration
â”‚   â”œâ”€â”€ rag_pipeline.py         # RAG implementation
â”‚   â””â”€â”€ config.py               # Configuration management
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Earnings transcripts (not uploaded)
â”‚   â””â”€â”€ processed/              # Cleaned text split into train/val
â”‚
â”œâ”€â”€ tokenizer/                  # SentencePiece tokenizer files
â”‚
â”œâ”€â”€ checkpoints/                # Model checkpoints (not uploaded)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_config.json       # Model configuration
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # Project documentation
```


The following directories intentionally contain **no raw data or heavy checkpoints** due to storage and laptop capacity constraints:
- `data/raw/`
- `checkpoints/`
- `tokenizer/`  
However, the complete **pipeline code is included**, so you can recreate everything.

---

# ğŸš€ What This Demo Can Do

Even in demo mode, the project lets you:

### **1. Build and train your own tokenizer**
- Byte-Pair Encoding (BPE)
- Adjustable vocab size
- Adjust epoch
### **2. Train a small GPT-like model**
- Embedding sizes from 128â€“512  
- Context size 128â€“256  
- 2â€“6 transformer blocks  
- CPU-friendly tiny model available

### **3. Run RAG (Retrieval Augmented Generation)**
- TF-IDF vectorizer  
- Retrieve top-K most relevant transcript chunks  
- Feed context into your mini-LLM for earnings call analysis  

### **4. Run inference and generate text**
- â€œExplain what NVIDIA said about data center revenueâ€  
- â€œSummarize Appleâ€™s Q3 callâ€  
- â€œPredict forward guidance sentimentâ€  

---

# ğŸ›  Installation


ğŸ“¦ Installation
===============

```bash
# Clone the repository
git clone https://github.com/<your-username>/LLM-Earnings-Call-Analyst.git
cd LLM-Earnings-Call-Analyst
```

```bash
# Create virtual environment
python -m venv .venv
```

```bash
# Activate environment (Windows)
.venv\Scripts\activate
```

```bash
# Activate environment (macOS/Linux)
source .venv/bin/activate
```

```bash
# Install dependencies
pip install -r requirements.txt
```

---

â–¶ï¸ Usage Guide (Copyâ€“Paste Ready)
==================================

Everything runs through:

```bash
python src/main.py <command>
```

---

1ï¸âƒ£ Build the cleaned corpus
----------------------------

```bash
python src/main.py build_corpus
```

---

2ï¸âƒ£ Train the tokenizer
-----------------------

```bash
python src/main.py train_tokenizer --vocab_size 16000
```

---

3ï¸âƒ£ Train a tiny language model
-------------------------------

```bash
python src/main.py train_lm --model_size tiny --epochs 1
```

---

4ï¸âƒ£ Build the RAG index
-----------------------

```bash
python src/main.py build_index
```

---

5ï¸âƒ£ Ask a question (RAG + LLM)
------------------------------

```bash
python src/main.py ask   --question "What did NVIDIA say about data center demand?"   --ckpt checkpoints/lm_tiny_best.pt
```

---

6ï¸âƒ£ Generate text from a prompt
------------------------------

```bash
python src/main.py generate   --prompt "NVIDIA expects"   --ckpt checkpoints/lm_tiny_best.pt
```

---

7ï¸âƒ£ Index a single transcript file
----------------------------------

```bash
python src/main.py index_single --file path/to/transcript.txt
```

---

8ï¸âƒ£ Full Pipeline (Run Everything)
----------------------------------

```bash
python src/main.py build_corpus
python src/main.py train_tokenizer --vocab_size 16000
python src/main.py train_lm --model_size tiny --epochs 10
python src/main.py build_index
python src/main.py ask --question "What did META say about forward guidance?"
```


