[
  {
    "text": "Second Quarter 2025 Results Conference Call\nJuly 30th, 2025\nKenneth Dorell, Director, Investor Relations\nThank you. Good afternoon and welcome to Meta’s second quarter 2025 earnings conference call.\nJoining me today are Mark Zuckerberg, CEO and Susan Li, CFO.\nOur remarks today will include forward‐looking statements, which are based on assumptions as of\ntoday. Actual results may differ materially as a result of various factors including those set forth in\ntoday’s earnings press release, and in our quarterly report on Form 10-Q filed with the SEC. We\nundertake no obligation to update any forward-looking statement.\nDuring this call we will present both GAAP and certain non‐GAAP financial measures. A\nreconciliation of GAAP to non‐GAAP measures is included in today’s earnings press release. The\nearnings press release and an accompanying investor presentation are available on our website at\ninvestor.atmeta.com.\nAnd now, I’d like to turn the call over to Mark.\nMark Zuckerberg, CEO\nThanks Ken -- thanks everyone for joining today.\nWe had another strong quarter with more than 3.4 billion people using at least one of our apps\neach day -- and strong engagement across the board. Our business continues to perform very\nwell, which enables us to invest heavily in our AI efforts.\nOver the last few months we have begun to see glimpses of our AI systems improving themselves.\nThe improvement is slow for now, but undeniable.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 0
  },
  {
    "text": "l, which enables us to invest heavily in our AI efforts.\nOver the last few months we have begun to see glimpses of our AI systems improving themselves.\nThe improvement is slow for now, but undeniable.\n\nDeveloping superintelligence -- which we define\nas AI that surpasses human intelligence in every way -- we think is now in sight.\nMeta's vision is to bring personal superintelligence to everyone -- so that people can direct it\ntowards what they value in their own lives. We believe this has the potential to begin an exciting\nnew era of individual empowerment.\nA lot has been written about the economic and scientific advances that superintelligence can\nbring. I am extremely optimistic about this. But I think that if history is a guide, then an even more\nimportant role will be how superintelligence empowers people to be more creative, develop\nculture and communities, connect with each other, and lead more fulfilling lives.\nTo build this future, we've established Meta Superintelligence Labs, which includes our\nfoundations, product, and FAIR teams, as well as a new lab that is focused on developing the next\ngeneration of our models. We're making good progress towards Llama 4.1 and 4.2 -- and in\nparallel, we're also working on our next generation of models that will push the frontier in the next\nyear or so.\nWe're building an elite, talent-dense team. Alexandr Wang is leading the overall team, Nat\nFriedman is leading our AI products and applied research, and Shengjia Zhao is Chief Scientist for\nthe new effort.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 1
  },
  {
    "text": " building an elite, talent-dense team. Alexandr Wang is leading the overall team, Nat\nFriedman is leading our AI products and applied research, and Shengjia Zhao is Chief Scientist for\nthe new effort.\n\nThey're all incredibly talented leaders and I'm excited to work closely with them\nand the world-class group of AI researchers, and infrastructure and data engineers that we're\nassembling.\nI've spent a lot of time building this team this quarter, and the reason that so many people are\nexcited to join is because Meta has all the ingredients required to build leading models and deliver\nthem to billions of people. The people who are joining us will have access to unparalleled compute\nas we build out several multi-GW clusters. Our Prometheus cluster is coming online next year and\nwe think it'll be the world's first 1GW+ cluster. We're also building out Hyperion, which will be able\nto scale up to 5GW over several years. And we have multiple more titan clusters in development as\nwell.\nWe're making all these investments because we have conviction that superintelligence is going to\nimprove every aspect of what we do.\nFrom a business perspective, I mentioned last quarter that there are five basic opportunities that\nwe're pursuing: improved advertising, more engaging experiences, business messaging, Meta AI,\nand AI devices. So I can go into a bit of detail on each.\nOn advertising, the strong performance this quarter is largely thanks to AI unlocking greater\nefficiency and gains across our ads system. This quarter, we expanded our new AI-powered\nrecommendation model for ads to new surfaces and improved its performance by using more\nsignals and a longer context.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 2
  },
  {
    "text": "ency and gains across our ads system. This quarter, we expanded our new AI-powered\nrecommendation model for ads to new surfaces and improved its performance by using more\nsignals and a longer context.\n\nIt's driven roughly 5% more ad conversions on Instagram and 3% on\nFacebook.\nWe're also seeing good progress with AI for ad creative -- with a meaningful percent of our ad\nrevenue now coming from campaigns using one of our Generative AI features. This is going to be\nespecially valuable for smaller advertisers with limited budgets, while agencies will continue the\nimportant work to help larger brands apply these tools strategically.\nThe second opportunity is more engaging experiences. AI is significantly improving our ability to\nshow people content that they’re going to find interesting and useful. Advancements in our\nrecommendation systems have improved quality so much that it has led to a 5% increase in time\nspent on Facebook and 6% on Instagram just this quarter.\nThere's a lot of potential for content itself to get better too. We're seeing early progress with the\nlaunch of our AI video editing tools across Meta AI and our new Edits app, and there's a lot more to\ndo here.\nThe third opportunity is business messaging. I've talked before about how I believe every business\nwill soon have a business AI just like they have an email address, social media account, and\nwebsite. We're starting to see some product market fit in a number of countries where we're\ntesting these agents, and we're integrating these business AIs into ads on Facebook and\nInstagram, as well as directly into e-commerce websites.\nThe fourth opportunity is Meta AI.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 3
  },
  {
    "text": "ountries where we're\ntesting these agents, and we're integrating these business AIs into ads on Facebook and\nInstagram, as well as directly into e-commerce websites.\nThe fourth opportunity is Meta AI.\n\nIts reach is already quite impressive with more than a billion\nmonthly actives. Our focus is now deepening the experience and making Meta AI the leading\npersonal AI. As we continue improving our models we see engagement grow, so our next\ngeneration of models is going to continue to really help here.\nThe fifth opportunity is AI devices. We continue to see strong momentum with our Ray-Ban Meta\nglasses, with sales accelerating. We're also launching new performance AI glasses with the Oakley\nMeta HSTNs. They have longer battery life, higher resolution camera, and are designed for sports.\nThe percent of people using Meta AI is growing and we're seeing new users' AI retention increase\ntoo, which is a good sign for that continued use. I think that AI glasses are going to be the main\nway that we integrate superintelligence into our day-to-day lives, so it's important to have all\nthese different styles that appeal to different people in different settings.\nFinally, we’re seeing people continue to spend more time with our Quest ecosystem and the\ncommunity continues to grow steadily. We launched the Meta Quest 3S Xbox Edition last month,\nand we're seeing record interest in cloud gaming.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 4
  },
  {
    "text": "tinue to spend more time with our Quest ecosystem and the\ncommunity continues to grow steadily. We launched the Meta Quest 3S Xbox Edition last month,\nand we're seeing record interest in cloud gaming.\n\nAnd beyond gaming, we continue to see a\nbroader set of use cases with media and web-browsing contributing a significant portion of\nengagement.\nWe’re going to have more to share on all of this, especially our Reality Labs work, at Connect on\nSeptember 17th, so I encourage you to tune into that.\nOverall, this has been a busy quarter. Strong business performance and real momentum in\nassembling both the talent and the compute needed to build personal superintelligence for\neveryone. I am very grateful for our teams who are working hard to deliver this, and thanks to all of\nyou for being on this journey with us. And now, here's Susan.\nSusan Li, CFO\nThanks Mark and good afternoon everyone.\nLet’s begin with our consolidated results.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 5
  },
  {
    "text": "rd to deliver this, and thanks to all of\nyou for being on this journey with us. And now, here's Susan.\nSusan Li, CFO\nThanks Mark and good afternoon everyone.\nLet’s begin with our consolidated results.\n\nAll comparisons are on a year-over-year basis unless\notherwise noted.\nQ2 total revenue was $47.5 billion, up 22% on both a reported and constant currency basis.\nQ2 total expenses were $27.1 billion, up 12% compared to last year.\nIn terms of the specific line items:\nCost of revenue increased 16%, driven mostly by higher infrastructure costs and payments to\npartners, partially offset by a benefit from the previously announced extension of server useful\nlives.\nR&D increased 23%, mostly due to higher employee compensation and infrastructure costs.\nMarketing & Sales increased 9%, primarily due to an increase in professional services related to\nour ongoing platform integrity efforts as well as marketing costs, partially offset by lower\nemployee compensation.\nG&A decreased 27%, driven mostly by lower legal-related costs.\nWe ended Q2 with over 75,900 employees, down 1% quarter-over-quarter as the vast majority of\nthe employees impacted by performance-related reductions earlier this year were no longer\ncaptured in our headcount.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 6
  },
  {
    "text": "Q2 with over 75,900 employees, down 1% quarter-over-quarter as the vast majority of\nthe employees impacted by performance-related reductions earlier this year were no longer\ncaptured in our headcount.\n\nThis was partially offset by continued hiring in priority areas of\nmonetization, infrastructure, Reality Labs, AI, as well as regulation and compliance.\nSecond quarter operating income was $20.4 billion, representing a 43% operating margin.\nOur tax rate for the quarter was 11%, which reflects excess tax benefits from share based\ncompensation due to the increase in our share price versus prior periods.\nNet income was $18.3 billion or $7.14 per share.\nCapital expenditures, including principal payments on finance leases, were $17.0 billion, driven by\ninvestments in servers, data centers and network infrastructure.\nFree cash flow was $8.5 billion. We repurchased $9.8 billion of our Class A common stock and paid\n$1.3 billion in dividends to shareholders. We also made $15.1 billion in non-marketable equity\ninvestments in the second quarter, which includes our minority investment in Scale AI along with\nother investment activities.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 7
  },
  {
    "text": "dends to shareholders. We also made $15.1 billion in non-marketable equity\ninvestments in the second quarter, which includes our minority investment in Scale AI along with\nother investment activities.\n\nWe ended the quarter with $47.1 billion in cash and marketable\nsecurities and $28.8 billion in debt.\nMoving now to our segment results.\nI’ll begin with our Family of Apps segment.\nOur community across the Family of Apps continues to grow, and we estimate more than 3.4\nbillion people used at least one of our Family of Apps on a daily basis in June.\nQ2 Total Family of Apps revenue was $47.1 billion, up 22% year-over-year.\nQ2 Family of Apps ad revenue was $46.6 billion, up 21% or 22% on a constant currency basis.\nWithin ad revenue, the online commerce vertical was the largest contributor to year-over-year\ngrowth.\nOn a user geography basis, ad revenue growth was strongest in Europe and Rest of World at 24%\nand 23%, respectively. North America and Asia-Pacific grew 21% and 18%.\nIn Q2, the total number of ad impressions served across our services increased 11%, with growth\nmainly driven by Asia-Pacific. Impression growth accelerated across all regions due primarily to\nengagement tailwinds on both Facebook and Instagram and, to a lesser extent, ad load\noptimizations on Facebook. The average price per ad increased 9%, benefiting from increased\nadvertiser demand, largely driven by improved ad performance.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 8
  },
  {
    "text": "and Instagram and, to a lesser extent, ad load\noptimizations on Facebook. The average price per ad increased 9%, benefiting from increased\nadvertiser demand, largely driven by improved ad performance.\n\nPricing growth slowed modestly\nfrom the first quarter due to the accelerated impression growth in Q2.\nFamily of Apps other revenue was $583 million, up 50%, driven by WhatsApp paid messaging\nrevenue growth as well as Meta Verified subscriptions.\nWe continue to direct the majority of our investments toward the development and operation of\nour Family of Apps. In Q2, Family of Apps expenses were $22.2 billion, representing 82% of our\noverall expenses. Family of Apps expenses were up 14%, mainly due to growth in employee\ncompensation and infrastructure costs, partially offset by lower legal-related costs.\nFamily of Apps operating income was $25.0 billion, representing a 53% operating margin.\nWithin our Reality Labs segment, Q2 revenue was $370 million, up 5% year-over-year due to\nincreased sales of AI glasses, partially offset by lower Quest sales.\nReality Labs expenses were $4.9 billion, up 1% year-over-year driven by higher non-headcount\nrelated technology development costs.\nReality Labs operating loss was $4.5 billion.\nTurning now to the business outlook.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 9
  },
  {
    "text": "expenses were $4.9 billion, up 1% year-over-year driven by higher non-headcount\nrelated technology development costs.\nReality Labs operating loss was $4.5 billion.\nTurning now to the business outlook.\n\nThere are two primary factors that drive our revenue\nperformance: our ability to deliver engaging experiences for our community, and our effectiveness\nat monetizing that engagement over time.\nOn the first, daily actives continue to grow across Facebook, Instagram and WhatsApp as we\nmake additional improvements to our recommendation systems and product experiences.\nWe continue to see momentum with video engagement in particular. In Q2, Instagram video time\nwas up more than 20% year-over-year globally. We’re seeing strong traction on Facebook as well,\nparticularly in the US where video time spent similarly expanded more than 20% year-over-year.\nThese gains have been enabled by ongoing optimizations to our ranking systems to better identify\nthe most relevant content to show.\nWe expect to deliver additional improvements throughout the year as we further scale up our\nmodels and make recommendations more adaptive to a person’s interests within their session.\nAnother emphasis of our recommendations work is promoting original content. On Instagram,\nover two-thirds of recommended content in the US now comes from original posts. In the second\nhalf, we’ll be focused on further increasing the freshness of original posts so the right audiences\ncan discover original content from creators soon after it is posted.\nWe are also making good progress on our longer-term ranking innovations that we expect will\nprovide the next leg of improvements over the coming years.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 10
  },
  {
    "text": "l content from creators soon after it is posted.\nWe are also making good progress on our longer-term ranking innovations that we expect will\nprovide the next leg of improvements over the coming years.\n\nOur research efforts to develop\ncross-surface foundation recommendation models continue to progress. We are also seeing\npromising results from using LLMs in Threads recommendation systems. The incorporation of\nLLMs are now driving a meaningful share of the ranking-related time spent gains on Threads.\nWe’re now exploring how to extend the use of LLMs in recommendation systems to our other\napps. We’re leveraging Llama in several other back-end processes as well, including actioning bug\nreports so we can identify and resolve recurring issues more quickly and efficiently. This has\nresulted in top-line bug reports in the US & Canada in Facebook Feed and Notifications dropping\nby roughly 30% over the past 10 months.\nThe primary way we’re using Llama in our apps today is to power Meta AI, which is now available\nin over 200 countries and territories. WhatsApp continues to be the largest driver of queries as\npeople message Meta AI directly for tasks such as information gathering, homework assistance,\nand generating images. Outside of WhatsApp, we’re seeing Meta AI become an increasingly\nvaluable complement to our content discovery engines. Meta AI usage on Facebook is expanding\nas people use it to ask about posts they see in Feed and find content across our platform in\nSearch. Another way we expect Meta AI will help with content discovery is through the automatic\ntranslation and dubbing of foreign-language content into the audience’s local language.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 11
  },
  {
    "text": " our platform in\nSearch. Another way we expect Meta AI will help with content discovery is through the automatic\ntranslation and dubbing of foreign-language content into the audience’s local language.\n\nWe'll have\nmore to share on our efforts there later this year.\nMoving to Reality Labs. The growth of Ray-Ban Meta sales accelerated in Q2, with demand still\noutstripping supply for the most popular SKUs despite increases to our production earlier this\nyear. We’re working to ramp supply to better meet consumer demand later this year.\nNow to the second driver of our revenue performance: increasing monetization efficiency.\nThe first part of this work is optimizing the level of ads within organic engagement.\nWe continue to optimize ad supply across each surface to better deliver ads at the time and place\nthey are most relevant to people. In Q2, we also began introducing ads within Feed on Threads\nand the Updates tab on WhatsApp, which is a separate space away from people’s chats.\nAs of May, advertisers globally can now run video and image ads to Threads users in most\ncountries, including the United States. While ad supply remains low and Threads is not expected to\nbe a meaningful contributor to overall impression growth in the near-term, we are optimistic\nabout the longer-term opportunity with Threads as the community and engagement grow and\nmonetization scales.\nOn WhatsApp, we are rolling out ads in Status and Channels, along with Channel Subscriptions in\nthe Updates tab to help businesses reach the more than 1.5 billion daily actives who visit that part\nof the app.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 12
  },
  {
    "text": "App, we are rolling out ads in Status and Channels, along with Channel Subscriptions in\nthe Updates tab to help businesses reach the more than 1.5 billion daily actives who visit that part\nof the app.\n\nWe expect the introduction of ads in Status will be gradual over the course of this year\nand next, with low levels of expected ad supply initially. We also expect WhatsApp ads in Status to\nearn a lower average price than Facebook or Instagram ads for the foreseeable future, due in part\nto WhatsApp’s skew toward lower monetizing markets and more limited information that can be\nused for targeting. Given this, we do not expect ads in Status to be a meaningful contributor to\ntotal impressions or revenue growth for the next few years.\nThe second part of increasing monetization efficiency is improving marketing performance. There\nare three areas of this work that I’ll focus on today: improving our ads systems, advancing our ads\nproducts, including by building tools that assist in ads creation, and evolving our ads platform to\ndrive results that are optimized for each business’ objectives.\nFirst is our ads systems, where we’re innovating in both the ads retrieval and ranking stages to\nserve more relevant ads to people. A lot of this work involves us continuing to advance the\nmodeling innovations we’ve introduced previously while expanding their adoption across our\nplatform.\nThe Andromeda model architecture we began introducing in the second half of 2024 powers the\nads retrieval stage of our ads system, where we select the few thousand most relevant ads from\ntens of millions of potential candidates.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 13
  },
  {
    "text": "ture we began introducing in the second half of 2024 powers the\nads retrieval stage of our ads system, where we select the few thousand most relevant ads from\ntens of millions of potential candidates.\n\nIn Q2, we made enhancements to Andromeda that\nenabled it to select more relevant and more personalized ads candidates, while also expanding\ncoverage to Facebook Reels. These improvements have driven nearly 4% higher conversions on\nFacebook mobile Feed and Reels.\nOur new Generative Ads Recommendation System, or GEM, powers the ranking stage of our ads\nsystem, which is the part of the process after ads retrieval where we determine which ads to show\nsomeone from candidates suggested by our retrieval engine. In Q2, we improved the performance\nof GEM by further scaling our training capacity and adding organic and ads engagement data on\nInstagram. We also incorporated new advanced sequence modeling techniques that helped us\ndouble the length of event sequences we use, enabling our systems to consider a longer history of\nthe content or ads that a person has engaged with in order to provide better ad selections. The\ncombination of these improvements increased ad conversions by approximately 5% on Instagram\nand 3% on Facebook Feed and Reels in Q2.\nFinally, we expanded coverage of our Lattice model architecture in Q2. We first began deploying\nLattice in 2023 with our later stage ads ranking efforts, allowing us to run significantly larger\nmodels that generalize learnings across objectives and surfaces in place of numerous, smaller ads\nmodels that have historically been optimized for individual objectives and surfaces.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 14
  },
  {
    "text": "nificantly larger\nmodels that generalize learnings across objectives and surfaces in place of numerous, smaller ads\nmodels that have historically been optimized for individual objectives and surfaces.\n\nIn April, we\nbegan deploying Lattice to earlier stage ads ranking models as well. This is leading not only to\ngreater capacity and engineering efficiency, but also improved performance, with the recent\nLattice deployments driving a nearly 4% increase in ad conversions across Facebook Feed and\nReels in Q2.\nNext, ads products. Here, we’re seeing strong momentum with our Advantage+ suite of AI\npowered solutions.\nIn Q2, we completed the roll out of our streamlined campaign creation flow for Advantage+ sales\nand app campaigns, which makes it easier for advertisers to realize the performance benefits from\nAdvantage+ by having it turned on at the beginning. We’ve seen lifts in advertiser adoption of\nSales and App campaigns since we’ve expanded availability and are working to complete the\nrollout for leads campaigns in the coming months.\nWithin our Advantage+ creative suite, adoption of gen AI ad creative tools continues to broaden.\nNearly 2 million advertisers are now using our video generation features - Image Animation and\nVideo Expansion, and we’re seeing strong results with our text generation tools as we continue to\nadd new features. In Q2, we started testing AI-powered translations so that advertisers can\nautomatically translate the caption of their ads to 10 different languages.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 15
  },
  {
    "text": "neration tools as we continue to\nadd new features. In Q2, we started testing AI-powered translations so that advertisers can\nautomatically translate the caption of their ads to 10 different languages.\n\nWhile it’s early, we’ve\nseen promising performance lifts in our pre-launch tests.\nWe are also continuing to see strong adoption of Image Expansion among small and medium-sized\nadvertisers, which speaks to how these tools help businesses who have fewer resources to\ndevelop creative. With larger advertisers, we expect agencies will continue to be valuable partners\nin helping apply these new tools to drive performance.\nOutside of Advantage+, we’re seeing good momentum in business messaging, particularly in the\nUS where click-to-message revenue grew more than 40% year-over-year in Q2. The strong US\ngrowth is benefiting from a ramp in adoption of our Website to Message ads, which drive people\nto a businesses’ website for more information before choosing to launch a chat with the business\nin one of our messaging apps.\nFinally, we continue to evolve our ads platform to drive results that are optimized for each\nbusiness’ objectives and the way they measure results.\nIn Q2, we completed the global roll out of our incremental attribution feature, which is the only\nproduct on the market that optimizes for and reports on incremental conversions, which are\nconversions that would not have happened without a person seeing the ad.\nWe also launched Omnichannel ads globally in Q2, which enable advertisers to optimize for\nincremental sales both in-store and online with just one campaign.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 16
  },
  {
    "text": "ve happened without a person seeing the ad.\nWe also launched Omnichannel ads globally in Q2, which enable advertisers to optimize for\nincremental sales both in-store and online with just one campaign.\n\nIn tests, Advertisers using\nOmnichannel ads have seen a median 15% reduction in total Cost Per Purchase compared to\nwebsite-only optimization.\nNext, I would like to discuss our approach to capital allocation. Our primary focus remains\ninvesting capital back into the business, with infrastructure and talent being our top priorities.\nI’ll start with hiring. Our approach to adding headcount continues to be targeted at the company’s\nhighest priority areas. We expect talent additions across all of our priority areas will continue to\ndrive overall headcount growth through this year and 2026, while headcount growth in our other\nfunctions remains constrained. Within AI, we’ve had a particular emphasis on recruiting leading\ntalent within the industry as we build out Meta Superintelligence Labs to accelerate our AI model\ndevelopment and product initiatives.\nNext, infrastructure. We expect having sufficient compute capacity will be central to realizing\nmany of the largest opportunities in front of us over the coming years. We continue to see very\ncompelling returns from our AI capacity investments in our core ads and organic engagement\ninitiatives, and expect to continue investing significantly there in 2026. We also expect that\ndeveloping leading AI infrastructure will be a core advantage in developing the best AI models and\nproduct experiences, so we expect to ramp our investments significantly in 2026 to support that\nwork.\nMoving to our financial outlook.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 17
  },
  {
    "text": "ure will be a core advantage in developing the best AI models and\nproduct experiences, so we expect to ramp our investments significantly in 2026 to support that\nwork.\nMoving to our financial outlook.\n\nWe expect third quarter 2025 total revenue to be in the range of\n$47.5-50.5 billion. Our guidance assumes foreign currency is an approximately 1% tailwind to\nyear-over-year total revenue growth, based on current exchange rates. While we are not providing\nan outlook for fourth quarter revenue, we would expect our year-over-year growth rate in the\nfourth quarter of 2025 to be slower than the third quarter as we lap a period of stronger growth in\nthe fourth quarter of 2024.\nTurning now to the expense outlook.\nWe expect full year 2025 total expenses to be in the range of $114-118 billion, narrowed from our\nprior outlook of $113-118 billion and reflecting a growth rate of 20-24% year-over-year.\nWhile we’re still very early in planning for next year, there are a few factors we expect will provide\nmeaningful upward pressure on our 2026 total expense growth rate. The largest single driver of\ngrowth will be infrastructure costs, driven by a sharp acceleration in depreciation expense growth\nand higher operating costs as we continue to scale up our infrastructure fleet. Aside from\ninfrastructure, we expect the second largest driver of growth to be employee compensation as we\nadd technical talent in priority areas and recognize a full year of compensation expenses for\nemployees hired throughout 2025. We expect these factors will result in a 2026 year-over-year\nexpense growth rate that is above the 2025 expense growth rate.\nTurning now to the capex outlook.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 18
  },
  {
    "text": "nses for\nemployees hired throughout 2025. We expect these factors will result in a 2026 year-over-year\nexpense growth rate that is above the 2025 expense growth rate.\nTurning now to the capex outlook.\n\nWe currently expect 2025 capital expenditures, including\nprincipal payments on finance leases, to be in the range of $66-72 billion, narrowed from our prior\noutlook of $64-72 billion and up approximately $30 billion year-over-year at the mid-point. While\nthe infrastructure planning process remains highly dynamic, we currently expect another year of\nsimilarly significant capex dollar growth in 2026 as we continue aggressively pursuing\nopportunities to bring additional capacity online to meet the needs of our AI efforts and business\noperations.\nOn to tax. With the enactment of the new U.S. tax law, we anticipate a reduction in our U.S. federal\ncash tax for the remainder of the current year and future years. There are several alternative ways\nof implementing the provisions of the Act, which we are currently evaluating. While we estimate\nthat the 2025 tax rate will be higher than our Q2 tax rate, we cannot quantify the magnitude at\nthis time.\nIn addition, we continue to monitor an active regulatory landscape, including the increasing legal\nand regulatory headwinds in the EU that could significantly impact our business and our financial\nresults. For example, we continue to engage with the European Commission on our Less\nPersonalized Ads offering, or LPA, which we introduced in November 2024 based on feedback\nfrom the European Commission in connection with the DMA.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 19
  },
  {
    "text": "ue to engage with the European Commission on our Less\nPersonalized Ads offering, or LPA, which we introduced in November 2024 based on feedback\nfrom the European Commission in connection with the DMA.\n\nAs the Commission provides further\nfeedback on LPA, we cannot rule out that it may seek to impose further modifications to it that\nwould result in a materially worse user and advertiser experience. This could have a significant\nnegative impact on our European revenue, as early as later this quarter. We have appealed the\nEuropean Commission’s DMA decision but any modifications to our model may be imposed during\nthe appeal process.\nIn closing, this was another strong quarter for our business as our investments in infrastructure\nand technical talent continue to improve core ads performance and engagement on our platforms.\nWe expect the significant investments we’re making now will allow us to continue leveraging\nadvances in AI to extend those gains and unlock a new set of opportunities in the years to come.\nWith that, Krista, let’s open up the call for questions.\nOperator: Thank you. We will now open the lines for a question and answer session. To\nask a question, please press star one on your touchtone phone. To withdraw\nyour question, again press star one.\nPlease limit yourself to one question. Please pick up your handset before\nasking your question to ensure clarity. If you are streaming today’s call, please\nmute your computer speakers. And your first question comes from the line of\nEric Sheridan with Goldman Sachs. Please go ahead.\nEric Sheridan: Thanks so much for taking the questions.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 20
  },
  {
    "text": "ay’s call, please\nmute your computer speakers. And your first question comes from the line of\nEric Sheridan with Goldman Sachs. Please go ahead.\nEric Sheridan: Thanks so much for taking the questions.\n\nMark, when you think about where\nthe AI parts of your business have been evolving over the last three to six\nmonths, I wanted to know what your key learnings were as you went deep into\nthat strategy that inform some of the shifts in both talent, acquisition and\ncompute.\nCoupled with some of the blogs you put out recently in terms of how that\nstrategy might have evolved based on those key learnings. And Susan, building\non Mark’s comments on scaling talent and compute, I wanted to know if you\ncould go a little bit deeper in how we should be thinking about those two\ncomponents driving some of the commentary you’ve given around OpEx and\nCapEx over the next 12 to 18 months. Thanks so much.\nMark Zuckerberg: Yes. Sure. I can start. At a high level, I think that there are all these questions\nthat people have about what are going to be the timelines to get to really\nstrong AI or Superintelligence or whatever you want to call it.\nAnd I guess at each step along the way so far, we’ve observed the more kind of\naggressive assumptions or the fastest assumptions have been the ones that\nhave most accurately predicted what would happen. And I think that, that just\ncontinued to happen over the course of this year, too.\nAnd -- so I’ve given a number of those anecdotes on these earnings calls in the\npast.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 21
  },
  {
    "text": "ly predicted what would happen. And I think that, that just\ncontinued to happen over the course of this year, too.\nAnd -- so I’ve given a number of those anecdotes on these earnings calls in the\npast.\n\nAnd I think, certainly, some of the work that we’re seeing with teams\ninternally being able to adapt Llama 4 to build autonomous AI agents that can\nhelp improve the Facebook algorithm to increase quality and engagement, or\nlike.\nI mean that’s like a fairly profound thing if you think about it. I mean it’s\nhappening in low volume right now. So I’m not sure that, that result by itself\nwas a major contributor to this quarter’s earnings or anything like that.\nBut I think the trajectory on this stuff is very optimistic. And I think it’s one of\nthe interesting challenges in running a business like this now is there’s just a\nvery high chance, it seems, like the world is going to look pretty different in a\nfew years from now. And on the one hand, there are all these things that we can\ndo, there are improvements to our core products that exist.\nAnd then I think we have this principle that we believe in across the company,\nwhich we tell people, take Superintelligence seriously. And the basic principle is\nthis idea that we think that this is going to really shape all of our systems\nsooner rather than later, not necessarily on the trajectory of a quarter or two,\nbut on the trajectory of a few years.\nAnd I think that, that’s just going to change a lot of the assumptions around\nhow different things work across the company.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 22
  },
  {
    "text": "on the trajectory of a quarter or two,\nbut on the trajectory of a few years.\nAnd I think that, that’s just going to change a lot of the assumptions around\nhow different things work across the company.\n\nSo anyway, I think it’s basically\njust we’re continually observing how this works and what the trajectory or the\npace of AI progress has been.\nI think it continues to be on the faster end. And that I think informs a lot of the\ndecisions from everything from the importance and value of having the\nabsolute best and most elite talent dense team at the company to making sure\nthat we have a leading compute fleet so that the people here can do – so that\nthe researchers here have more compute per person to be able to leave their\nresearch and then roll it out to billions of people across our products, making\nsure that we build and drive these products through all the different things that\nwe do.\nWhich I think is one of the things that our company is the best in the world at is\nbasically when we take a technology, we’re good at driving that through all of\nour apps and our ad systems and all that stuff, it’s not just going to kind of sit\non the vine.\nI think that there’s no other company, I think that is as good as us at kind of\ntaking something and kind of getting it in front of billions of people. So yes, I\nmean we’re just going to push very aggressively on all of that.\nBut at some level, yes, this is -- there’s sort of a bet and a trajectory that we’re\nseeing and those are the signals that we’re seeing. But we’re just trying to read\nit.\nSusan Li: Eric, for the second part of your question, we haven’t, in fact, kicked off our\nbudgeting process for 2026.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 23
  },
  {
    "text": "ing and those are the signals that we’re seeing. But we’re just trying to read\nit.\nSusan Li: Eric, for the second part of your question, we haven’t, in fact, kicked off our\nbudgeting process for 2026.\n\nSo thinking about next year, there are clearly\nmany, many moving pieces in a very dynamic operating environment.\nBut there are certain aspects that we have some visibility into today including\nthe rough shape of our 2026 infrastructure plans. And that flows through into\nour expense expectations next year. And we also have some visibility into the\ncompensation expense growth that we’ll recognize from the AI talent that\nwe’re hiring this year.\nAnd so those two things are part of why we gave a little bit of an early preview\ninto the expectations for growth for 2026 total expenses as well as for 2026\nCapEx.\nSo on the total expenses side, as I mentioned, we expect infrastructure will be\nthe single largest contributor to 2026 expense growth. That’s driven primarily\nby a sharp acceleration in depreciation expense growth in 2026, largely driven\nby recognizing incremental depreciation from assets that we purchased and\nplaced in service in ‘26 as well as from infrastructure deployed through 2025\nthat we’ll recognize a full year of depreciation next year.\nWe also expect a greater mix of our CapEx to be in shorter-lived assets in 2025\nand ‘26 than it has been in prior years.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 24
  },
  {
    "text": "ployed through 2025\nthat we’ll recognize a full year of depreciation next year.\nWe also expect a greater mix of our CapEx to be in shorter-lived assets in 2025\nand ‘26 than it has been in prior years.\n\nAnd then the other component of infra\ncost growth next year would come from higher operating expenses including\nenergy costs, leases, maintenance and operational expenses that are\nassociated with maintaining that fleet.\nAnd we also expect some increased spend on cloud services in ‘26 to meet our\ncapacity needs as well as growth in network-related costs.\nSo a lot going on, on the infrastructure side as it contributes to the 2026 total\nexpense number. After that, employee compensation is the next largest driver\nof expense growth in ’26, again, driven primarily in the investments that we’re\nmaking in technical talent including recognizing a full year of compensation\nexpense for the AI talent we hire this year.\nI realize this answer is getting a little long, so I’ll try to wrap up quickly. On the\nCapEx side, the big driver of our increased CapEx in ‘26 will be scaling GenAI\ncapacity as we build out training capacity that’s going to drive higher spend\nacross servers, networking, data centers next year.\nWe also expect that we’re going to continue investing significantly in core AI in\n2026. And again, this is a pretty very dynamic area of planning, but we wanted\nto share kind of our early thoughts as things are shaping up.\nOperator: Your next question comes from the line of Brian Nowak with Morgan Stanley.\nPlease go ahead.\nBrian Nowak: Thanks for taking my questions. I have two.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 25
  },
  {
    "text": " early thoughts as things are shaping up.\nOperator: Your next question comes from the line of Brian Nowak with Morgan Stanley.\nPlease go ahead.\nBrian Nowak: Thanks for taking my questions. I have two.\n\nThe first one, Mark, just to kind of\ngo back to the intelligence labs and sort of the vision for Superintelligence.\nAs you sort of sit here now versus 12 months ago, can you just sort of walk us\nthrough any changes of technological constraints or technological gating\nfactors that you are most focused on overcoming in the next 24 months that\nmay have been different than they were in the past just to make sure you can\nreally lead in the idea of Superintelligence over the next ten years?\nAnd then the second one to Susan or Mark, one on the core, you’ve made so\nmany improvements to the core to drive higher engagement,\nrecommendations, et cetera.\nCan you just walk us through a couple of the factors you’re still most excited\nabout to come in the next 18 months that you think could drive a further lift to\nengagement on the core platform? Thanks.\nMark Zuckerberg: Yes. Sure.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 26
  },
  {
    "text": " a couple of the factors you’re still most excited\nabout to come in the next 18 months that you think could drive a further lift to\nengagement on the core platform? Thanks.\nMark Zuckerberg: Yes. Sure.\n\nI mean in terms of the research agenda and a bunch of the areas that\nwe’re very focused on.\nI do think focusing on self-improvement is a very important area of research.\nAnd there’s obviously different scaling paradigms, and I don’t want to get too\nmuch into the detail of research that we’re doing on this.\nBut I think that for developing superintelligence at some level, you’re not just\ngoing to be learning from people because you’re trying to build something that\nis fundamentally smarter than people.\nSo it’s going to need to learn how to -- or you’re going to need to develop a way\nfor it to be able to improve itself.\nSo that, I think, is a very fundamental thing. That is going to have very broad\nimplications for how we build products, how we run the company, new things\nthat we can invent, new discoveries that can be made, society more broadly.\nI think that, that’s just a very fundamental part of this. In terms of the shape of\nthe effort overall, I guess I’ve just gotten a little bit more convinced around the\nability for small talent-dense teams to be the optimal configuration for driving\nfrontier research.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 27
  },
  {
    "text": " of the shape of\nthe effort overall, I guess I’ve just gotten a little bit more convinced around the\nability for small talent-dense teams to be the optimal configuration for driving\nfrontier research.\n\nAnd it’s a bit of a different setup than we have on our other\nworld-class machine learning systems.\nSo if you look at like what we do in Instagram or Facebook or our ad system, we\ncan very productively have many hundreds or thousands of people basically\nworking on improving those systems, and we have very well-developed\nsystems for kind of individuals to run tests and be able to test a bunch of\ndifferent things. You don’t need every researcher there to have the whole\nsystem in their head.\nBut I think for this -- for the leading research on superintelligence, you really\nwant the smallest group that can hold the whole thing in their head, which\ndrives, I think, some of the physics around the team size and how -- and the\ndynamics around how that works. But I’ll hand it over to Susan to talk about\nmore of the practical stuff.\nSusan Li: Brian, on the sort of forward-looking roadmap for the core recommendation\nengine. There are a handful of shorter-term things that we’re focused on in the\nnear term.\nOne is we’re focused on making recommendations even more adaptive to what\na person is engaging with during their session so that the recommendations we\nsurface are the most relevant to what they’re interested in at that moment.\nAnd we’re making optimizations to help the best content from smaller creators\nbreak out by matching it to the right audiences sooner after it gets posted.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 28
  },
  {
    "text": "to what they’re interested in at that moment.\nAnd we’re making optimizations to help the best content from smaller creators\nbreak out by matching it to the right audiences sooner after it gets posted.\n\nAnd\nwe’re also working on improving the ability for our systems to discover more\ndiversified and niche interests for each person through interest exploration and\nlearning explicit user preferences.\nWe’re also planning to scale up our models further and incorporate more\nadvanced techniques that should improve the overall quality of\nrecommendations. But we also have a lot of long-term bets in the hopper\naround areas like developing foundational models that will support\nrecommendations across multiple services, incorporating LLMs more deeply\ninto our recommendation systems.\nAnd a big focus of this work is going to be on optimizing the systems to make\nthem more efficient, so that we can continue to scale up the capacity that we\nuse for our recommendation systems without eroding the ROI that we deliver.\nOperator: Your next question comes from the line of Doug Anmuth with JPMorgan.\nPlease go ahead.\nDouglas Anmuth: Thanks so much for taking the questions. One for Mark and one for Susan.\nMark, Meta has been a huge proponent of open source AI. How has your\nthinking changed here at all, just as you pursue superintelligence and push for\neven greater returns on your significant infrastructure investments? And then,\nSusan, your comments on ‘26 CapEx suggest more than $100 billion of spend\nnext year potentially. Do you continue to expect to finance all this yourself? Or\ncould there be opportunities to partner here? Thanks.\nMark Zuckerberg: Yes.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 29
  },
  {
    "text": "Ex suggest more than $100 billion of spend\nnext year potentially. Do you continue to expect to finance all this yourself? Or\ncould there be opportunities to partner here? Thanks.\nMark Zuckerberg: Yes.\n\nI mean on open source, I don’t think that our thinking has particularly\nchanged on this. We’ve always open-sourced some of our models and not open\nsourced everything that we’ve done.\nSo I would expect that we will continue to produce and share leading open\nsource models. I also think that there are a couple of trends that are playing\nout.\nOne is that we’re getting models that are so big that they’re just not practical\nfor a lot of other people to use. So it’s -- we would kind of wrestle with whether\nit’s productive or helpful to share that or if that’s really just primarily helping\ncompetitors or something like that.\nSo I think that there’s that concern. And then obviously as you approach real\nsuperintelligence, I think there is a whole different set of safety concerns that I\nthink we need to take very seriously that I wrote about in my note this morning.\nBut I think the bottom line is, I would expect that we will continue open\nsourcing work.\nI expect us to continue to be a leader there. And I also expect us to continue to\nnot open source everything that we do, which is a continuation of kind of what\nwe’ve been kind of working on. And yes, I mean I think Susan will talk a little bit\nmore about the infrastructure, but it really is a massive investment.\nWe think it will be good over time.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 30
  },
  {
    "text": "nd of what\nwe’ve been kind of working on. And yes, I mean I think Susan will talk a little bit\nmore about the infrastructure, but it really is a massive investment.\nWe think it will be good over time.\n\nBut we do take very seriously that this is a\njust massive amount of capital to convert into many gigawatts of compute\nwhich we think is going to help us produce leading research and quality\nproducts and running the business, I do look for opportunities to basically\nconvert capital into quality of products that we can deliver for people.\nBut this is certainly a massive bet that we’re kind of -- we’re focused on and we\nwant to make sure that what we build -- accrues to building the best products\nthat we can deliver to the billions of people who use our services.\nSusan Li: Doug, on your second question about how we expect to finance the growing\nCapEx next year. We certainly expect that we will finance some large share of\nthat ourselves, but we’re also exploring ways to work with financial partners to\ncodevelop data centers.\nWe don’t have any finalized transactions to announce, but we generally believe\nthat there will be models here that will attract significant external financing to\nsupport large-scale data center projects that are developed using our ability to\nbuild world-class infrastructure while providing us with flexibility should our\ninfrastructure requirements change over time. So we are exploring many\ndifferent paths.\nOperator: Your next question comes from the line of Justin Post with Bank of America.\nPlease go ahead.\nJustin Post: Great, thank you. I’ll ask another one on the infrastructure.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 31
  },
  {
    "text": " many\ndifferent paths.\nOperator: Your next question comes from the line of Justin Post with Bank of America.\nPlease go ahead.\nJustin Post: Great, thank you. I’ll ask another one on the infrastructure.\n\nMark, your spend is\nnow approaching some of the biggest hyperscalers out there. Do you think of\nall this capacity mostly for internal uses? Or do you think there’s a way to share\nor even come up with a business model where leveraging that capacity for\nexternal uses?\nAnd then Susan, when you think about the ROI on this CapEx, I’m sure you\nhave internal models, I’m sure you can’t share all that, but how are you thinking\nabout the ROI? And are you optimistic about the long-term returns? Thank you.\nSusan Li: Justin, I can go ahead and take a crack at both of those. And obviously Mark,\nyou should feel free to weigh in. Right now we are focused on ensuring that we\nhave enough capacity for our internal use cases, which includes both all of the\ncore AI work that we do to support the recommendation engine work on the\norganic content side, to support all the ads ranking and recommendation work.\nAnd then, of course, to make sure that we are building the training capacity\nthat we think we need in order to build frontier AI models. And to make sure\nthat we’re preparing ourselves for the types of inference use cases that we\nthink might -- that we might have ahead of us as we eventually focus not only\non developing frontier models, but also how we can expand into the kinds of\nconsumer use cases that we think will be hopefully widely useful and engaging\nfor our users.\nSo at present, we’re not really thinking about external use cases on the\ninfrastructure, but I’d say it’s a good question.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 32
  },
  {
    "text": "ses that we think will be hopefully widely useful and engaging\nfor our users.\nSo at present, we’re not really thinking about external use cases on the\ninfrastructure, but I’d say it’s a good question.\n\nOn your second question, which\nis really around the sort of ROI on CapEx, there are a couple of things.\nSo again, on the core AI side, we continue to see strong ROI. Our ability to\nmeasure that is quite good, and we feel sort of very good about the rigorous\nmeasurement and returns that we see there.\nOn the GenAI side, we are clearly much, much earlier on the return curve and\nwe don’t expect that the GenAI work is going to be a meaningful driver of\nrevenue this year or next year.\nBut we remain generally very optimistic about the monetization opportunities\nthat will open up, and Mark spoke to them in his script, the sort of five pillars, so\nI won’t repeat them here.\nAnd we think that over the medium- to long-term timeframe, those are\nopportunities that are very adjacent and intuitive for where -- in terms of where\nour business is today, why they would be big opportunities for us and that\nthere will be sort of big markets attached to each of them.\nSo we, again, are also -- I would say, the last thing I would add here is we are\nbuilding the infrastructure with fungibility in mind.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 33
  },
  {
    "text": " that\nthere will be sort of big markets attached to each of them.\nSo we, again, are also -- I would say, the last thing I would add here is we are\nbuilding the infrastructure with fungibility in mind.\n\nObviously there are a lot of\nthings that you have to build up front in terms of the data center shells, the\nnetworking infrastructure, et cetera.\nBut we will be ordering servers, which ultimately will be the biggest bulk of\nCapEx spend as we need them and when we need them and making sort of the\nbest decisions at those times in terms of figuring out where the capacity will go\nto use.\nOperator: Your next question comes from the line of Mark Shmulik with Bernstein. Please\ngo ahead.\nMark Shmulik: Yes, thank you for taking my questions. Mark, as you go after the\nSuperintelligence vision, especially for those of us on the outside, what are kind\nof some of the markers or KPIs that you’re tracking on whether you’re on track\nand making progress? Is it really against kind of those five pillars you outlined\nabove? Or should we be thinking more broadly?\nAnd Susan, obviously AI is delivering great ROI today, all those investments\nand also building towards kind of longer-term goals. Just curious, has there just\nbeen any change or adjustment to how you think about the relationship\nbetween revenues or core business performance and the cadence of\ninvestments? Thank you.\nMark Zuckerberg: Yes.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 34
  },
  {
    "text": "ous, has there just\nbeen any change or adjustment to how you think about the relationship\nbetween revenues or core business performance and the cadence of\ninvestments? Thank you.\nMark Zuckerberg: Yes.\n\nIn terms of what to look at, I mean what I’m going to look at internally, the\nquality of the people on the teams, the quality of the models that we’re\nproducing, the rate of improvement of our other AI systems across the\ncompany and the extent to which the leading kind of foundation models that\nwe’re building contribute to improving all of the other AI systems and kind of\neverything that we’re doing around the company.\nThen I think you just get into our standard product and business playbook,\nwhich is translating that technology into new products, which will first scale to\nbillions of people and then over time we will monetize.\nBut I think that there’s going to be some lag in that, right? And that, I think, is\nkind of always the way that we work is, whether we’re building some new social\nproduct or this something like Meta AI or a new product around this that we’re\ngoing to work on getting to leading scale, building the highest quality product,\nfocused on that for a few years.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 35
  },
  {
    "text": " social\nproduct or this something like Meta AI or a new product around this that we’re\ngoing to work on getting to leading scale, building the highest quality product,\nfocused on that for a few years.\n\nAnd then once we’re really confident in that\nposition, then we’ll focus on ramping up the business around it.\nSo it’s -- I mean going back to the last question a little bit, it’s sort of when you\ncompare this business to some of the cloud businesses, it’s like we do have this\ndelay where we focus on building research and then doing research and then\nramping consumer products, and it often does take some period of time before\nwe really are ramping up the business around it.\nI think that’s kind of a known property of our business and the cycle around it.\nBut I guess, on the flip side, we believe that if you are building\nsuperintelligence, you should use all of your GPUs to make it so that you’re\nserving your customers really well with that.\nAnd we think that there’s going to be a much higher return than we can do by\ngenerating that directly rather than just kind of renting or leasing out the\ninfrastructure at other companies.\nSusan Li: On the second part of your question, we’ve said in the past that our primary\nfocus from a profitability perspective is driving consolidated operating profit\ngrowth over time. And it won’t be linear.\nIn some years, we’ll deliver above-average profit growth. And in years where\nwe’re making big investments, I think we will see that impact the amount of\noperating profit growth that we can deliver.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 36
  },
  {
    "text": "r.\nIn some years, we’ll deliver above-average profit growth. And in years where\nwe’re making big investments, I think we will see that impact the amount of\noperating profit growth that we can deliver.\n\nAnd at the moment, we see a lot of\nattractive investment opportunities that we believe are going to set us up to\ndeliver compelling profit growth in the coming years for all of our investors.\nAnd so we’re focused on constraining investments elsewhere as we pursue\nthose investments. But we really believe that this is a time for us to really make\ninvestments in the future of AI as I think it will open up both new opportunities\nfor us in addition to strengthen our core business.\nOperator: Your next question comes from the line of Ron Josey with Citi. Please go\nahead.\nRonald Josey: Great, thanks for taking the question. Mark, I wanted to ask you on Meta AI and\nI think you talked about in the call just growing engagement overall, particularly\non WhatsApp and now you have 1 billion users on the platform and the focus is\nnow on driving personalization.\nSo I want to understand a little bit more how these next-gen models can help\ndrive adoption here, particularly with Behemoth coming online at some point.\nAnd then as people are using Meta AI with WhatsApp, thoughts on search and\nqueries and potentially monetizing that.\nMark Zuckerberg: Yes.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 37
  },
  {
    "text": " here, particularly with Behemoth coming online at some point.\nAnd then as people are using Meta AI with WhatsApp, thoughts on search and\nqueries and potentially monetizing that.\nMark Zuckerberg: Yes.\n\nI’m not going to get super deep into the roadmap on this, but the basic --\nwe do see that as we continue improving the models behind Meta AI and post\ntraining and just engagement increases and as we swap in the updated models,\nwhen we go from Llama 4 to Llama 4.1 when we have that, we expect that just\n-- the models are inherently pretty general.\nSo it’s -- yes, you focus on specific areas, but in general, just sort of gets better\nat a lot of different things that people want to ask it or want to do with it. And I\nthink with each version, both like what we’re doing on a week-to-week basis in\nterms of continuing to train it. And when we drop kind of new generations or\nbig dot releases of each generation, that will improve engagement, too.\nSo we’re focused on that. I’m not going to go into the specific research areas or\ncapabilities that we’re planning on dropping in the future. But obviously I’m\npretty excited about it.\nOperator: Our last question comes from the line of Youssef Squali with Truist Securities.\nPlease go ahead.\nYoussef Squali: Great, thank you guys for taking the questions. I have two. So Mark, the Ray-\nBan initiative has been a homerun for you guys so far.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 38
  },
  {
    "text": "sef Squali with Truist Securities.\nPlease go ahead.\nYoussef Squali: Great, thank you guys for taking the questions. I have two. So Mark, the Ray-\nBan initiative has been a homerun for you guys so far.\n\nWhere are we on the\ndevelopment of glasses? Is that new computational platform that you’ve talked\nabout in the past? Is it moving faster or slower than you thought? And as you\nleverage Meta AI, do you believe glasses will ultimately replace smartphones?\nOr do you need a new form factor that’s AI first? And then, Susan, just quickly,\nhow do you guys see SBC progressing over the next couple of years? Is it fair to\nassume it will grow materially faster than revenue and OpEx? And how do you\nminimize shareholder dilution? Thank you.\nMark Zuckerberg: Yes. I can talk a bit about the glasses. Yes. I mean I’m very excited about the\nprogress that we’re making. I think both the Ray-Ban Metas and I’m very\nexcited about the Oakley Meta, the HSTN’s too and other things that we have\nplanned.\nYes. I mean this product category is clearly doing quite well. And I think it’s\ngood for a lot of things. It is stylish eyewear, so people like wearing them just as\nglasses.\nIt has a bunch of interesting functionality. And then the use of Meta AI in them\njust continues to grow, and the percent of people who are using it for that on a\ndaily basis is increasing, and that’s all good to see.\nI mean I continue to think that glasses are basically going to be the ideal form\nfactor for AI because you can let an AI see what you see throughout the day,\nhear what you hear, talk to you.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 39
  },
  {
    "text": "d to see.\nI mean I continue to think that glasses are basically going to be the ideal form\nfactor for AI because you can let an AI see what you see throughout the day,\nhear what you hear, talk to you.\n\nOnce you get a display in there, whether it’s the\nkind of wide holographic field of view like we showed with Orion or just a\nsmaller display that might be good for displaying some information.\nAnd that’s also going to unlock a lot of value where you can just interact with\nan AI assistant throughout the day in this multimodal way. It can see the\ncontent around you. It can generate a UI for you, show you information and be\nhelpful.\nI mean I personally think that -- I wear contact lenses, I feel like if I didn’t have\nmy vision corrected, I’d be sort of at a cognitive disadvantage going through\nthe world. And I think in the future, if you don’t have glasses that have AI or\nsome way to interact with AI, I think you’re kind of similarly probably be at a\npretty significant cognitive disadvantage compared to other people who you’re\nworking with, or competing against.\nSo I think that this is a pretty fundamental form factor. There are a lot of\ndifferent versions of it. Right now we’re building ones that I think are stylish,\nbut aren’t focused on the display.\nI think that there’s a whole set of different things to explore with displays.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 40
  },
  {
    "text": "t of\ndifferent versions of it. Right now we’re building ones that I think are stylish,\nbut aren’t focused on the display.\nI think that there’s a whole set of different things to explore with displays.\n\nThis\nis kind of what we’ve been maxing out with Reality Labs over the last 5 to 10\nyears is basically doing the research on all of these different things.\nAnd it’s a -- I don’t know 10 years ago, I would have -- like the other thing that’s\nawesome about glasses is, they are going to be the ideal way to blend the\nphysical and digital worlds together.\nSo the whole metaverse vision, I think, is going to end up being extremely\nimportant, too, and AI is going to accelerate that, too.\nIt’s just that if you’d asked me five years ago, whether we’d have kind of\nholograms that created immersive experiences or superintelligence first, I think\nmost people would have thought that you’d get the holograms first. And it’s\nthis interesting kind of quirk of the tech industry that I think we’re going to end\nup having really strong AI first.\nBut because we’ve been investing in this, I think we’re just several years ahead\non building out glasses. And I think that, that’s something that we’re excited to\nkeep on investing in heavily because I think it’s going to be a really important\npart of the future.\nKenneth Dorell: Youssef, we didn’t quite catch your second question, do you mind just\nrepeating it?\nYoussef Squali: Sure. Just as you look at the spend on stock-based compensation over the next\ncouple of years with all these hires, I’m assuming that we’re going to see that\nmaterially or grow materially faster maybe than revenue and OpEx.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 41
  },
  {
    "text": "at the spend on stock-based compensation over the next\ncouple of years with all these hires, I’m assuming that we’re going to see that\nmaterially or grow materially faster maybe than revenue and OpEx.\n\nAnd I just\nwant to know how -- what you guys are doing to plan to minimize shareholder\ndilution? Is it mostly buybacks or anything else? Thank you.\nSusan Li: Thanks, Youssef. So I mean the impact of the sort of increased compensation\ncosts including SBC, of our AI hires this year is reflected in the revised 2025\nexpense outlook and in the comments I made about sort of the 2026, expense\noutlook.\nThose are obviously a big driver of 2026 expense growth as we recognize the\nfull year of compensation for the additional talent we’re bringing on. Having\nsaid that, so we factored that into our sort of expense outlook. Having said\nthat, we certainly -- we are very focused on making sure, on keeping an eye on\ndilution.\nAnd we generally believe that our strong financial position is going to allow us\nto support these investments while continuing to repurchase shares as part of\nthe sort of buyback program that offsets equity compensation and as well as\nprovide quarterly cash dividend distributions to our investors.\nKenneth Dorell: Great. Thank you, everyone, for joining us today. We look forward to speaking\nwith you again soon.\nOperator: This concludes today’s conference call. Thank you for your participation, and\nyou may now disconnect.",
    "doc_path": "data\\raw\\META\\META_2025_Q2.txt",
    "chunk_id": 42
  },
  {
    "text": "Third Quarter 2025 Results Conference Call\nOctober 29th, 2025\nKenneth Dorell, Director, Investor Relations\nThank you. Good afternoon and welcome to Meta’s third quarter 2025 earnings conference call.\nJoining me today are Mark Zuckerberg, CEO and Susan Li, CFO.\nOur remarks today will include forward‐looking statements, which are based on assumptions as of\ntoday. Actual results may differ materially as a result of various factors including those set forth in\ntoday’s earnings press release, and in our quarterly report on Form 10-Q filed with the SEC. We\nundertake no obligation to update any forward-looking statement.\nDuring this call we will present both GAAP and certain non‐GAAP financial measures. A\nreconciliation of GAAP to non‐GAAP measures is included in today’s earnings press release. The\nearnings press release and an accompanying investor presentation are available on our website at\ninvestor.atmeta.com.\nAnd now, I’d like to turn the call over to Mark.\nMark Zuckerberg, CEO\nThanks Ken, and thanks everyone for joining today.\nWe had another strong quarter -- with 3.5 billion people using at least one of our apps every day.\nInstagram hit a major milestone with 3 billion monthly actives.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 0
  },
  {
    "text": "nd thanks everyone for joining today.\nWe had another strong quarter -- with 3.5 billion people using at least one of our apps every day.\nInstagram hit a major milestone with 3 billion monthly actives.\n\nWe're seeing good momentum\nacross our other apps as well, including Threads which recently passed 150 million daily actives\nand remains on track to become the leader in its category.\nI'm very focused on establishing Meta as the leading frontier AI lab -- building personal\nsuperintelligence for everyone, and delivering the app experiences and computing devices that will\nimprove the lives of billions of people around the world. Our approach of advancing open source AI\nmeans that when Meta innovates, everyone benefits.\nMeta Superintelligence Labs is off to a strong start. I think that we have already built the lab with\nthe highest talent density in the industry. We're heads down developing our next generation of\nmodels and products, and I'm looking forward to sharing more on that front over the coming\nmonths. We're also building what we expect to be an industry-leading amount of compute.\nNow there's a range of timelines for when people think that we’re going to get superintelligence.\nSome people think that we’ll get there in a few years, others think it'll be 5, 7 years, or longer. I\nthink it's the right strategy to aggressively front-load building capacity so that way we're prepared\nfor the most optimistic cases. That way, if superintelligence arrives sooner, we will be ideally\npositioned for a generational paradigm shift and many large opportunities.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 1
  },
  {
    "text": "y so that way we're prepared\nfor the most optimistic cases. That way, if superintelligence arrives sooner, we will be ideally\npositioned for a generational paradigm shift and many large opportunities.\n\nIf it takes longer, then\nwe'll use the extra compute to accelerate our core business -- which continues to be able to\nprofitably use much more compute than we’ve been able to throw at it. And we’re seeing very high\ndemand for additional compute both internally and externally. And in the worst case, we would\njust slow building new infrastructure for some period while we grow into what we build.\nThe upside is extremely high for both our existing apps and new products and businesses that are\nbecoming possible to build.\nAcross Facebook, Instagram, and Threads, our AI recommendation systems are delivering higher\nquality and more relevant content, which led to 5% more time spent on Facebook in Q3 and 10%\non Threads. Video is a particular bright spot, with video time spent on Instagram up more than\n30% since last year. As video continues to grow across our apps, Reels now has an annual run rate\nof over $50 billion.\nImprovements in our recommendation systems will also become even more leveraged as the\nvolume of AI-created content grows. Social media has gone through two eras so far. First was\nwhen all content was from friends, family, and accounts that you followed directly. The second\nwas when we added all the creator content. Now, as AI makes it easier to create and remix\ncontent, we're going to add yet another huge corpus of content on top of those.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 2
  },
  {
    "text": " followed directly. The second\nwas when we added all the creator content. Now, as AI makes it easier to create and remix\ncontent, we're going to add yet another huge corpus of content on top of those.\n\nRecommendation\nsystems that understand all of this content more deeply and can show you the right content to\nhelp you achieve your goals are going to be increasingly valuable.\nOur ads business continues to perform very well, largely due to improvements in our AI ranking\nsystems as well. This quarter, we saw meaningful advances from unifying different models into\nsimpler, more general models -- which drive both better performance and efficiency. And now the\nannual run-rate going through our completely end-to-end AI-powered ad tools has passed $60\nbillion.\nOne way that I think about our company overall is that there are three giant transformers that run\nFacebook, Instagram, and ads recommendations. We have a very strong pipeline of lots of ways to\nimprove these models by incorporating new AI advances and capabilities. And at the same time,\nwe're also working on combining these three major AI systems into a single unified AI system that\nwill effectively run our family of apps and business -- using increasing intelligence to improve the\ntrillions of recommendations that it will make for people every day.\nI'm also very excited about the new products that we're going to be able to build.\nMore than a billion monthly actives already use Meta AI, and we see usage increase as we improve\nour underlying models. I'm very excited to get a frontier model into Meta AI and I think that the\nopportunity there is very large.\nThe same goes for our Business AI.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 3
  },
  {
    "text": "e see usage increase as we improve\nour underlying models. I'm very excited to get a frontier model into Meta AI and I think that the\nopportunity there is very large.\nThe same goes for our Business AI.\n\nEvery day, people have more than 1 billion active threads with\nbusiness accounts across our messaging platforms -- ranging from product questions to customer\nsupport. Our Business AIs will enable tens of millions of businesses to scale these conversations\nand improve their sales at low cost. The better our models get, the better this is going to work for\nall businesses.\nThis quarter we also launched Vibes, which is the next generation of our AI creation tools and\ncontent experiences. Retention is looking good so far and its usage keeps growing quickly week\nover week. I'm looking forward to ramping up the growth of Vibes over the coming months.\nMore broadly, I think that Vibes is an example of a new content type enabled by AI, and I think that\nthere are more opportunities to build many more novel types of content ahead as well. As our new\nmodels become ready, I'm looking forward to starting to show everyone some of the new kinds of\nproducts we're working on.\nAt Connect, we announced our 2025 line of AI glasses, and the response so far has been great.\nThe new Ray-Ban Meta glasses and Oakley Meta Vanguards are both selling well as people love\nthe improved battery life, camera resolution, new AI capabilities, and the great design. And there's\nour new Meta Ray-Ban Display glasses -- our first glasses with a high-resolution display and the\nMeta Neural Band to interact with them.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 4
  },
  {
    "text": "olution, new AI capabilities, and the great design. And there's\nour new Meta Ray-Ban Display glasses -- our first glasses with a high-resolution display and the\nMeta Neural Band to interact with them.\n\nThey sold out in almost every store within 48 hours, with\ndemo slots fully booked through the end of next month. So we're going to have to invest in\nincreasing manufacturing and selling more of those. This is an area where we're clearly leading and\nhave a huge opportunity ahead.\nTaking a step back, if we deliver even a fraction of the opportunity ahead for our existing apps and\nthe new experiences that are possible, then I think that the next few years will be the most\nexciting period in our history.\nWe've got a lot to do. But we're making real progress, delivering strong business results, building\nthe talent density and infrastructure needed for the next era, and leading the way on AI devices\nthat will define the next computing platform. I'm proud of how our teams are rising to the\nchallenge, and I'm grateful for their dedication, hard work, and creativity. As always, thank you all\nfor being a part of this journey with us.\nAnd now, here's Susan.\nSusan Li, CFO\nThanks Mark and good afternoon everyone.\nLet’s begin with our segment results.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 5
  },
  {
    "text": ", and creativity. As always, thank you all\nfor being a part of this journey with us.\nAnd now, here's Susan.\nSusan Li, CFO\nThanks Mark and good afternoon everyone.\nLet’s begin with our segment results.\n\nAll comparisons are on a year-over-year basis unless\notherwise noted.\nOur community across the Family of Apps continues to grow, and we estimate more than 3.5\nbillion people used at least one of our Family of Apps on a daily basis in September.\nQ3 Total Family of Apps revenue was $50.8 billion, up 26% year over year.\nQ3 Family of Apps ad revenue was $50.1 billion, up 26% or 25% on a constant currency basis.\nIn Q3, the total number of ad impressions served across our services increased 14%. Impression\ngrowth was healthy across all regions, driven by engagement and user growth, particularly on\nvideo surfaces. The average price per ad increased 10% year-over-year, benefiting from increased\nadvertiser demand, largely driven by improved ad performance. This was partially offset by\nimpression growth, particularly from lower-monetizing regions and surfaces.\nFamily of Apps other revenue was $690 million, up 59%, driven by WhatsApp paid messaging\nrevenue growth as well as Meta Verified subscriptions.\nWithin our Reality Labs segment, Q3 revenue was $470 million, up 74% year-over-year. The\nsignificant year-over-year growth in Q3 was partly due to retail partners stocking up on Quest\nheadsets ahead of the holiday season. We did not have a similar benefit in the third quarter of last\nyear since our Quest 3S headset launched in the fourth quarter of 2024.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 6
  },
  {
    "text": "rtners stocking up on Quest\nheadsets ahead of the holiday season. We did not have a similar benefit in the third quarter of last\nyear since our Quest 3S headset launched in the fourth quarter of 2024.\n\nAside from this, strong AI\nglasses revenue also contributed to revenue growth in Q3.\nMoving now to our consolidated results.\nQ3 total revenue was $51.2 billion, up 26% or 25% on a constant currency basis.\nQ3 total expenses were $30.7 billion, up 32% compared to last year. Year-over-year expense\ngrowth accelerated 20 percentage points from Q2 due primarily to three factors:\nFirst, legal-related expense growth was higher than in Q2, due to charges we recorded in the third\nquarter, as well as us lapping a period of accrual reversals in the third quarter a year ago.\nSecond, employee compensation growth accelerated, driven by technical hires, particularly AI\ntalent.\nFinally, growth in infrastructure costs accelerated, due to increased infrastructure operating costs\nassociated with our expanded data center fleet, depreciation on our incremental capex spend, and\nthird party cloud spend.\nWe ended Q3 with over 78,400 employees, up 8% year-over-year, driven by hiring in priority areas\nof monetization, infrastructure, Reality Labs, Meta Superintelligence Labs, as well as regulation\nand compliance.\nThird quarter operating income was $20.5 billion, representing a 40% operating margin.\nQ3 interest and other income was $1.1 billion, driven primarily by unrealized gains on our\nmarketable equity securities.\nOur tax rate for the quarter was 87%, which was unfavorably impacted by a one-time, non-cash\nreduction in deferred tax assets that we no longer anticipate using under new US tax law.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 7
  },
  {
    "text": "quity securities.\nOur tax rate for the quarter was 87%, which was unfavorably impacted by a one-time, non-cash\nreduction in deferred tax assets that we no longer anticipate using under new US tax law.\n\nOur tax\nrate would have been 14% excluding this charge. Although the transition to the new US tax law\nresulted in an accounting charge in the third quarter, we continue to expect we will recognize\nsignificant cash tax savings for the remainder of the current year and future years under the new\nlaw, and this quarter's charge reflects the total expected impact from the transition to the new US\ntax law.\nNet income was $2.7 billion or $1.05 per share. Excluding the one-time tax charge, our net income\nand EPS would have been $18.6 billion and $7.25 per share, respectively.\nCapital expenditures, including principal payments on finance leases, were $19.4 billion, driven by\ninvestments in servers, data centers and network infrastructure.\nFree cash flow was $10.6 billion. We repurchased $3.2 billion of our Class A common stock and\npaid $1.3 billion in dividends to shareholders. We ended the quarter with $44.4 billion in cash and\nmarketable securities and $28.8 billion in debt.\nTurning now to the business outlook.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 8
  },
  {
    "text": "mmon stock and\npaid $1.3 billion in dividends to shareholders. We ended the quarter with $44.4 billion in cash and\nmarketable securities and $28.8 billion in debt.\nTurning now to the business outlook.\n\nThere are two primary factors that drive our revenue\nperformance: our ability to deliver engaging experiences for our community, and our effectiveness\nat monetizing that engagement over time.\nOn the first, daily actives continue to grow year-over-year across Facebook, Instagram and\nWhatsApp.\nWe’re continuing to see improvements to our products and recommendations drive incremental\nengagement, with year-over-year growth in global time spent accelerating on both Facebook and\nInstagram in Q3. In the US, overall time spent on Facebook and Instagram grew double digits year-\nover-year, driven by continued video strength as well as healthy growth in non-video time on\nFacebook.\nThe engagement gains continue to be driven by product work and ongoing improvements to our\nrecommendation systems as we optimize our model architectures, implement advanced modeling\ntechniques, and integrate more signals about people’s interests. We also continue to focus on\nincreasing the freshness of recommended content. On Facebook, our systems are now surfacing\ntwice as many Reels published that day than at the start of the year.\nLooking to 2026, we expect to advance our recommendation systems across several dimensions.\nOn Instagram, one focus is evolving our systems to surface content across a broader set of topics\nthat cater to the diverse interests of each person.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 9
  },
  {
    "text": " recommendation systems across several dimensions.\nOn Instagram, one focus is evolving our systems to surface content across a broader set of topics\nthat cater to the diverse interests of each person.\n\nThis follows a similar approach we’ve\nimplemented on Facebook that has driven good results.\nWe also expect to make significant progress on our longer-term ranking innovations in 2026. We\nare seeing promising results from our research efforts to create foundational ranking models and\nexpect the new model innovations we’re developing as part of this will enable us to significantly\nscale up the amount of data and compute we use to train our recommendation models in 2026,\nyielding more relevant recommendations.\nAnother large focus next year is leveraging LLMs to improve content understanding. We expect\nthis is going to enable our systems to more precisely label the keywords and topics within videos\nand posts, which will allow our systems to both develop deeper intuition about a person’s\ninterests and retrieve the content that matches them.\nFinally, we’re making good progress with Meta AI and Threads.\nThe number of people using Meta AI across our family of apps continues to grow and we’re\nincreasingly leveraging first party content into Meta AI results, with the majority of Meta AI’s\nresponses to Facebook Deep Dive queries in the US now showing related Reels. We’re also seeing\na lot of traction with media generation.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 10
  },
  {
    "text": "rty content into Meta AI results, with the majority of Meta AI’s\nresponses to Facebook Deep Dive queries in the US now showing related Reels. We’re also seeing\na lot of traction with media generation.\n\nPeople have created over 20 billion images using our\nproducts, and since launching Vibes within Meta AI in September, we've seen media generation in\nthe app increase more than tenfold.\nOn Threads, we’re seeing strong growth in both daily actives and the depth of engagement as we\ncontinue to improve recommendations. The ranking optimizations we made in Q3 alone drove a\n10% increase in time spent on Threads. We also continue to ship new features, including launching\ndirect messaging in Q3 so anyone on Threads can now message one another within the app.\nNow to the second driver of our revenue performance: increasing monetization efficiency.\nThe first part of this work is optimizing the level of ads within organic engagement.\nWe continue to refine ad supply across each of our major surfaces within Facebook and Instagram\nto better deliver ads at the time and place they are most relevant to people. Longer-term, we have\nexciting ads supply opportunities on both Threads and WhatsApp Status. Ads are now running\nglobally in Feed on Threads, and we’re following our typical monetization playbook of optimizing\nthe ads formats and performance before we ramp supply.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 11
  },
  {
    "text": "Threads and WhatsApp Status. Ads are now running\nglobally in Feed on Threads, and we’re following our typical monetization playbook of optimizing\nthe ads formats and performance before we ramp supply.\n\nWithin WhatsApp Status, we are\ncontinuing to gradually introduce ads and expect to complete the roll out next year.\nThe second part of increasing monetization efficiency is improving marketing performance.\nAdvancing our ads systems remains a critical aspect of this work, and we are driving performance\ngains through ongoing improvements in our larger-scale ads ranking models.\nFor example, we continue to broaden the adoption of Lattice, our unified model architecture. In\nQ3 we rolled out Lattice to app ads, which drove a nearly 3% gain in conversions for that objective.\nSince introducing Lattice back in 2023 along with other back-end improvements, we have now cut\nthe number of ads ranking and recommendation models by approximately one hundred as we\nconsolidated smaller and more specialized models into larger ones that use the Lattice\narchitecture to generalize learnings across surfaces and objectives. We continue to observe\nperformance improvements as we combine models, and expect to drive additional gains as we\nconsolidate another two hundred models over the coming years into a smaller number of highly\ncapable models.\nIn addition to advancing our foundational ads models, we’re innovating on our run-time models we\nuse downstream of them for ads inference. For example, we began piloting a new run-time ads\nranking model in Q3 that leverages more compute and data than our prior models to select more\nrelevant ads.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 12
  },
  {
    "text": "\nuse downstream of them for ads inference. For example, we began piloting a new run-time ads\nranking model in Q3 that leverages more compute and data than our prior models to select more\nrelevant ads.\n\nIn testing, we’ve seen this new model drive a more than 2% lift in conversions on\nInstagram.\nWe also significantly improved performance of Andromeda in Q3 by combining models across\nretrieval and early stage ranking into a single model, driving a 14% increase in ads quality on\nFacebook surfaces.\nWithin our ads products, we’re seeing continued momentum with Advantage+. In Q3, we\ncompleted the roll out of our streamlined campaign creation flow for Advantage+ Lead\ncampaigns, so now advertisers running sales, app or lead campaigns have end-to-end automation\nturned on from the beginning, allowing our systems to look across our platform to optimize\nperformance by automatically choosing criteria like who to show the ads to and where to show\nthem. The annual run-rate of revenue running through our end-to-end automated solutions has\nnow reached $60 billion following the implementation of the new streamlined creation flow as we\ncontinue to see more advertisers leverage the performance benefits of our solutions.\nWithin our Advantage+ creative suite, the number of advertisers using at least one of our video\ngeneration features was up 20% versus the prior quarter as adoption of image animation and\nvideo expansion continues to scale. We’ve also added more generative AI features to make it\neasier for advertisers to optimize their ad creatives and drive increased performance.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 13
  },
  {
    "text": "f image animation and\nvideo expansion continues to scale. We’ve also added more generative AI features to make it\neasier for advertisers to optimize their ad creatives and drive increased performance.\n\nIn Q3, we\nintroduced AI-generated music so advertisers can have music generated for their ad that aligns\nwith the tone and message of the creative.\nFinally, business messaging remains a significant opportunity for us. We’re seeing strong growth\nacross our portfolio of solutions, including with click-to-WhatsApp ads, which grew revenue 60%\nyear-over-year in Q3. We’re also making good progress on our Business AI efforts, where we’ve\nbeen focused on building a turnkey AI that helps businesses generate leads and drive sales. We’ve\nbeen opening access in recent months to more businesses within our initial test markets, the\nPhilippines and Mexico, and have seen strong usage, with millions of conversations between\npeople and Business AIs taking place since July. This month, we expanded availability within\nWhatsApp and Messenger to all eligible businesses in Mexico and the Philippines, respectively. In\nthe US, we’re also starting to roll out the ability for merchants to add their Business AIs to their\nwebsite so we can support the full sale funnel from ad to purchase.\nNext, I would like to discuss our approach to capital allocation.\nOur primary focus is deploying capital to support the company’s highest order priorities, including\ndeveloping leading AI products, models, and business solutions.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 14
  },
  {
    "text": " our approach to capital allocation.\nOur primary focus is deploying capital to support the company’s highest order priorities, including\ndeveloping leading AI products, models, and business solutions.\n\nAs we make significant\ninvestments in infrastructure to support this work, we are focused on preserving maximum long-\nterm flexibility to ensure we can meet our future capacity needs while also being able to respond\nto how the market develops in the years ahead.\nWe’re doing so in several ways, including staging data center sites so we can spring up capacity\nquickly in future years as we need it, as well as establishing strategic partnerships that give us\noption value for future compute needs. The strong financial position and cash generation of our\nbusiness enable us to make these investments while also accessing additional pools of cost\nefficient capital.\nMoving to our financial outlook. We expect fourth quarter 2025 total revenue to be in the range of\n$56-59 billion. Our guidance assumes foreign currency is an approximately 1% tailwind to year-\nover-year total revenue growth, based on current exchange rates. Our outlook reflects an\nexpectation for continued strong ad revenue growth, partially offset by lower year-over-year\nReality Labs revenue in Q4. The anticipated reduction in Reality Labs revenue is due to us lapping\nthe introduction of Quest 3S in Q4 of last year as well as retail partners procuring Quest headsets\nduring Q3 of this year to prepare for the holiday season, which were recorded as revenue in the\nthird quarter.\nTurning to the expense and capex outlooks.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 15
  },
  {
    "text": "ll as retail partners procuring Quest headsets\nduring Q3 of this year to prepare for the holiday season, which were recorded as revenue in the\nthird quarter.\nTurning to the expense and capex outlooks.\n\nI’ll first start with 2025 before providing some\ncommentary on our planning for 2026.\nWe expect full year 2025 total expenses to be in the range of $116-118 billion, updated from our\nprior outlook of $114-118 billion and reflecting a growth rate of 22-24% year-over-year.\nWe currently expect 2025 capital expenditures, including principal payments on finance leases, to\nbe in the range of $70-72 billion, increased from our prior outlook of $66-72 billion.\nOn to tax. Absent any changes to our tax landscape, we expect our fourth quarter 2025 tax rate to\nbe 12-15%.\nTurning now to 2026.\nWe are at an exciting point for our company, where we have continued runway to improve our core\nservices today as well as the opportunity to build new AI-powered experiences and services that\nwill transform how people engage with our products in the future. We expect the set of\ninvestments we’re making within our ads and organic engagement initiatives next year will enable\nus to continue to deliver strong revenue growth in 2026, while our progress on AI models and\nproducts will position us to capitalize on new revenue opportunities in the years to come.\nA central requirement to realizing these opportunities is infrastructure capacity. As we have begun\nto plan for next year, it’s become clear that our compute needs have continued to expand\nmeaningfully, including versus our own expectations last quarter.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 16
  },
  {
    "text": " is infrastructure capacity. As we have begun\nto plan for next year, it’s become clear that our compute needs have continued to expand\nmeaningfully, including versus our own expectations last quarter.\n\nWe are still working through our\ncapacity plans for next year, but we expect to invest aggressively to meet these needs both by\nbuilding our own infrastructure and contracting with third party cloud providers. We anticipate\nthis will provide further upward pressure on our capex and expense plans next year.\nAs a result, our current expectation is that capex dollar growth will be notably larger in 2026 than\n2025. We also anticipate total expenses will grow at a significantly faster percentage rate in 2026\nthan 2025, with growth driven primarily by infrastructure costs, including incremental cloud\nexpenses and depreciation. Employee compensation costs will be the second largest contributor\nto growth, as we recognize a full year of compensation for employees hired throughout 2025,\nparticularly AI talent, and add technical talent in priority areas.\nFinally, we continue to monitor active legal and regulatory matters, including the increasing\nheadwinds in the EU and the US that could significantly impact our business and financial results.\nFor example, in the EU, we continue to engage constructively with the European Commission on\nour Less Personalized Ads offering. However, we cannot rule out the Commission imposing\nfurther changes to that offering that could have a significant negative impact on our European\nrevenue, as early as this quarter.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 17
  },
  {
    "text": "ized Ads offering. However, we cannot rule out the Commission imposing\nfurther changes to that offering that could have a significant negative impact on our European\nrevenue, as early as this quarter.\n\nIn the US, a number of youth-related trials are scheduled for\n2026, and may ultimately result in a material loss.\nIn closing, this was another good quarter for our business. We have an exciting set of\nopportunities to continue improving our core business while delivering innovative new\nexperiences and services for the people and businesses using our products in the years to come.\nWith that, Krista, let’s open up the call for questions.\nOperator: Thank you. We will now open the line for a question and answer session. To ask\na question, please press star one on your touchtone phone. To withdraw your\nquestion again, press star one. Please limit yourself to one question. Please\npick up your handset before asking your question to ensure clarity. If you are\nstreaming today’s call, please mute your computer speakers. And your first\nquestion comes from the line of Brian Nowak with Morgan Stanley. Please go\nahead.\nBrian Nowak: Thanks for taking my questions. I have two for Susan. The first one, Susan, so\nthe pipeline for core improvements to come in ‘26 with models and ad ranking\nmodels and more types of compute seems very exciting, and the infrastructure\nbuild seems sizable behind that.\nSo can you help us a little understand some of the early quantifiable signals\nyou’re seeing on A/B tests from some of these improvements to come that\nsort of make you most excited and give you confidence you’re going to get\nROIC from all this CapEx. That’s the first one. Second one’s a little faster.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 18
  },
  {
    "text": "sts from some of these improvements to come that\nsort of make you most excited and give you confidence you’re going to get\nROIC from all this CapEx. That’s the first one. Second one’s a little faster.\n\nHow\nlarge is the Reality Labs revenue headwind in the 4Q guidance? Thanks.\nSusan Li: Thanks Brian for the question. I think your first question had a couple parts to\nit, so I’m going to try to disaggregate those parts, and let me know if this -- if\nthis addresses what you’re getting to. I will say that the growth in 2026 CapEx\nrelative to 2025 comes from growth in each of the core areas, MSL, core AI, as\nwell as non-AI spend.\nSo all of those areas are growing, but the MSL AI needs are growing the most.\nIn terms of the core AI pipeline, I think we talked about -- last year when we\nwere going into the 2025 budget process, we had a roadmap of resource\ninvestments across both headcount and compute that we thought would pay\noff in 2026.\nAnd it’s really a very broad range of sort of different ads ranking and\nperformance efforts. And we’re continuing to see that those have paid off\nthrough the course of the year.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 19
  },
  {
    "text": "would pay\noff in 2026.\nAnd it’s really a very broad range of sort of different ads ranking and\nperformance efforts. And we’re continuing to see that those have paid off\nthrough the course of the year.\n\nThere is a long list of specific efforts, but one of\nthe measures that we look at to monitor this is, how are we driving ad\nperformance?\nHow are conversions growing? Conversions is a complex metric for us because\nadvertisers optimize for so many different conversions with different values.\nBut when we control for that and look at value weighted conversion rates,\nwe’re seeing very strong year-over-year growth and conversions -- weighted\nconversions continue to grow faster than impressions.\nWe also talked about some of the new model architecture over the course of\nthe year and the degree to which the new model architecture is enabling us also\nto take advantage of having more data and more compute to drive ads\nperformance.\nSo we expect that that’s going to be a continued story in 2026. We are in fact\nat the beginning of our 2026 budgeting process now, and we see a similar list\nof revenue investments, that we -- that we’re excited to be able to invest in.\nAnd so we think that that’s going to be a big part of our ability to continue to\ndrive strong revenue performance throughout the year. On your second\nquestion, which is the Reality Labs revenue headwind, I don’t think we have\nquantified the exact size of that.\nWe expect that Q4 Reality Labs revenue will be lower than last year for a\ncouple reasons that I alluded to.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 20
  },
  {
    "text": " the Reality Labs revenue headwind, I don’t think we have\nquantified the exact size of that.\nWe expect that Q4 Reality Labs revenue will be lower than last year for a\ncouple reasons that I alluded to.\n\nThe biggest factor is we’re lapping the\nintroduction of Quest 3S in Q4 of last year and we don’t have a new headset in\nthe market this year.\nWe also recorded all of our holiday related Quest 3S sales in Q4‘24, since the\nheadset was launched in October ’24. This year, we’re recognizing some of\nthose Quest 3S sales in Q3 as retail partners have procured Quest headsets in\nadvance of the holiday season.\nWe’re still expecting significant year-over-year growth in AI glasses revenue in\nQ4, as we benefit from strong demand for the recent products that we’ve\nintroduced, but that is more than offset by the headwinds to the Quest\nheadsets.\nOperator: Your next question comes from the line of Doug Anmuth with JP Morgan.\nPlease go ahead.\nDoug Anmuth: Great. Thanks for taking the question. I appreciate the strategy to front load\ncapacity for Superintelligence. Can you just talk about your thought process in\nkind of triangulating the CapEx dollar growth and the significantly faster\nexpense growth next year with core growth in the business, and then the\nimpact on earnings and free cash flow? And do you have targets that we should\nbe thinking about for cash on hand or net cash overall? Thanks.\nSusan Li: Thanks Doug. We’re, right now, I would say in the process of -- relatively early,\nactually, still in the process of putting together our budget for 2026.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 21
  },
  {
    "text": "on hand or net cash overall? Thanks.\nSusan Li: Thanks Doug. We’re, right now, I would say in the process of -- relatively early,\nactually, still in the process of putting together our budget for 2026.\n\nAnd it is\non the capacity side, a particularly dynamic process.\nWe’re certainly seeing that we wish we had more capacity today than we do.\nWe would be able to put it towards good use, certain not only would the MSL\nteam appreciate having more capacity, but we’d be able to put it towards good\nand ROI positive use in the core business as well.\nSo we’re really trying to plan ahead not only to ensure that we have the\ncapacity we need in 2026, but also to give ourselves the sort of flexibility and\noption value to have the capacity that we think we could need in ‘27 and ‘28. So\nthat said, there are lots of moving pieces in the budget. It’s not baked yet.\nIt’s still sort of in the process of coming together. We don’t have specific\ntargets to share, but we do feel like our strategic priority is really making sure\nthat we have the compute that we need to be well positioned to succeed at AI.\nAnd that’s sort of the foremost priority as we’re putting together the budget.\nMark Zuckerberg: Yes.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 22
  },
  {
    "text": "is really making sure\nthat we have the compute that we need to be well positioned to succeed at AI.\nAnd that’s sort of the foremost priority as we’re putting together the budget.\nMark Zuckerberg: Yes.\n\nI mean, I’ll add a few thoughts on this too, although I mean, as Susan said,\nwe’re still working through the actual budget and I think we’ll typically have\nmore to share on that early next year.\nBut to date, we keep on seeing this pattern where we build some amount of\ninfrastructure to what we think is an aggressive assumption and then we keep\non having more demand to be able to use more compute, especially in the core\nbusiness in ways that we think would be quite profitable than we end up having\ncompute for.\nSo I think that that suggests that being able to make a significantly larger\ninvestment here is very likely to be a profitable thing over some period,\nbecause if the primary use of it is going to be to accelerate the AI research and\nthe new AI work that we’re doing and how that relates to both the core\nbusiness and new products, but any compute that we don’t need for that, we\nfeel pretty good that we’re going to be able to absorb a very large amount of\nthat to just convert into more intelligence and better recommendations in our\nFamily of Apps and ads in a profitable way.\nNow, I mean, it’s of course possible to overshoot that, right. If we do, I mean,\nthis is what I mentioned in my comments then we see that there’s just a lot of\ndemand for other new things that we build internally, externally.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 23
  },
  {
    "text": "se possible to overshoot that, right. If we do, I mean,\nthis is what I mentioned in my comments then we see that there’s just a lot of\ndemand for other new things that we build internally, externally.\n\nLike almost\nevery week, people come to us from outside the company asking us to stand up\nan API service or asking if we have different compute that they could get from\nus.\nAnd we haven’t done that yet, but obviously if you got to a point where you\noverbuilt, you could have that as an option. And then, the kind of very worst\ncase would be that we effectively have just prebuilt for a couple of years, in\nwhich case of course there would be some loss and depreciation, but we’d\ngrow into that and use it over time.\nSo my view on this is that rather than continuing to be constrained on CapEx\nand feeling in the core business, like we have significant investments that we\ncould make that we’re not able to make that would be profitable, that the right\nthing to do is to try to accelerate this to make sure that we have the compute\nthat we need, both for the AI research and new things that we’re doing, and to\ntry to get to a different state on our compute stance on the core business.\nSo that’s kind of how I’m thinking about that overall.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 24
  },
  {
    "text": "eed, both for the AI research and new things that we’re doing, and to\ntry to get to a different state on our compute stance on the core business.\nSo that’s kind of how I’m thinking about that overall.\n\nOf course there’s a lot of\noperational constraints too on what one can build, right? So, we’re basically\ntrying to work through this all, and I think we’ll have more to share in the\ncoming months and over the course of next year, but I think that there’s just a\nhuge, huge amount of opportunities ahead here.\nOperator: Your next question comes from the line of Eric Sheridan with Goldman Sachs.\nPlease go ahead.\nEric Sheridan: Thanks so much for taking the question. Mark, wanted to reflect on some of\nyour comments with respect to scaling towards superintelligence and bringing\nit back to consumer AI. Maybe reflect a little bit on the signals you’ve gotten on\nthe way consumers across Family of Apps interact with Meta AI today, and\nhow you think about scaling and exiting models from the superintelligence\neffort might change the utility and behavior around Meta AI in the years ahead.\nThanks.\nMark Zuckerberg: Yeah, I mean, a lot of people use Meta AI today. I mean, as I said in my\ncomments up front, there’s more than a billion people who use it on a monthly\nbasis.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 25
  },
  {
    "text": " the years ahead.\nThanks.\nMark Zuckerberg: Yeah, I mean, a lot of people use Meta AI today. I mean, as I said in my\ncomments up front, there’s more than a billion people who use it on a monthly\nbasis.\n\nAnd what we see is that, as we improve the quality of the model,\nprimarily for post training Llama 4, at this point, we are -- we continue to see\nimprovements in usage.\nSo our view is that when we get the new models that we’re building in MSL in\nthere and get like truly frontier models with novel capabilities that you don’t\nhave in other places, then I think that this is just a massive latent opportunity,\nright?\nWe know -- I mean, I would guess that Meta, I think has the best track record of\nany company out there of taking a new product that people love and getting it\nto billions of people in terms of usage.\nSo I think that the ability to plug in leading models is going to, I would predict,\nlead to a very large amount of use of these things over the coming years. So I’m\nvery excited about that in terms of new products. It’s not just Meta AI as an\nassistant.\nI think that there are going to be all kinds of new products around different\ncontent formats, and we’re starting to see that with video and content\ncreation, but I think that there’s going to be a lot more like that that I’m quite\nexcited about.\nAnd then there are the business versions of all these too, like business AI. And\nthen, that’s, of course, one part of the story is the new things that will be\npossible to build.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 26
  },
  {
    "text": "m quite\nexcited about.\nAnd then there are the business versions of all these too, like business AI. And\nthen, that’s, of course, one part of the story is the new things that will be\npossible to build.\n\nAnd then the other part is how more intelligent models are\njust going to improve the core business and improve the recommendations\nthat we make across the Family of Apps and improve the recommendations in\nadvertising.\nAnd I think that there’s just a, as we’ve shown, there’s sort of this very large\namount of headroom and the opportunity there keeps growing as we are\nimproving and optimizing the AI there. And I think that that really shows no\nsign of being near the end.\nI think that there’s quite a bit more to do there. And like I said in response to the\nlast question, we are sort of perennially operating the Family of Apps and ads\nbusiness in a compute-starved state at this point, which is on the one hand sort\nof an odd thing to say, given the compute that we built up.\nBut we really are taking a lot of the resources and using them to advance future\nthings that we’re doing. And we think that there’s a lot more compute that we\ncould put towards these that would just unlock a huge amount of opportunity\nin the core business as well.\nOperator: Your next question comes from the line of Mark Shmulik with Bernstein. Please\ngo ahead.\nMark Shmulik: Yes, hi. Thanks for taking the questions.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 27
  },
  {
    "text": "t of opportunity\nin the core business as well.\nOperator: Your next question comes from the line of Mark Shmulik with Bernstein. Please\ngo ahead.\nMark Shmulik: Yes, hi. Thanks for taking the questions.\n\nSusan, if you think about the visibility\ninto kind of the runway next year of continued ad performance and\nengagement improvements, how do you think about kind of the scale of those\nimprovements versus kind of the progress we’ve seen over the last two years?\nAnd then Mark, as you think about kind of the timing of some of these newer\nefforts coming out of Superintelligence Labs, is us anchoring to kind of an\nupdated frontier model launch sometime next year like the right way for us to\nthink about it, or should we be looking at kind of progress from new products\nyou’re excited to see ship like Vibes? Thank you.\nSusan Li: Thanks Mark. So on the sort of ads improvement side, some of the innovations\nthat we have been launching actually involve sort of improving our larger scale\nmodels. So we don’t use our larger model architectures like GEM for inference\nbecause their size and complexity would make it too cost prohibitive. The way\nthat we drive performance from those models is by using them to transfer\nknowledge to smaller, lightweight models that are used at runtime.\nAnd then in addition to the foundation model work, we are working on\nadvancing our inference models by developing new techniques and\narchitectures that then allow us to scale up compute and complexity in an ROI\npositive way.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 28
  },
  {
    "text": "the foundation model work, we are working on\nadvancing our inference models by developing new techniques and\narchitectures that then allow us to scale up compute and complexity in an ROI\npositive way.\n\nSo, in general, we obviously have a very large base of advertisers.\nThere’s a lot of demand liquidity, in the system and even small scale\nimprovements that we are able to make in terms of driving basis point\nimprovements in the performance of ads or single digit increases in\nconversions relative to impressions in a given quarter off of a large base mean\nthat we’re really able to continue to grow the absolute dollars of revenue\ngrowth in a pretty meaningful way.\nOperator: Your next question comes from the line of Justin Post with Bank of America.\nPlease go ahead.\nJustin Post: Great. Thank you.\nKenneth Dorell: Hey, Justin, just give us one second. I think there was a second part to Mark’s\nquestion that we just want to get to on MSL.\nMark Zuckerberg: Yeah, I mean, I’ll keep it quick. I mean, I don’t think we have any specific timing\nto announce certainly on the models or products, but I expect that you will see\nboth. We expect to build novel models and novel products, and I’m excited to\nshare more when we have it.\nOperator: Justin, please go ahead.\nJustin Post: Great, thanks. So Mark, you mentioned the prior two content cycles, and\nobviously you’ve been able to generate very attractive margins on them. As we\nget into the AI cycle, obviously some concerns on the investment, but could\nyou talk a little bit about how you’re thinking about tools, that could be coming\nout for users? I know there’s some new competition.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 29
  },
  {
    "text": "the AI cycle, obviously some concerns on the investment, but could\nyou talk a little bit about how you’re thinking about tools, that could be coming\nout for users? I know there’s some new competition.\n\nAnd then secondly, how\nyou think about margins in this content cycle, any reason to think they would\nbe different versus prior cycles? Thank you.\nMark Zuckerberg: I think it’s too early to really understand what the margins are going to be for\nthe new products that we build. I mean, I think certainly every -- each product\nhas somewhat different characteristics and I think we’ll kind of understand how\nthat goes over time. I mean my general goal is to build a business that\nmaximizes value for people who use our products and maximizes profitability,\nnot margin. So I think we’ll kind of just try to build the best things that we can\nand try to deliver the most value that we can for most people.\nOperator: Your next question comes from the line of Ross Sandler with Barclays. Please\ngo ahead.\nRoss Sandler: Great. Hey Mark, some of the goals for competing AI labs are around achieving\nAGI or these other milestones that are kind of like out there and a little esoteric.\nHow are you setting up your new team in terms of achieving those types of\ngoals versus products that can generate revenue for Meta kind of right out of\nthe gate?\nAnd is the goal that you had articulated to us previously around giving billions\nof people kind of a personal AI to use still the direction of travel that you see, or\nis there other things like kind of this Vibes or Sora angle that you think are\npotentially important? How should we think about the overall direction? Thank\nyou.\nMark Zuckerberg: Sure.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 30
  },
  {
    "text": "hat you see, or\nis there other things like kind of this Vibes or Sora angle that you think are\npotentially important? How should we think about the overall direction? Thank\nyou.\nMark Zuckerberg: Sure.\n\nSo the way that I think about this is that the research is going to enable\nnew technological capabilities to exist, and then those capabilities can get built\ninto all kinds of different products.\nSo the ability to reason more intelligently is, for example, very important across\na large number of things. It would be useful for an assistant. It will also be\nuseful in business AI. It will also be useful in the AI agent that we’re building to\nhelp advertisers figure out what their campaigns are going to be.\nIt will also have implications for eventually how we do ranking and\nrecommendations of people’s feeds and make different decisions there. That’s\njust one example. I mean, certainly the capability to be able to produce very\nhigh quality good video is going to be useful for giving people new creative\ntools.\nIt will help increase the amount of content inventory that can be shown in\nInstagram and Facebook and, therefore, should enable an increase in\nengagement there. It should help advertisers be able to create creative that will\nhelp us monetize better.\nSo you can just go kind of down the list of capabilities that you’d expect. And I\nthink each one will enable a bunch of different things.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 31
  },
  {
    "text": " be able to create creative that will\nhelp us monetize better.\nSo you can just go kind of down the list of capabilities that you’d expect. And I\nthink each one will enable a bunch of different things.\n\nAnd I think the art of\nproduct development here is looking at the list of technology capabilities and\nfiguring out what new products are going to be useful and prioritizing those.\nBut fundamentally, I would sort of expect this exponential curve in new\ntechnology capabilities that are going to become available. And the other thing\nthat I expect is that I think being the best in a given area will drive great returns\nrather than this is not like a check the box exercise of like, okay, we can\ngenerate some kind of content and someone else can. I think that like the\ncompany that is the best at each of these capabilities, I think, will get a large\namount of the potential value for doing that.\nSo there are lots of different capabilities to build. I’m not sure that any one\ncompany is going to be the best at all of them. I doubt that’s going to be the\ncase, but a lot of what we’re trying to do is not kind of do some things that\nothers have done. We’re really trying to build novel capabilities and I’m keeping\nthis high level because I’m not -- I don’t want to necessarily from a competitive\nor strategic perspective get into what we’re prioritizing.\nBut that hopefully gives you a sense of how we’re thinking about what we’re\ndoing.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 32
  },
  {
    "text": " I’m not -- I don’t want to necessarily from a competitive\nor strategic perspective get into what we’re prioritizing.\nBut that hopefully gives you a sense of how we’re thinking about what we’re\ndoing.\n\nWe want to be able to kind of build novel things, build them into a lot of\nour products, and then have the compute to scale them to billions of people.\nAnd we think that that’s going to both show up in terms of new products, keep\nbeing possible, and new businesses and very significant improvements to the\ncurrent business too.\nOperator: Your next question comes from the line of Mark Mahaney with Evercore ISI.\nPlease go ahead.\nMark Mahaney: Thanks. Could I just ask just a question on Meta AI and both the product and\nthe monetization path? So when you look at it, what you’ve seen that’s most\nencouraging to you in terms of the adoption and the use of Meta AI, and then\nwhen you think about -- I know you generally like to roll out and then deepen\nengagement and then later think about monetization. Like where do you think\nyou are on that path now? Is it clear to you what the monetization options are\nfor Meta AI? Thank you very much.\nMark Zuckerberg: I mean, I think the most promising thing that we’re seeing is one, that we’re\nable to build something that a large number of people use. And I think that\nthat’s valuable. And then secondly, that as we -- there is a clear correlation as\nwe improve the models in ways that we think make them better, that people\nuse them more.\nSo that shows that we have a runway to basically be able to improve\nengagement and turn this into a product that’s leading over time.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 33
  },
  {
    "text": "in ways that we think make them better, that people\nuse them more.\nSo that shows that we have a runway to basically be able to improve\nengagement and turn this into a product that’s leading over time.\n\nIn terms of\nwhere we are on this and we basically just did this huge effort to boot up Meta\nSuperintelligence Labs and build what I am very proud of is, I think the highest\ntalent density lab in the industry at this point.\nThere are a lot of really great researchers and infrastructure folks and data folks\nwho are now a part of this effort who are focused on training the next\ngeneration of work and doing some really novel work.\nAnd when that is ready, I think that we will be able to plug that into a number of\nthe products that we’re building, and I think that that will be very exciting.\nBut I think that that’s really the next thing that we’re looking at. And then from\nthere, I think these models will also improve monetization in all of the different\nways that we’ve talked about so far in terms of improving engagement,\nimproving advertising, helping advertisers engage.\nI mean, there’s -- the one opportunity that we just usually talk about on these\ncalls, but hasn’t come up as much here is just the ability to make it so that\nadvertisers are increasingly just going to be able to give us a business objective\nand give us a credit card or bank account, and have the AI system basically\nfigure out everything else that’s necessary, including generating video or\ndifferent types of creative that might resonate with different people that are\npersonalized in different ways, finding who the right customers are.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 34
  },
  {
    "text": "else that’s necessary, including generating video or\ndifferent types of creative that might resonate with different people that are\npersonalized in different ways, finding who the right customers are.\n\nAll of the\ncapabilities that we’re building, I think, go towards improving all of these\ndifferent things. So I’m quite optimistic about that.\nOperator: Your next question comes from the line of Ronald Josey with Citi. Please go\nahead.\nRonald Josey: Great. Thanks for taking the question and this maybe dovetails perfectly off of,\nMark, what you just talked about. We heard a lot about end-to-end automation\nhere. I think we’ve seen a $60 billion ARR. Wanted to hear about, if you can talk\nto us more just about adoption rates amongst the advertisers, and then maybe\nbigger picture as you incorporate ranking recommendation changes like\nAndromeda or GEM or Lattice. Just talk to us how this automation is driving,\ncall it a higher ROI for advertisers overall, as we bring it all together. Thank you.\nSusan Li: Yeah. So we’ve been sort of laying the continued brick by brick build of\nAdvantage+ and extending the set of objectives that it applies to over time.\nAnd so in Q3, we completed the global rollout of the streamlined campaign\ncreation flow for Advantage+ lead campaigns.\nSo now advertisers who are running sales app or lead campaigns have end-to-\nend automation turned on from the beginning.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 35
  },
  {
    "text": "ollout of the streamlined campaign\ncreation flow for Advantage+ lead campaigns.\nSo now advertisers who are running sales app or lead campaigns have end-to-\nend automation turned on from the beginning.\n\nAnd like the kind of application\nof the streamlined campaign creation flow for other objectives, this generally\nallows advertisers to optimize and automate several aspects of the campaign\nsetup process at once.\nThat includes things like audience selection, where to show the ad, how the\nbudget gets placed and distributed across ad sets to just drive the most\nefficient outcomes. And we see that Advantage+ continues to drive\nperformance gains, advertisers who run lead campaigns using Advantage+ are\nseeing a 14% lower cost per lead on average than those who are not.\nAnd I would say that we think that there is still a lot of opportunity, generally, to\ngrow adoption of Advantage+.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 36
  },
  {
    "text": "ng Advantage+ are\nseeing a 14% lower cost per lead on average than those who are not.\nAnd I would say that we think that there is still a lot of opportunity, generally, to\ngrow adoption of Advantage+.\n\nA lot of advertisers only use our end-to-end\nautomated solutions for a portion of their campaigns, so we can grow share\nthere, and to capture that opportunity we’re focused on driving continued\nperformance improvements and addressing some of the key use cases that we\nstill need in order to grow adoption.\nWe’re also working to broaden adoption among advertisers who use one of our\nsingle step automated solutions, for example, advertisers who might only use a\npiece of it like Advantage+ audiences by helping them understand, the benefits\nof using more than one automated system -- one automated solution at the\nsame time.\nSo I would say, Advantage+ is sort of an ongoing platform by which we both\ncontinue to expand the feature set that is available in Advantage+, and then\nexpand the extensibility or the coverage of that feature set to -- sort of the\nbroader set of advertisers. I think Mark mentioned that the annual revenue run\nright now for advertisers who are using these automated options is $60 billion,\nand again, we see that there’s room to continue growing that.\nOperator: Your next question comes from the line of Youssef Squali with Truist\nSecurities. Please go ahead.\nYoussef Squali: Great. Thank you very much.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 37
  },
  {
    "text": " see that there’s room to continue growing that.\nOperator: Your next question comes from the line of Youssef Squali with Truist\nSecurities. Please go ahead.\nYoussef Squali: Great. Thank you very much.\n\nMark, on wearables in particular, do you think\nyou’ll be able to sell enough hardware to recoup your investment or is that\ndependent on maybe creating new avenues for revenue from things like\nadvertising services and commerce through that new computing platform?\nAnd if so, what are kind of the gating factors there? And then Susan, how do\nyou see the on balance sheet versus off balance sheet financing of your AI\ninitiatives? You’ve recently struck a deal with Blue Owl for the Louisiana data\ncenter. Is that part of the CapEx guide for ‘26? And if it’s not, how significant\nwill that way of funding be for Meta going forward and basically will that slow\ndown your CapEx growth past 2026? Thank you.\nMark Zuckerberg: I can talk about wearables, and Susan can jump in on the other part. So I think\nthat there are a few pieces here. One is that the work that on Ray-Ban Meta\nand the Oakley Meta products is going very well. I think, yeah, I mean, at some\npoint, if these continue going as well as it has been, then I think it will be a very\nprofitable investment.\nI think that there’s some revenue that we get from basically selling the devices\nand then some that will come from additional services and from the AI on top\nof it. So I think that there’s a big opportunity. Certainly, the investment here is\nnot just to kind of build a -- just the device.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 38
  },
  {
    "text": " some that will come from additional services and from the AI on top\nof it. So I think that there’s a big opportunity. Certainly, the investment here is\nnot just to kind of build a -- just the device.\n\nIt’s also to build the services on top.\nRight now a lot of people get the devices for a range of things that don’t even\ninclude the AI, even though they like the AI. But I think over time, the AI is going\nto become the main thing that people are using them for. And I think that that’s\ngoing to end up having a big business opportunity by itself.\nBut as products like the Ray-Ban Meta and Oakley Metas are growing, we’re\nalso going to keep on investing in things like the more full field of view product\nform of the Orion prototype that we showed at Connect last year. So those\nthings are obviously earlier in their curve towards getting to being a sustaining\nbusiness.\nAnd our general view is that we want to build these out to reach many\nhundreds of millions or billions of people. And that’s the point at which we think\nthat this is going to be just an extremely profitable business.\nSusan Li: Youssef, to your second question. So the JV that we announced with Blue Owl\nis sort of an example of finding a solution that enabled us to partner with\nexternal capital providers to co-develop data centers in a way that gives us\nlong term optionality in supporting our future capacity needs, just given both\nthe magnitude, but also uncertainty of what the capacity outlook in future\nyears looks like.\nIn terms of how that is recognized as CapEx, our prior CapEx reflected a portion\nof the data center build cost prior to the joint venture being established.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 39
  },
  {
    "text": "apacity outlook in future\nyears looks like.\nIn terms of how that is recognized as CapEx, our prior CapEx reflected a portion\nof the data center build cost prior to the joint venture being established.\n\nGoing\nforward, the construction cost of the data center will not be recorded in CapEx.\nAs the data center is constructed, we will contribute 20% of the remaining\nconstruction costs required, which is in line with our ownership stake, and\nthose will be recorded as other investing cash flows.\nOperator: Your last question comes from the line of Ken Gawrelski with Wells Fargo.\nPlease go ahead.\nKen Gawrelski: Thank you. Just one for me, please. Mark, as you think about with a, hopefully,\na leading frontier model next year in hand, could you talk about where you think\nthe value will accrue in this evolving ecosystem? Will it be with the platforms,\nor do you think that this will be mostly -- the value will accrue to the scaled first\nparty applications? Thank you.\nMark Zuckerberg: I guess I’m not exactly sure what you mean by platform versus application in\nthis context, but I mean, I think that -- I mean, I think that there’s just a lot of\nvalue to create with AI overall. So, I mean, clearly you’re seeing the people who\nare making the hardware, Nvidia’s doing an amazing job, right.\nI think extremely well deserved success. The cloud partners and companies are\nmaking -- or are doing very well. I think that that will likely continue. I think\nthere’s a huge opportunity there. And -- but if you look at it today, the\ncompanies that are building apps, I mean, a lot of the apps are still relatively\nsmall.\nAnd I think that that’s obviously going to be a huge opportunity.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 40
  },
  {
    "text": "there. And -- but if you look at it today, the\ncompanies that are building apps, I mean, a lot of the apps are still relatively\nsmall.\nAnd I think that that’s obviously going to be a huge opportunity.\n\nAnd I think\nwhat we’ve seen overall is basically, people take like individual technology\nadvances and build them into products that then build either communities or\nother kinds of network effects and then end up being very sustaining\nbusinesses.\nAnd I think what we haven’t really seen as much in the history of the\ntechnology industry is the rate of new capabilities being introduced because\naround each of these capabilities, you can build many new products that I think\neach will turn into interesting businesses.\nSo, yeah, so I don’t know. I mean, I’m generally pretty optimistic about there\nbeing a very large opportunity, but in terms of new things to build, I think being\nable to build them and then scale them to billions of people is a huge muscle\nthat Meta has developed and I think we do very well.\nAnd I certainly think that that’s going to deliver a huge amount of value, both in\nthe core business for all the ways that we talked about, how it’s going to\nimprove recommendations and the quality of the services, as well as unifying\nthe models together. So that way, when these systems are deciding what to\nshow, they can just pull from a wider pool.\nAnd these are things that we’ve just seen over the 20 plus years of running the\ncompany that they just deliver consistent wins, that we’re going to keep on\nbeing able to make the systems more general and smarter and make better\nrecommendations for people and have a larger pool of inventory.\nAnd that is all going to be great.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 41
  },
  {
    "text": ", that we’re going to keep on\nbeing able to make the systems more general and smarter and make better\nrecommendations for people and have a larger pool of inventory.\nAnd that is all going to be great.\n\nAnd then there’s going to be a lot of new\nthings that I think we’re going to be able to take and scale to billions of people\nover time and build new businesses, whether that’s advertising or commerce-\nsupported or people paying for it or different kinds of things.\nSo, yeah, it’s -- I think it’s pretty early, but I think we’re seeing the returns in the\ncore business. That’s giving us a lot of confidence that we should be investing a\nlot more and we want to make sure that we’re not underinvesting.\nKenneth Dorell: Great. Thank you everyone for joining us today. We look forward to speaking\nwith you again soon.\nOperator: This concludes today’s conference call. Thank you for your participation and\nyou may now disconnect.",
    "doc_path": "data\\raw\\META\\META_2025_Q3.txt",
    "chunk_id": 42
  },
  {
    "text": "Earnings Call Transcript\nNvidia (NVDA) Q2 2025 Earnings Call Transcript\nBy Motley Fool Transcribing – Aug 28, 2024 at 8:15PM\nFollow\n\nNVDA earnings call for the period ending June 30, 2024.\n\nNvidia (\nNVDA\n1.06%)\nQ2 2025 Earnings Call\nAug 28, 2024, 5:00 p.m. ET\nContents:\n\nPrepared Remarks\nQuestions and Answers\nCall Participants\n\nPrepared Remarks:\n\nOperator\n\nGood afternoon. My name is Abby, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's second-quarter earnings call. All lines have been placed on mute to prevent any background noise.\n\nAfter the speakers' remarks, there will be a question-and-answer session. [Operator instructions] Thank you. And Mr. Stewart Stecker, you may begin your conference.\n\nStewart Stecker -- Senior Director, Investor Relations\n\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the second quarter of fiscal 2025. With me today from NVIDIA are Jensen Huang, president and chief executive officer; and Colette Kress, executive vice president and chief financial officer. I would like to remind you that our call is being webcast live on NVIDIA's Investor Relations website.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 0
  },
  {
    "text": "chief executive officer; and Colette Kress, executive vice president and chief financial officer. I would like to remind you that our call is being webcast live on NVIDIA's Investor Relations website.\n\nThe webcast will be available for replay until the conference call to discuss our financial results for the third quarter of fiscal 2025. The content of today's call is NVIDIA's property. It cannot be reproduced or transcribed without prior written consent. During this call, we may make forward-looking statements based on current expectation.\n\nThese are subject to a number of risks, significant risks, and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, August 28, 2024, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.\n\nDuring this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. Let me highlight an upcoming event for the financial community. We will be attending the Goldman Sachs Communacopia and Technology Conference on September 11 in San Francisco, where Jensen will participate in a keynote fireside chat.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 1
  },
  {
    "text": "t for the financial community. We will be attending the Goldman Sachs Communacopia and Technology Conference on September 11 in San Francisco, where Jensen will participate in a keynote fireside chat.\n\nOur earnings call to discuss the results of our third quarter of fiscal 2025 is scheduled for Wednesday, November 20, 2024. With that, let me turn the call over to Colette.\n\nColette M. Kress -- Chief Financial Officer, Executive Vice President\n\nThanks, Stewart. Q2 was another record quarter. Revenue of $30 billion was up 15% sequentially and up 122% year on year and well above our outlook of $28 billion. Starting with Data Center.\n\nData Center revenue of $26.3 billion was a record, up 16% sequentially and up 154% year on year, driven by strong demand for NVIDIA Hopper, GPU computing, and our networking platforms. Compute revenue grew more than 2.5x. Networking revenue grew more than 2x from the last year. Cloud service providers represented roughly 45% of our Data Center revenue, and more than 50% stemmed from the consumer Internet and enterprise companies.\n\nCustomers continue to accelerate their Hopper architecture purchases while gearing up to adopt Blackwell. Key workloads driving our Data Center growth include generative AI model training and inferencing; video, image, and text data pre and post processing with CUDA and AI workloads; synthetic data generation; AI-powered recommender systems; SQL and Vector database processing as well. Next-generation models will require 10 to 20 times more compute to train with significantly more data. The trend is expected to continue.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 2
  },
  {
    "text": "commender systems; SQL and Vector database processing as well. Next-generation models will require 10 to 20 times more compute to train with significantly more data. The trend is expected to continue.\n\nOver the trailing four quarters, we estimate that inference drove more than 40% of our Data Center revenue. CSPs, consumer Internet companies, and enterprises benefit from the incredible throughput and efficiency of NVIDIA's inference platform. Demand for NVIDIA is coming from frontier model makers, consumer Internet services, and tens of thousands of companies and start-ups building generative AI applications for consumers, advertising, education, enterprise and healthcare, and robotics. Developers desire NVIDIA's rich ecosystem and availability in every cloud.\n\nCSPs appreciate the broad adoption of NVIDIA and are growing their NVIDIA capacity given the high demand. NVIDIA H200 platform began ramping in Q2, shipping to large CSPs, consumer Internet, and enterprise company. The NVIDIA H200 builds upon the strength of our Hopper architecture and offering over 40% more memory bandwidth compared to the H100. Our Data Center revenue in China grew sequentially in Q2 and a significant contributor to our Data Center revenue.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 3
  },
  {
    "text": "Hopper architecture and offering over 40% more memory bandwidth compared to the H100. Our Data Center revenue in China grew sequentially in Q2 and a significant contributor to our Data Center revenue.\n\nAs a percentage of total Data Center revenue, it remains below levels seen prior to the imposition of export controls. We continue to expect the China market to be very competitive going forward. The latest round of MLPerf inference benchmarks highlighted NVIDIA's inference leadership with both NVIDIA Hopper and Blackwell platform combining to win gold medals on all tasks. At Computex, NVIDIA, with the top computer manufacturers, unveiled an array of Blackwell architecture-powered systems and NVIDIA networking for building AI factories and data centers.\n\nWith the NVIDIA MGX modular reference architecture, our OEMs and ODM partners are building more than 100 Blackwell-based systems designed quickly and cost-effectively. The NVIDIA Blackwell platform brings together multiple GPU, CPU, DPU, NVLink, and Link Switch and the networking chips, systems, and NVIDIA CUDA software to power the next generation of AI across the cases, industries, and countries. The NVIDIA GB200 NVL72 system with the fifth-generation NVLink enables all 72 GPUs to act as a single GPU and deliver up to 30x faster inference for LLM's workloads and unlocking the ability to run trillion-parameter models in real time. Hopper demand is strong, and Blackwell is widely sampling.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 4
  },
  {
    "text": "ingle GPU and deliver up to 30x faster inference for LLM's workloads and unlocking the ability to run trillion-parameter models in real time. Hopper demand is strong, and Blackwell is widely sampling.\n\nWe executed a change to the Blackwell GPU mass to improve production yields. Blackwell production ramp is scheduled to begin in the fourth quarter and continue into fiscal year '26. In Q4, we expect to get several billion dollars in Blackwell revenue. Hopper shipments are expected to increase in the second half of fiscal 2025.\n\nHopper supply and availability have improved. Demand for Blackwell platforms is well above supply, and we expect this to continue into next year. Networking revenue increased 16% sequentially. Our Ethernet for AI revenue, which includes our Spectrum-X end-to-end Ethernet platform, doubled sequentially with hundreds of customers adopting our Ethernet offerings.\n\nSpectrum-X has broad market support from OEM and ODM partners and is being adopted by CSPs, GPU cloud providers, and enterprises, including xAI to connect the largest GPU compute cluster in the world. Spectrum-X supercharges Ethernet for AI processing and delivers 1.6x the performance of traditional Ethernet. We plan to launch new Spectrum-X products every year to support demand for scaling compute clusters from tens of thousands of GPUs today to millions of DPUs in the near future. Spectrum-X is well on track to begin a multibillion-dollar product line within a year.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 5
  },
  {
    "text": " demand for scaling compute clusters from tens of thousands of GPUs today to millions of DPUs in the near future. Spectrum-X is well on track to begin a multibillion-dollar product line within a year.\n\nOur sovereign AI opportunities continue to expand as countries recognize AI expertise and infrastructure at national imperatives for their society and industries. Japan's National Institute of Advanced Industrial Science and Technology is building its AI Bridging Cloud Infrastructure 3.0 supercomputer with NVIDIA. We believe sovereign AI revenue will reach low double-digit billions this year. The enterprise AI wave has started.\n\nEnterprises also drove sequential revenue growth in the quarter. We are working with most of the Fortune 100 companies on AI initiatives across industries and geographies. A range of applications are fueling our growth, including AI-powered chatbots, generative AI copilots, and agents to build new monetizable business applications and enhance employee productivity. Amdocs is using NVIDIA generative AI for their smart agent, transforming the customer experience and reducing customer service costs by 30%.\n\nServiceNow is using NVIDIA for its Now Assist offering, the fastest-growing new product in the company's history. SAP is using NVIDIA to build dual copilots. Cohesity is using NVIDIA to build their generative AI agent and lower generative AI development costs. Snowflake, serves over 3 billion queries a day for over 10,000 enterprise customers, is working with NVIDIA to build copilots.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 6
  },
  {
    "text": "uild their generative AI agent and lower generative AI development costs. Snowflake, serves over 3 billion queries a day for over 10,000 enterprise customers, is working with NVIDIA to build copilots.\n\nAnd lastly, is using NVIDIA AI Omniverse to reduce end-to-end cycle times for their factories by 50%. Automotive was a key growth driver for the quarter as every automaker developing autonomous vehicle technology is using NVIDIA in their data centers. Automotive will drive multibillion dollars in revenue across on-prem and cloud consumption and will grow as next-generation AV models require significantly more compute. Health care is also on its way to being a multibillion-dollar business as AI revolutionizes medical imaging, surgical robots, patient care, electronic health record processing, and drug discovery.\n\nDuring the quarter, we announced a new NVIDIA AI foundry service to supercharge generative AI for the world's enterprises with Meta's Llama 3.1 collection of models. This marks a watershed moment for enterprise AI. Companies for the first time can leverage the capabilities of an open-source frontier-level model to develop customized AI applications to encode their institutional knowledge into an AI flywheel to automate and accelerate their business. Accenture is the first to adopt the new service to build custom Llama 3.1 models for both its own use and to assist clients seeking to deploy generative AI applications.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 7
  },
  {
    "text": "and accelerate their business. Accenture is the first to adopt the new service to build custom Llama 3.1 models for both its own use and to assist clients seeking to deploy generative AI applications.\n\nNVIDIA NIMs accelerate and simplify model deployment. Companies across healthcare, energy, financial services, retail, transportation, and telecommunications are adopting NIMs, including Aramco, Lowes, and Uber. AT&T realized 70% cost savings and eight times latency reduction after moving into NIMs for generative AI, call transcription, and classification. Over 150 partners are embedding NIMs across every layer of the AI ecosystem.\n\nWe announced NIM Agent Blueprint, a catalog of customizable reference applications that include a full suite of software for building and deploying enterprise generative AI applications. With NIM Agent Blueprint, enterprises can refine their AI applications over time, creating a data-driven AI flywheel. The first NIM Agent Blueprints include workloads for customer service, computer-aided drug discovery, and enterprise retrieval augmented generation. Our system integrators, technology solution providers, and system builders are bringing NVIDIA NIM Agent Blueprints to enterprises.\n\nNVIDIA NIM and NIM Agent Blueprints are available through the NVIDIA AI Enterprise software platform, which has great momentum. We expect our software, SaaS, and support revenue to approach a $2 billion annual run rate exiting this year, with NVIDIA AI Enterprise notably contributing to growth. Moving to gaming and AI PC. Gaming revenue of $2.88 billion increased 9% sequentially and 16% year on year.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 8
  },
  {
    "text": "annual run rate exiting this year, with NVIDIA AI Enterprise notably contributing to growth. Moving to gaming and AI PC. Gaming revenue of $2.88 billion increased 9% sequentially and 16% year on year.\n\nWe saw sequential growth in console, notebook, and desktop revenue, and demand is strong and growing and channel inventory remains healthy. Every PC with RTX is an AI PC. RTX PCs can deliver up to 1,300 AI tops and are now over 200 RTX AI laptops designed from leading PC manufacturers. With 600 AI-powered applications and games and an installed base of 100 million devices, RTX is set to revolutionize consumer experiences with generative AI.\n\nNVIDIA ACE, a suite of generative AI technologies is available for RTX AI PCs. Megabreak is the first game to use NVIDIA ACE, including our small language model, Nemotron 4B optimized on device inference. The NVIDIA gaming ecosystem continues to grow. Recently added RTX and DLSS titles include Indiana Jones and The Great Circle, Awakening, and Dragon Age: The Vanguard.\n\nThe GeForce NOW library continues to expand with total catalog size of over 2,000 titles, the most content of any cloud gaming service. Moving to pro visualization. Revenue of $454 million was up 6% sequentially and 20% year on year. Demand is being driven by AI and graphic use cases, including model fine-tuning and Omniverse-related workloads.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 9
  },
  {
    "text": "pro visualization. Revenue of $454 million was up 6% sequentially and 20% year on year. Demand is being driven by AI and graphic use cases, including model fine-tuning and Omniverse-related workloads.\n\nAutomotive and manufacturing were among the key industry verticals driving growth this quarter. Companies are racing to digitalize workflows to drive efficiency across their operations. The world's largest electronics manufacturer, Foxconn, is using NVIDIA Omniverse to power digital twins of the physical plants that produce NVIDIA Blackwell systems. And several large global enterprises, including Mercedes-Benz, signed multiyear contracts for NVIDIA Omniverse Cloud to build industrial digital twins of factories.\n\nWe announced new NVIDIA USD NIMs and connectors to open Omniverse to new industries and enable developers to incorporate generative AI copilots and agents into USD workloads, accelerating our ability to build highly accurate virtual worlds. WPP is implementing the USD NIM microservices in its generative AI-enabled content creation pipeline for customers such as The Coca-Cola Company. Moving to automotive and robotics. Revenue was $346 million, up 5% sequentially and up 37% year on year.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 10
  },
  {
    "text": "ts generative AI-enabled content creation pipeline for customers such as The Coca-Cola Company. Moving to automotive and robotics. Revenue was $346 million, up 5% sequentially and up 37% year on year.\n\nYear-on-year growth was driven by the new customer ramp in self-driving platforms and increased demand for AI cockpit solutions. At the consumer -- at the Computer Vision and Pattern Recognition Conference, NVIDIA won the Autonomous Brand Challenge in the end-to-end driving upscale category, outperforming more than 400 entries worldwide. Boston Dynamics, BYD Electronics, Figure, Intrinsyc, Siemens, and Teradyne Robotics are using the NVIDIA Isaac robotics platform for autonomous robot arms, humanoids, and mobile robots. Now, moving to the rest of the P&L.\n\nGAAP gross margins were 75.1% and non-GAAP gross margins were 75.7%, down sequentially due to a higher mix of new products within Data Center and inventory provisions for low-yielding Blackwell material. Sequentially, GAAP and non-GAAP operating expenses were up 12%, primarily reflecting higher compensation-related costs. Cash flow from operations was $14.5 billion. In Q2, we utilized cash of $7.4 billion toward shareholder returns in the form of share repurchases and cash dividends, reflecting the increase in dividend per shareholder.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 11
  },
  {
    "text": "erations was $14.5 billion. In Q2, we utilized cash of $7.4 billion toward shareholder returns in the form of share repurchases and cash dividends, reflecting the increase in dividend per shareholder.\n\nOur board of directors recently approved a $50 billion share repurchase authorization to add to our remaining $7.5 billion of authorization at the end of Q2. Let me turn the outlook for the third quarter. Total revenue is expected to be $32.5 billion, plus or minus 2%. Our third-quarter revenue outlook incorporates continued growth of our Hopper architecture and sampling of our Blackwell products.\n\nWe expect Blackwell production ramp in Q4. GAAP and non-GAAP gross margins are expected to be 74.4% and 75%, respectively, plus or minus 50 basis points. As our Data Center mix continues to shift to new products, we expect this trend to continue into the fourth quarter of fiscal 2025. For the full year, we expect gross margins to be in the mid-70% range.\n\nGAAP and non-GAAP operating expenses are expected to be approximately $4.3 billion and $3.0 billion, respectively. Full-year operating expenses are expected to grow in the mid- to upper 40% range as we work on developing our next generation of products. GAAP and non-GAAP other income and expenses are expected to be about $350 million, including gains and losses from nonaffiliated investments and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 12
  },
  {
    "text": "lion, including gains and losses from nonaffiliated investments and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items.\n\nFurther financial details are included in the CFO commentary and other information available on our IR website. We are now going to open the call for questions. Operator, would you please help us poll for questions?\nQuestions & Answers:\n\nOperator\n\nThank you. [Operator instructions] We will pause for just a moment to compile the Q&A roster. And as a reminder, we ask that you please limit yourself to one question. And your first question comes from the line of Vivek Arya with Bank of America Securities.\n\nYour line is open.\n\nVivek Arya -- Analyst\n\nThanks for taking my question. Jensen, you mentioned in the prepared comments that there's a change in the Blackwell GPU mask. I'm curious, are there any other incremental changes in back-end packaging or anything else? And I think related, you suggested that you could ship several billion dollars of Blackwell in Q4 despite the change in the design. Is it because all these issues will be solved by then? Just help us size what is the overall impact of any changes in Blackwell timing, what that means to your kind of revenue profile and how are customers reacting to it.\n\nJensen Huang -- President and Chief Executive Officer\n\nYeah. Thanks, Vivek. The change to the mask is complete. There were no functional changes necessary.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 13
  },
  {
    "text": "ofile and how are customers reacting to it.\n\nJensen Huang -- President and Chief Executive Officer\n\nYeah. Thanks, Vivek. The change to the mask is complete. There were no functional changes necessary.\n\nAnd so, we're sampling functional samples of Blackwell, Grace Blackwell, and a variety of system configurations as we speak. There are something like 100 different types of Blackwell-based systems that are built that were shown at Computex, and we're enabling our ecosystem to start sampling those. The functionality of Blackwell is as it is, and we expect to start production in Q4.\n\nOperator\n\nAnd your next question comes from the line of Toshiya Hari with Goldman Sachs. Your line is open.\n\nToshiya Hari -- Analyst\n\nHi. Thank you so much for taking the question. Jensen, I had a relatively longer-term question. As you may know, there's a pretty heated debate in the market on your customers and customers' customers return on investment and what that means for the sustainability of capex going forward.\n\nInternally at NVIDIA, like what are you guys watching? What's on your dashboard as you try to gauge customer return and how that impacts capex? And then a quick follow-up maybe for Colette. I think your sovereign AI number for the full year went up maybe a couple of billion. What's driving the improved outlook and how should we think about fiscal '26? Thank you.\n\nJensen Huang -- President and Chief Executive Officer",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 14
  },
  {
    "text": "ber for the full year went up maybe a couple of billion. What's driving the improved outlook and how should we think about fiscal '26? Thank you.\n\nJensen Huang -- President and Chief Executive Officer\n\nThanks, Toshiya. First of all, when I said ship production in Q4, I mean shipping out, I don't mean starting to ship, but I mean -- I don't mean starting production but shipping up. On the longer-term question, let's take a step back. And you've heard me say that we're going through two simultaneous platform transitions at the same time.\n\nThe first one is transitioning from accelerated computing to -- from general-purpose computing to accelerated computing. And the reason for that is because CPU scaling has been known to be slowing for some time and it has slowed to a crawl. And yet the amount of computing demand continues to grow quite significantly. You could maybe even estimate it to be doubling every single year.\n\nAnd so, if we don't have a new approach, computing inflation would be driving up the cost for every company, and it would be driving up the energy consumption of data centers around the world. In fact, you're seeing that. And so, the answer is accelerated computing. We know that accelerated computing, of course, speeds up applications.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 15
  },
  {
    "text": "he energy consumption of data centers around the world. In fact, you're seeing that. And so, the answer is accelerated computing. We know that accelerated computing, of course, speeds up applications.\n\nIt also enables you to do computing at a much larger scale, for example, scientific simulations or database processing, but what that translates directly to is lower cost and lower energy consumed. And in fact, this week, there's a blog that came out that talked about a whole bunch of new libraries that we offer. And that's really the core of the first platform transition, going from general-purpose computing to accelerated computing. And it's not unusual to see someone save 90% of their computing cost.\n\nAnd the reason for that is, of course, you just sped up an application 50x. You would expect the computing cost to decline quite significantly. The second was enabled by accelerated computing because we drove down the cost of training large language models or training deep learning so incredibly that it is now possible to have gigantic scale models, multitrillion-parameter models and train it on -- pretrain it on just about the world's knowledge corpus and let the model go figure out how to understand human language representation and how to codify knowledge into its neural networks and how to learn reasoning, and so which caused the generative AI revolution. Now, generative AI, taking a step back about why it is that we went so deeply into it is because it's not just a feature, it's not just the capability.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 16
  },
  {
    "text": "nd so which caused the generative AI revolution. Now, generative AI, taking a step back about why it is that we went so deeply into it is because it's not just a feature, it's not just the capability.\n\nIt's a fundamental new way of doing software. Instead of human-engineered algorithms, we now have data. We tell the AI, we tell the model, we tell the computer what are the expected answers. What are our previous observations? And then for it to figure out what the algorithm is, what's the function.\n\nIt learns a universal -- AI is a bit of a universal function approximator and it learns the function. And so, you could learn the function of almost anything. And anything that you have that's predictable, anything that has structure, anything that you have previous examples of. And so, now here we are with generative AI.\n\nIt's a fundamental new form of computer science. It's affecting how every layer of computing is done from CPU to GPU, from human-engineered algorithms to machine-learned algorithms, and the type of applications you could now develop and produce is fundamentally remarkable. And there are several things that are happening in generative AI. So, the first thing that's happening is the frontier models are growing in quite substantial scale.\n\nAnd they're still seeing -- we're still all seeing the benefits of scaling. And whenever you double the size of a model, you also have to more than double the size of the data set to go train it. And so, the amount of flops necessary in order to create that model goes up quadratically. And so, it's not unexpected to see that the next-generation models could take 10, 20, 40 times more compute than last generation.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 17
  },
  {
    "text": "f flops necessary in order to create that model goes up quadratically. And so, it's not unexpected to see that the next-generation models could take 10, 20, 40 times more compute than last generation.\n\nSo, we have to continue to drive the generational performance up quite significantly so we can drive down the energy consumed and drive down the cost necessary to do it. And so, the first one is there are larger frontier models trained on more modalities. And surprisingly, there are more frontier model makers than last year. And so, you have more and more and more.\n\nThat's one of the dynamics going on in generative AI. The second is although it's below the tip of the iceberg, what we see are ChatGPT image generators. We see coding. We use generative AI for coding quite extensively here at NVIDIA now.\n\nWe, of course, have a lot of digital designers and things like that. But those are kind of the tip of the iceberg. What's below the iceberg are the largest systems, largest computing systems in the world today, which are -- and you've heard me talk about this in the past, which are recommender systems moving from CPUs. It's now moving from CPUs to generative AI.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 18
  },
  {
    "text": " largest computing systems in the world today, which are -- and you've heard me talk about this in the past, which are recommender systems moving from CPUs. It's now moving from CPUs to generative AI.\n\nSo, recommender systems, ad generation, custom ad generation targeting ads at very large scale and quite hyper-targeting, search, and user-generated content, these are all very large-scale applications have now evolved to generative AI. Of course, the number of generative AI start-ups is generating tens of billions of dollars of cloud renting opportunities for our cloud partners. And sovereign AI, countries that are now realizing that their data is their natural and national resource and they have to use AI, build their own AI infrastructure so that they could have their own digital intelligence. Enterprise AI, as Colette mentioned earlier, is starting, and you might have seen our announcement that the world's leading IT companies are joining us to take the NVIDIA AI Enterprise platform to the world's enterprises.\n\nThe companies that we're talking to, so many of them are just so incredibly excited to drive more productivity out of the company. And then general robotics. The big transformation last year as we are able to now learn physical AI from watching video and human demonstration and synthetic data generation from reinforcement learning from systems like Omniverse, we are now able to work with just about every robotics companies now to start thinking about, start building general robotics. And so, you can see that there are just so many different directions that generative AI is going.\n\nAnd so, we're actually seeing the momentum of generative AI accelerating.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 19
  },
  {
    "text": " building general robotics. And so, you can see that there are just so many different directions that generative AI is going.\n\nAnd so, we're actually seeing the momentum of generative AI accelerating.\n\nColette M. Kress -- Chief Financial Officer, Executive Vice President\n\nAnd Toshiya, to answer your question regarding sovereign AI and our goals in terms of growth, in terms of revenue, it certainly is a unique and growing opportunity, something that surfaced with generative AI and the desires of countries around the world to have their own generative AI that would be able to incorporate their own language, incorporate their own culture, incorporate their own data in that country. So, more and more excitement around these models and what they can be specific for those countries. So, yes, we are seeing some growth opportunity in front of us.\n\nOperator\n\nAnd your next question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n\nJoe Moore -- Analyst\n\nGreat. Thank you. Jensen, in the press release, you talked about Blackwell anticipation being incredible. But it seems like Hopper demand is also really strong.\n\nI mean, you're guiding for a very strong quarter without Blackwell in October. So, how long do you see sort of coexisting strong demand for both? And can you talk about the transition to Blackwell? Do you see people intermixing clusters? Do you think most of the Blackwell activities, new clusters? Just some sense of what that transition looks like.\n\nJensen Huang -- President and Chief Executive Officer\n\nYeah. Thanks, Joe. The demand for Hopper is really strong. And it's true, the demand for Blackwell is incredible.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 20
  },
  {
    "text": "at that transition looks like.\n\nJensen Huang -- President and Chief Executive Officer\n\nYeah. Thanks, Joe. The demand for Hopper is really strong. And it's true, the demand for Blackwell is incredible.\n\nThere's a couple of reasons for that. The first reason is if you just look at the world's cloud service providers, the amount of GPU capacity they have available, it's basically none. And the reason for that is because they're either being deployed internally for accelerating their own workloads, data processing, for example. Data processing, we hardly ever talk about it because it's mundane.\n\nIt's not very cool because it doesn't generate a picture or generate words. But almost every single company in the world processes data in the background. And NVIDIA's GPUs are the only accelerators on the planet that process and accelerate data. SQL data, Panda's data, data science toolkits like Panda's, and the new one, Polar's.\n\nThese are the ones -- the most popular data processing platforms in the world. And aside from CPUs, which as I've mentioned before, really running out of steam, NVIDIA's accelerated computing is really the only way to get boosting performance out of that. And so, number one is the primary -- the No. 1 use case long before generative AI came along is that the migration of applications one after another to accelerated computing.\n\nThe second is, of course, the rentals. They're renting capacity to model makers. They're renting it to start-up companies. And a generative AI company spends the vast majority of their invested capital into infrastructure so that they could use an AI to help them create products.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 21
  },
  {
    "text": " They're renting it to start-up companies. And a generative AI company spends the vast majority of their invested capital into infrastructure so that they could use an AI to help them create products.\n\nAnd so, these companies need it now. They just simply can't afford -- you just raise money. They want you to put it to use now. You have processing that you have to do.\n\nYou can't do it next year, you got to do it today. And so, there's a fair -- that's one reason. The second reason for Hopper demand right now is because of the race to the next plateau. The first person to the next plateau gets to be -- get to introduce a revolutionary level of AI.\n\nThe second person who gets there is incrementally better or about the same. And so, the ability to systematically and consistently race to the next plateau and be the first one there is how you establish leadership. NVIDIA is constantly doing that, and we show that to the world and the GPUs we make and the AI factories that we make, the networking systems that we make, the SoCs we create. I mean, we want to set the pace.\n\nWe want to be consistently the world's best. And that's the reason why we drive ourselves so hard. Of course, we also want to see our dreams come true and all of the capabilities that we imagine in the future and the benefits that we can bring to society, we want to see all that come true. And so, these model makers are the same.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 22
  },
  {
    "text": "ur dreams come true and all of the capabilities that we imagine in the future and the benefits that we can bring to society, we want to see all that come true. And so, these model makers are the same.\n\nOf course, they want to be the world's best. They want to be the world's first. And although Blackwell will start shipping out in billions of dollars at the end of this year, the standing up of the capacity is still probably weeks and a month or so away. And so, between now and then is a lot of generative AI market dynamic.\n\nAnd so, everybody is just really in a hurry. It's either operational reasons that they need it. They need accelerated computing. They don't want to build any more general-purpose computing infrastructure and even Hopper.\n\nOf course, H200 is state-of-the-art. Hopper, if you have a choice between building CPU infrastructure right now for business or Hopper infrastructure for business right now, that decision is relatively clear. And so, I think people are just clamoring to transition the $1 trillion of established installed infrastructure to a modern infrastructure and Hopper's state-of-the-art.\n\nOperator\n\nAnd your next question comes from the line of Matt Ramsay with TD Cowen. Your line is open.\n\nMatt Ramsay -- Analyst\n\nThank you very much. Good afternoon, everybody. Jensen, I wanted to kind of circle back to an earlier question about the debate that investors are having about the ROI on all of this capex. And hopefully, this question and the distinction will make some sense.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 23
  },
  {
    "text": "nted to kind of circle back to an earlier question about the debate that investors are having about the ROI on all of this capex. And hopefully, this question and the distinction will make some sense.\n\nBut what I'm having discussions about is with like the percentage of folks that you see that are spending all of this money and looking to sort of push the frontier toward AGI convergence and, as you just said, a new plateau in capability, and they're going to spend regardless to get to that level of capability because it opens up so many doors for the industry and for their company versus customers that are really, really focused today on capex versus ROI. I don't know if that distinction makes sense. I'm just trying to get a sense of how you're seeing the priorities of people that are putting the dollars in the ground on this new technology and what their priorities are and their time frames are for that investment. Thanks.\n\nJensen Huang -- President and Chief Executive Officer\n\nThanks, Matt. The people who are investing in NVIDIA infrastructure are getting returns on it right away. It's the best ROI infrastructure, computing infrastructure investment you can make today. And so, one way to think through it, probably the most -- the easiest way to think through it is just go back to first principles.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 24
  },
  {
    "text": "frastructure, computing infrastructure investment you can make today. And so, one way to think through it, probably the most -- the easiest way to think through it is just go back to first principles.\n\nYou have $1 trillion worth of general-purpose computing infrastructure. And the question is, do you want to build more of that or not? And for every $1 billion worth of Juniper CPU-based infrastructure that you stand up, you probably rent it for less than $1 billion. And so, because it's commoditized, there's already $1 trillion on the ground. What's the point of getting more? And so, the people who are clamoring to get this infrastructure, one, when they build out Hopper-based infrastructure and soon, Blackwell-based infrastructure, they start saving money.\n\nThat's a tremendous return on investment. And the reason why they start saving money is because data processing saves money, and data processing is probably just a giant part of it already. And so, recommender systems save money, so on and so forth, OK? And so, you start saving money. The second thing is everything you stand up are going to get rented because so many companies are being founded to create generative AI.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 25
  },
  {
    "text": " money, so on and so forth, OK? And so, you start saving money. The second thing is everything you stand up are going to get rented because so many companies are being founded to create generative AI.\n\nAnd so, your capacity gets rented right away and the return on investment of that is really good. And then the third reason is your own business. Do you want to either create the next frontier yourself or your own Internet services benefit from a next-generation ad system or a next-generation recommender system or a next-generation search system? So, for your own services, for your own stores, for your own user-generated content, social media platforms, for your own services, generative AI is also a fast ROI. And so, there's a lot of ways you could think through it.\n\nBut at the core, it's because it is the best computing infrastructure you could put in the ground today. The world of general-purpose computing is shifting to accelerated computing. The world of human-engineered software is moving to generative AI software. If you were to build infrastructure to modernize your cloud and your data centers, build it with accelerated computing NVIDIA.\n\nThat's the best way to do it.\n\nOperator\n\nAnd your next question comes from the line of Timothy Arcuri with UBS. Your line is open.\n\nTimothy Arcuri -- Analyst\n\nThanks a lot. I had a question on the shape of the revenue growth, both near and longer term. I know Colette, you did increase opex for the year. And if I look at the increase in your purchase commitments and your supply obligations, that's also quite bullish.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 26
  },
  {
    "text": "rowth, both near and longer term. I know Colette, you did increase opex for the year. And if I look at the increase in your purchase commitments and your supply obligations, that's also quite bullish.\n\nOn the other hand, there's some school of thought that not that many customers really seem ready for liquid cooling, and I do recognize that some of these racks can be air-cooled. But Jensen, is that something to consider sort of on the shape of how Blackwell is going to ramp? And then I guess when you look beyond next year, which is obviously going to be a great year and you look into '26, do you worry about any other gating factors like, say, the power supply chain or at some point, models start to get smaller? I'm just wondering if you can speak to that. Thanks.\n\nJensen Huang -- President and Chief Executive Officer\n\nI'm going to work backwards. I really appreciate the question, Tim. So, remember, the world is moving from general-purpose computing to accelerated computing. And the world builds about $1 trillion worth of data centers.\n\n$1 trillion worth of data centers in a few years will be all accelerated computing. In the past, no GPUs are in data centers, just CPUs. In the future, every single data center will have GPUs. And the reason for that is very clear: because we need to accelerate workloads so that we can continue to be sustainable, continue to drive down the cost of computing so that when we do more computing, we don't experience computing inflation.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 27
  },
  {
    "text": "ause we need to accelerate workloads so that we can continue to be sustainable, continue to drive down the cost of computing so that when we do more computing, we don't experience computing inflation.\n\nSecond, we need GPUs for a new computing model called generative AI that we could all acknowledge is going to be quite transformative to the future of computing. And so, I think working backwards, the way to think about that is the next $1 trillion of the world's infrastructure will clearly be different than the last $1 trillion, and it will be vastly accelerated. With respect to the shape of our ramp, we offer multiple configurations of Blackwell. Blackwell comes in either a Blackwell classic, if you will, that uses the HGX form factor that we pioneered with Volta.\n\nAnd I think it was Volta. And so, we've been shipping the HGX form factor for some time. It is air-cooled. The Grace Blackwell is liquid-cooled.\n\nHowever, the number of data centers that want to go to liquid-cooled is quite significant. And the reason for that is because we can, in a liquid-cooled data center, in any data center -- power-limited data center, whatever size data center you choose, you could install and deploy anywhere from three to five times the AI throughput compared to the past. And so, liquid cooling is cheaper. Liquid cooling, our TCO is better, and liquid cooling allows you to have the benefit of this capability we call NVLink, which allows us to expand it to 72 Grace Blackwell packages, which has essentially 144 GPUs.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 28
  },
  {
    "text": "ng, our TCO is better, and liquid cooling allows you to have the benefit of this capability we call NVLink, which allows us to expand it to 72 Grace Blackwell packages, which has essentially 144 GPUs.\n\nAnd so, imagine 144 GPUs connected in NVLink. And that, we're increasingly showing you the benefits of that. And the next click is obviously very low latency, very high throughput large language model inference, and the large NVLink domain is going to be a game changer for that. And so, I think people are very comfortable deploying both.\n\nAnd so, almost every CSP we're working with are deploying some of both. And so, I'm pretty confident that we'll ramp it up just fine. Your second question out of the third is that looking forward, yes, next year is going to be a great year. We expect to grow our Data Center business quite significantly next year.\n\nBlackwell is going to be a complete game changer for the industry. And Blackwell is going to carry into the following year. And as I mentioned earlier, working backwards from first principles, remember that computing is going through two platform transitions at the same time. And that's just really, really important to keep your head on -- your mind focused on, which is general-purpose computing is shifting to accelerated computing, and human-engineered software is going to transition to generative AI or artificial intelligence-learned software.\n\nOK.\n\nOperator\n\nAnd your next question comes from the line of Stacy Rasgon with Bernstein Research. Your line is open.\n\nStacy Rasgon -- Analyst",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 29
  },
  {
    "text": "nerative AI or artificial intelligence-learned software.\n\nOK.\n\nOperator\n\nAnd your next question comes from the line of Stacy Rasgon with Bernstein Research. Your line is open.\n\nStacy Rasgon -- Analyst\n\nHi, guys. Thanks for taking my question. So, I have two short questions for Colette. The first several billion dollars of Blackwell revenue in Q4, is that additive? You said you expected Hopper demand to strengthen in the second half.\n\nDoes that mean Hopper strengthens Q3 to Q4 as well on top of Blackwell adding several billion dollars? And the second question on gross margins. If I have mid-70s for the year, let's say, where I want to draw that, if I have 75 for the year, I'd be something like 71 to 72 for Q4, somewhere in that range. Is that the kind of exit rate for gross margins that you're expecting? And how should we think about the drivers of gross margin evolution into next year as Blackwell ramps? And I mean, hopefully, I guess the yields and the inventory reserves and everything come up.\n\nColette M. Kress -- Chief Financial Officer, Executive Vice President\n\nSo, Stacy, let's first take your question that you had about Hopper and Blackwell. So, we believe our Hopper will continue to grow into the second half. We have many new products for Hopper, our existing products for Hopper that we believe will start continuing to ramp in the next quarters, including our Q3 and those new products moving to Q4. So, let's say, Hopper there for versus H1 is a growth opportunity for that.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 30
  },
  {
    "text": "er that we believe will start continuing to ramp in the next quarters, including our Q3 and those new products moving to Q4. So, let's say, Hopper there for versus H1 is a growth opportunity for that.\n\nAdditionally, we have the Blackwell on top of that, and the Blackwell starting ramping in Q4. So, I hope that helps you on those two pieces. Your second piece is in terms of our gross margin. We provided gross margin for our Q3.\n\nWe provided our gross margin on a non-GAAP at about 75. We'll work with all the different transitions that we're going through, but we do believe we can do that 75 in Q3. We provided that we're still on track for the full year also in the mid-70s or approximately the 75. So, we're going to see some slight difference possibly in Q4, again with our transitions and the different cost structures that we have on our new product introductions.\n\nHowever, I'm not in the same number that you are there. We don't have exactly guidance, but I do believe you're lower than where we are.\n\nOperator\n\nAnd your next question comes from the line of Ben Reitzes with Melius. Your line is open.\n\nBen Reitzes -- Melius Research -- Analyst\n\nYeah. Hey, thanks a lot for the question. I wanted to ask about the geographies. There was the 10-Q that came out, and the United States was down sequentially while several Asian geographies were up a lot sequentially.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 31
  },
  {
    "text": "a lot for the question. I wanted to ask about the geographies. There was the 10-Q that came out, and the United States was down sequentially while several Asian geographies were up a lot sequentially.\n\nJust wondering what the dynamics are there. And obviously, China did very well. You mentioned it in your remarks. What are the puts and takes? And then I just wanted to clarify from Stacy's question if that means the sequential overall revenue growth rates for the company accelerate in the fourth quarter, given all those favorable revenue dynamics.\n\nThanks.\n\nColette M. Kress -- Chief Financial Officer, Executive Vice President\n\nLet me talk about a bit in terms of our disclosure in terms of the 10-Q, a required disclosure in a choice of geographies. Very challenging sometimes to create that right disclosure as we have to come up with one key piece. The pieces in terms of we have in terms of who we sell to and/or specifically who we invoice to, and so what you're seeing in terms of there is who we invoice. That's not necessarily where the product will eventually be and where it may even travel to the end customer.\n\nThese are just moving to our OEMs or ODMs and our system integrators for the most part across our product portfolio. So, what you're seeing there is sometimes just a swift shift in terms of who they are using to complete their full configuration before those things are going into the data center, going into notebooks and those pieces of it. And that shift happens from time to time. But yes, our China number there are invoicing to China.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 32
  },
  {
    "text": "ation before those things are going into the data center, going into notebooks and those pieces of it. And that shift happens from time to time. But yes, our China number there are invoicing to China.\n\nKeep in mind that is incorporating both gaming, also Data Center, also automotive in those numbers that we have. Going back to your statement and regarding gross margin and also what we're seeing in terms of what we're looking at for Hopper and Blackwell in terms of revenue. Hopper will continue to grow in the second half. We'll continue to grow from what we are currently seeing.\n\nDetermining that exact mix in each Q3 and Q4, we don't have here. We are not here to guide yet in terms of Q4. But we do see right now the demand expectations. We do see the visibility that that will be a growth opportunity in Q4.\n\nOn top of that, we will have our Blackwell architecture.\n\nOperator\n\nAnd your next question comes from the line of C.J. Muse with Cantor Fitzgerald. Your line is open.\n\nC.J. Muse -- Analyst\n\nYeah. Good afternoon. Thank you for taking the question. You've embarked on a remarkable annual product cadence with challenges only likely becoming more and more, given rising complexity in a rather limit advanced package world.\n\nSo, curious, if you take a step back, how does this backdrop alter your thinking around potentially greater vertical integration, supply chain partnerships, and then taking through a consequential impact to your margin profile? Thank you.\n\nJensen Huang -- President and Chief Executive Officer",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 33
  },
  {
    "text": "tially greater vertical integration, supply chain partnerships, and then taking through a consequential impact to your margin profile? Thank you.\n\nJensen Huang -- President and Chief Executive Officer\n\nYeah. Thanks. Let's see. I think the first answer to your -- the answer to your first question is that the reason why our velocity is so high is simultaneously because the complexity of the model is growing, and we want to continue to drive its cost down.\n\nIt's growing so we want to continue to increase its scale. And we believe that by continuing to scale the AI models, that we'll reach a level of extraordinary usefulness and that it would open up, realize the next industrial revolution. We believe it. And so, we're going to drive ourselves really hard to continue to go up that scale.\n\nWe have the ability, fairly uniquely, to integrate, to design an AI factory because we have all the parts. It's not possible to come up with a new AI factory every year unless you have all the parts. And so, we have -- next year, we're going to ship a lot more CPUs than we've ever had in the history of our company, more GPUs, of course, but also NVLink switches, CX DPUs, ConnectX for East and West, BlueField DPUs for North and South, and data and storage processing to InfiniBand for supercomputing centers, to Ethernet, which is a brand-new product for us, which is well on its way to becoming a multibillion-dollar business to bring AI to Ethernet. And so, the fact that we could build -- we have access to all of this, we have one architectural stack, as you know, it allows us to introduce new capabilities to the market as we complete it.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 34
  },
  {
    "text": "hernet. And so, the fact that we could build -- we have access to all of this, we have one architectural stack, as you know, it allows us to introduce new capabilities to the market as we complete it.\n\nOtherwise, what happens, you ship these parts, you go find customers to sell it to, and then you've got to build -- somebody's got to build up an AI factory, and the AI factory has got a mountain of software. And so, it's not about who integrates it. We love the fact that our supply chain is disintegrated in the sense that we could service Quanta, Foxconn, HP, Dell, Lenovo, Super Micro. We used to be able to serve ZTE.\n\nThey were recently purchased and so on and so forth. And so, the number of ecosystem partners that we have, Gigabyte, the number of ecosystem partners that we have that allows them to take our architecture, which all works, but integrated in a bespoke way into all of the world's cloud service providers, enterprise data centers, the scale and reach necessary from our ODMs and our integrators, integrated supply chain, is vast and gigantic because the world is huge. And so, that part, we don't want to do and we're not good at doing. And -- but we know how to design the AI infrastructure, provided the way that customers would like it and lets the ecosystem integrate it.\n\nWell, yes. So, anyways, that's the reason why.\n\nOperator\n\nAnd your final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\n\nAaron Rakers -- Analyst",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 35
  },
  {
    "text": "ystem integrate it.\n\nWell, yes. So, anyways, that's the reason why.\n\nOperator\n\nAnd your final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\n\nAaron Rakers -- Analyst\n\nYes. Thanks for taking the question. I wanted to go back into the Blackwell product cycle. One of the questions that we tend to get asked is how you see the rack scale system mix dynamic as you think about leveraging NVLink, you think about GB NVL72 and how that go-to-market dynamic looks as far as the Blackwell product cycle.\n\nI guess to put it simply, how do you see that mix of rack scale systems as we start to think about the Blackwell cycle playing out?\n\nJensen Huang -- President and Chief Executive Officer\n\nYeah. Aaron, thanks. The Blackwell rack system, it's designed and architected as a rack but it's sold in disaggregated system components. We don't sell the whole rack.\n\nAnd the reason for that is because everybody's rack's a little different surprisingly. Some of them are OCP standards, some of them are not. Some of them are enterprise. And the power limits for everybody could be a little different.\n\nChoice of CDUs, the choice of power bus bars, the configuration and integration into people's data centers, all different. And so, the way we designed it, we architected the whole rack. The software is going to work perfectly across the whole rack. And then we provide the system components.\n\nLike for example, the CPU and GPU compute board is then integrated into an MGX. It's a modular system architecture. MGX is completely ingenious. And we have MGX ODMs and integrators and OEMs all over the plant.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 36
  },
  {
    "text": "xample, the CPU and GPU compute board is then integrated into an MGX. It's a modular system architecture. MGX is completely ingenious. And we have MGX ODMs and integrators and OEMs all over the plant.\n\nAnd so, just about any configuration you would like, where you would like that 3,000-pound rack to be delivered, it's got to be close to. It has to be integrated and assembled close to the data center because it's fairly heavy. And so everything from the supply chain from the moment that we ship the GPU, CPUs, the switches, the NICs, from that point forward, the integration is done quite close to the location of the CSPs and the locations of the data centers. And so, you can imagine how many data centers in the world there are and how many logistics hubs we've scaled out to with our ODM partners.\n\nAnd so, I think because we show it as one rack and because it's always rendered that way and shown that way, we might have left the impression that we're doing the integration. Our customers hate that we do integration. The supply chain hates us doing integration. They want to do the integration.\n\nThat's their value-add. There's a final design-in, if you will. It's not quite as simple as shimmy into a data center but the design fit-in is really complicated. And so, the design fit-in, the installation, the bring-up, the repair and replace, that entire cycle is done all over the world.\n\nAnd we have a sprawling network of ODM and OEM partners that does this incredibly well. So, integration is not the reason why we're doing racks. It's the anti-reason of doing it. The way we don't want to be an integrator, we want to be a technology provider.\n\nOperator",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 37
  },
  {
    "text": "is incredibly well. So, integration is not the reason why we're doing racks. It's the anti-reason of doing it. The way we don't want to be an integrator, we want to be a technology provider.\n\nOperator\n\nAnd I will now turn the call back over to Jensen Huang for closing remarks.\n\nJensen Huang -- President and Chief Executive Officer\n\nThank you. Let me make a couple more -- make a couple of comments that I made earlier again. The data center worldwide are in full steam to modernize the entire computing stack with accelerated computing and generative AI. Hopper demand remains strong and the anticipation for Blackwell is incredible.\n\nLet me highlight the top five things, the top five things of our company. Accelerated computing has reached the tipping point. CPU scaling slows. Developers must accelerate everything possible.\n\nAccelerated computing starts with CUDA-X libraries. New libraries open new markets for NVIDIA. We released many new libraries, including CUDA-X Accelerated Polars, Pandas, and Spark, the leading data science and data processing libraries, CUVI-S for vector databases. This is incredibly hot right now.\n\nAriel and for 5G wireless base station, a whole suite of a whole world of data centers that we can go into now. Parabricks for gene sequencing and AlphaFold2 for protein structure prediction is now CUDA accelerated. We are at the beginning of our journey to modernize $1 trillion worth of data centers from general-purpose computing to accelerated computing. That's number one.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 38
  },
  {
    "text": "re prediction is now CUDA accelerated. We are at the beginning of our journey to modernize $1 trillion worth of data centers from general-purpose computing to accelerated computing. That's number one.\n\nNumber two, Blackwall is a step-function leap over Hopper. Blackwell is an AI infrastructure platform, not just the GPU. Also happens to be the name of our GPU but it's an AI infrastructure platform. As we reveal more of Blackwell and sample systems to our partners and customers, the extent of Blackwell's lead becomes clear.\n\nThe Blackwell vision took nearly five years and seven one-of-a-kind chips to realize, the Gray CPU, the Blackwell dual GPU, and a colos package, ConnectX DPU for East-West traffic, BlueField DPU for North-South and storage traffic, NVLink switch for all-to-all GPU communications, and Quantum and Spectrum-X for both InfiniBand and Ethernet can support the massive traffic of AI. Blackwell AI factories are building-size computers. NVIDIA designed and optimized the Blackwell platform, full stack end to end, from chips, systems, networking, even structured cables, power and cooling, and mounds of software to make it fast for customers to build AI factories. These are very capital-intensive infrastructures.\n\nCustomers want to deploy it as soon as they get their hands on the equipment and deliver the best performance and TCO. Blackwell provides three to five times more AI throughput in a power-limited data center than Hopper. The third is NVLink. This is a very big deal with its all-to-all GPU switch is game-changing.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 39
  },
  {
    "text": "TCO. Blackwell provides three to five times more AI throughput in a power-limited data center than Hopper. The third is NVLink. This is a very big deal with its all-to-all GPU switch is game-changing.\n\nThe Blackwell system lets us connect 144 GPUs in 72 GB200 packages into one NVLink domain, with an aggregate NVLink bandwidth of 259 terabytes per second in one rack. Just to put that in perspective, that's about 10x higher than Hopper. 259 terabytes per second kind of makes sense because you need to boost the training of multitrillion-parameter models on trillions of tokens. And so, that natural amount of data needs to be moved around from GPU to GPU.\n\nFor inference, NVLink is vital for low-latency, high-throughput large language model token generation. We now have three networking platforms, NVLink for GPU scale-up, Quantum InfiniBand for supercomputing and dedicated AI factories, and Spectrum-X for AI on Ethernet. NVIDIA's networking footprint is much bigger than before. Generative AI momentum is accelerating.\n\nGenerative AI frontier model makers are racing to scale to the next AI plateau to increase model safety and IQ. We're also scaling to understand more modalities from text, images, and video to 3D physics, chemistry, and biology. Chatbots, coding AIs, and image generators are growing fast but it's just the tip of the iceberg. Internet services are deploying generative AI for large-scale recommenders, ad targeting, and search systems.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 40
  },
  {
    "text": "s, coding AIs, and image generators are growing fast but it's just the tip of the iceberg. Internet services are deploying generative AI for large-scale recommenders, ad targeting, and search systems.\n\nAI start-ups are consuming tens of billions of dollars yearly of CSP's cloud capacity, and countries are recognizing the importance of AI and investing in sovereign AI infrastructure. And NVIDIA AI, NVIDIA Omniverse is opening up the next era of AI, general robotics. And now the enterprise AI wave has started, and we're poised to help companies transform their businesses. The NVIDIA AI Enterprise platform consists of Nemo, NIMs, NIM Agent Blueprints, and AI Foundry that our ecosystem partners, the world-leading IT companies used to help companies customize AI models and build bespoke AI applications.\n\nEnterprises can then deploy on NVIDIA AI Enterprise run time, and at $4,500 per GPU per year, NVIDIA AI Enterprise is an exceptional value for deploying AI anywhere. And for NVIDIA software, TAM can be significant as the CUDA-compatible GPU installed base grows from millions to tens of millions. And as Colette mentioned, NVIDIA software will exit the year at a $2 billion run rate. Thank you all for joining us today.\n\nOperator\n\n[Operator signoff]\n\nDuration: 0 minutes\nCall participants:\n\nStewart Stecker -- Senior Director, Investor Relations\n\nColette M. Kress -- Chief Financial Officer, Executive Vice President\n\nVivek Arya -- Analyst\n\nJensen Huang -- President and Chief Executive Officer\n\nToshiya Hari -- Analyst\n\nColette Kress -- Chief Financial Officer, Executive Vice President\n\nJoe Moore -- Analyst\n\nMatt Ramsay -- Analyst\n\nTimothy Arcuri -- Analyst\n\nStacy Rasgon -- Analyst",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 41
  },
  {
    "text": "e Officer\n\nToshiya Hari -- Analyst\n\nColette Kress -- Chief Financial Officer, Executive Vice President\n\nJoe Moore -- Analyst\n\nMatt Ramsay -- Analyst\n\nTimothy Arcuri -- Analyst\n\nStacy Rasgon -- Analyst\n\nBen Reitzes -- Melius Research -- Analyst\n\nC.J. Muse -- Analyst\n\nAaron Rakers -- Analyst\n\nMore NVDA analysis\n\nAll earnings call transcripts\n\nStocks Mentioned\nNvidia Stock Quote\nNvidia\n$178.73 (0.01%) $1.91\n\nMotley Fool Stock Advisor's Latest Pick\nGet Access\n982% Avg Return*\n\n*Average returns of all recommendations since inception. Cost basis and return based on previous market day close.\n\nRelated Articles\nbest quantum computing stocks ionq rgti qbts ibm goog\nQuantum Computing Stocks: How the Quantum Computing Players Stack Up by Patents (Yes, Nvidia Has Such Patents)\nGettyImages-1146642361-7360x4912-13a0741\nUnderstanding Michael Burry's Bet Against AI: Here's What it Really Means for Investors\nnvidia headquarters with nvidia sign in front (1)\nIf You'd Invested $10 Million in Nvidia Stock 10 Years Ago, Here's the Shocking Amount You'd Have Today\nGetty -- happy colleagues\nThe Best Stocks to Invest $50,000 in Right Now\nnvidia headquarters with grey nvidia sign in front with nvidia logo\nA Trump Policy Pivot Could Hand Nvidia Billions in AI Chip Sales -- If It Happens\n\nMotley Fool Returns\nMotley Fool Stock Advisor\n\nMarket-beating stocks from our flagship service.\nStock Advisor Returns\n982%\n188%\n\nCalculated by average return of all stock recommendations since inception of the Stock Advisor service in February of 2002. Returns as of 11/22/2025.\n\nDiscounted offers are only available to new members. Stock Advisor list price is $199 per year.",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 42
  },
  {
    "text": "mmendations since inception of the Stock Advisor service in February of 2002. Returns as of 11/22/2025.\n\nDiscounted offers are only available to new members. Stock Advisor list price is $199 per year.\n\nJoin Stock Advisor ›\nPremium Investing Services\n\nView Premium Services\n\nMaking the world smarter, happier, and richer.\n\nFacebook\n\nTwitter\nLinkedIn\nPinterest\nYouTube\nInstagram\nTikTok\n\nMarket data powered by Xignite and Polygon.io.\n\nAbout Us\nCareers\nResearch\nNewsroom\nContact\nAdvertise\n\nOur Services\n\nAll Services\nStock Advisor\nEpic\nEpic Plus\nFool Portfolios\nFool One\nMotley Fool Money\n\nAround the Globe\n\nFool UK\nFool Australia\nFool Canada\n\nFree Tools\n\nCAPS Stock Ratings\nDiscussion Boards\nCalculators\nFinancial Dictionary\n\nAffiliates & Friends\n\nMotley Fool Asset Management\nMotley Fool Wealth Management\nMotley Fool Ventures\nFool Community Foundation\nBecome an Affiliate Partner\n\nDisclosure Policy\nAccessibility Policy\nCopyright, Trademark and Patent Information\nTerms and Conditions\nDo Not Sell My Personal Information",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q2.txt",
    "chunk_id": 43
  },
  {
    "text": "NVIDIA Corporation (NVDA) Q3 FY2026 earnings call transcript\nPowered by Quartr\nNov 19, 2025, 5:00 PM EST\nEarnings call\n\nNVIDIA delivered Q3 FY26 revenue of $57B, up 62% YoY and 22% QoQ, with data center revenue hitting $51B (+66% YoY). Gross margins reached 73.6% (non-GAAP), and Q4 revenue is guided to $65B (+14% QoQ). Management reaffirmed its $500B Blackwell-Rubin pipeline through 2026 and expects continued strong demand across AI infrastructure.\nPowered by Yahoo Finance AI\nOperator\n\n0:00:00\n\nGood afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's third quarter earnings call. All lines have been placed on mute to prevent any background noise. After the speaker's remarks, there will be a question-and-answer session. If you would like to ask a question during this time, simply press star, followed by the number one on your telephone keypad. If you would like to withdraw your question, press star one again. Thank you. Toshiya Hari, you may begin your conference.\nToshiya Hari\nVP of Investor Relations\n\n0:00:33",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 0
  },
  {
    "text": " one on your telephone keypad. If you would like to withdraw your question, press star one again. Thank you. Toshiya Hari, you may begin your conference.\nToshiya Hari\nVP of Investor Relations\n\n0:00:33\n\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\nToshiya Hari\nVP of Investor Relations\n\n0:01:24",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 1
  },
  {
    "text": "ements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\nToshiya Hari\nVP of Investor Relations\n\n0:01:24\n\nFor a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 19, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\nColette Kress\nEVP and CFO\n\n0:02:08",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 2
  },
  {
    "text": " these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\nColette Kress\nEVP and CFO\n\n0:02:08\n\nThank you, Toshiya. We delivered another outstanding quarter with revenue of $57 billion, up 62% year over year, and a record sequential revenue growth of $10 billion, or 22%. Our customers continue to lean into three platform shifts, fueling exponential growth for accelerated computing, powerful AI models, and agentic applications. Yet, we are still in the early innings of these transitions that will impact our work across every industry. We currently have visibility to $500 billion in Blackwell and Rubin revenue from the start of this year through the end of calendar year 2026. By executing our annual product cadence and extending our performance leadership through full-stack design, we believe NVIDIA will be the superior choice for the $3 trillion-$4 trillion in annual AI infrastructure build we estimate by the end of the decade. Demand for AI infrastructure continues to exceed our expectations.\nColette Kress\nEVP and CFO\n\n0:03:18",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 3
  },
  {
    "text": "he $3 trillion-$4 trillion in annual AI infrastructure build we estimate by the end of the decade. Demand for AI infrastructure continues to exceed our expectations.\nColette Kress\nEVP and CFO\n\n0:03:18\n\nThe clouds are sold out, and our GPU-installed base, both new and previous generations, including Blackwell, Hopper, and Ampere, is fully utilized. Record Q3 data center revenue of $51 billion increased 66% year over year, a significant feat at our scale. Compute grew 56% year over year, driven primarily by the GB300 ramp, while networking more than doubled given the onset of NVLink scale-up and robust double-digit growth across Spectrum X Ethernet and Quantum X InfiniBand. The world hyperscalers, a trillion-dollar industry, are transforming search, recommendations, and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition, driving infrastructure investment measured in hundreds of billions of dollars. At Meta, AI recommendation systems are delivering higher quality and more relevant content, leading to more time spent on apps such as Facebook and Threads.\nColette Kress\nEVP and CFO\n\n0:04:35",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 4
  },
  {
    "text": "dollars. At Meta, AI recommendation systems are delivering higher quality and more relevant content, leading to more time spent on apps such as Facebook and Threads.\nColette Kress\nEVP and CFO\n\n0:04:35\n\nAnalyst expectations for the top CSPs and hyperscalers in 2026 aggregate CapEx have continued to increase and now sit roughly at $600 billion, more than $200 billion higher relative to the start of the year. We see the transition to accelerated computing and generative AI across current hyperscale workloads contributing toward roughly half of our long-term opportunity. Another growth pillar is the ongoing increase in compute spend driven by foundation model builders such as Anthropic, Mistral, OpenAI, Reflection, Safe Superintelligence, Thinking Machines Lab, and xAI, all scaling compute aggressively to scale intelligence. The three scaling laws, pre-training, post-training, and inference remain intact. In fact, we see a positive virtuous cycle emerging whereby the three scaling laws and access to compute are generating better intelligence and, in turn, increasing adoption and profits.\nColette Kress\nEVP and CFO\n\n0:05:49",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 5
  },
  {
    "text": "sitive virtuous cycle emerging whereby the three scaling laws and access to compute are generating better intelligence and, in turn, increasing adoption and profits.\nColette Kress\nEVP and CFO\n\n0:05:49\n\nOpenAI recently shared that their weekly user base has grown to $800 million, enterprise customers have increased to 1 million, and that their gross margins were healthy. While Anthropic recently reported that its annualized run rate revenue has reached $7 billion as of last month, up from $1 billion at the start of the year. We are also witnessing a proliferation of agentic AI across various industries and tasks. Companies such as Cursor, Anthropic, Open Evidence, Epic, and Abridge are experiencing a surge in user growth as they supercharge the existing workforce, delivering unquestionable ROI for coders and healthcare professionals. The world's most important enterprise software platforms like ServiceNow, CrowdStrike, and SAP are integrating NVIDIA's accelerated computing and AI stack. Our new partner, Palantir, is supercharging the incredibly popular ontology platform with NVIDIA CUDA X libraries and AI models for the first time.\nColette Kress\nEVP and CFO\n\n0:07:10",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 6
  },
  {
    "text": "ing and AI stack. Our new partner, Palantir, is supercharging the incredibly popular ontology platform with NVIDIA CUDA X libraries and AI models for the first time.\nColette Kress\nEVP and CFO\n\n0:07:10\n\nPreviously, like most enterprise software platforms, Ontology runs only on CPUs. Lowe's is leveraging the platform to build supply chain agility, reducing costs and improving customer satisfaction. Enterprises broadly are leveraging AI to boost productivity, increase efficiency, and reduce costs. RBC is leveraging agentic AI to drive significant analyst productivity, slashing report generation time from hours to minutes. AI and digital twins are helping Unilever accelerate content creation by 2x and cut costs by 50%. Salesforce's engineering team has seen at least a 30% productivity increase in new code development after adopting Cursor. This past quarter, we announced AI factory and infrastructure projects amounting to an aggregate of 5 million GPUs. This demand spans every market: CSPs, sovereigns, model builders, enterprises, and supercomputing centers, and includes multiple landmark buildouts.\nColette Kress\nEVP and CFO\n\n0:08:25",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 7
  },
  {
    "text": " 5 million GPUs. This demand spans every market: CSPs, sovereigns, model builders, enterprises, and supercomputing centers, and includes multiple landmark buildouts.\nColette Kress\nEVP and CFO\n\n0:08:25\n\nxAI's Colossus 2, the world's first gigawatt-scale data center, Lilly's AI factory for drug discovery, the pharmaceutical industry's most powerful data center. Just today, AWS and Humane expanded their partnership, including the deployment of up to 150,000 AI accelerators, including our GB300. xAI and Humane also announced a partnership in which the two will jointly develop a network of world-class GPU data centers anchored by the flagship 500-megawatt facility. Blackwell gained further momentum in Q3 as GB300 crossed over GB200 and contributed roughly two-thirds of the total Blackwell revenue. The transition to GB300 has been seamless, with production shipments to the major cloud service providers, hyperscalers, and GPU clouds, and is already driving their growth. The Hopper platform, in its 13th quarter since inception, recorded approximately $2 billion in revenue in Q3. H20 sales were approximately $50 million.\nColette Kress\nEVP and CFO\n\n0:09:50",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 8
  },
  {
    "text": "their growth. The Hopper platform, in its 13th quarter since inception, recorded approximately $2 billion in revenue in Q3. H20 sales were approximately $50 million.\nColette Kress\nEVP and CFO\n\n0:09:50\n\nSizable purchase orders never materialized in the quarter due to geopolitical issues and the increasingly competitive market in China. While we were disappointed in the current state that prevents us from shipping more competitive data center compute products to China, we are committed to continued engagement with the U.S. and China governments and will continue to advocate for America's ability to compete around the world. To establish a sustainable leadership position in AI computing, America must win the support of every developer and be the platform of choice for every commercial business, including those in China. The Rubin platform is on track to ramp in the second half of 2026. Powered by seven chips, the Vera Rubin platform will once again deliver an X-factor improvement in performance relative to Blackwell.\nColette Kress\nEVP and CFO\n\n0:10:51",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 9
  },
  {
    "text": "amp in the second half of 2026. Powered by seven chips, the Vera Rubin platform will once again deliver an X-factor improvement in performance relative to Blackwell.\nColette Kress\nEVP and CFO\n\n0:10:51\n\nWe have received silicon back from our supply chain partners and are happy to report that NVIDIA teams across the world are executing the bring-up beautifully. Rubin is our third-generation rack-scale system, substantially redefined the manufacturability while remaining compatible with Grace Blackwell. Our supply chain data center ecosystem and cloud partners have now mastered the build-to-installation process of NVIDIA's rack architecture. Our ecosystem will be ready for a fast Rubin ramp. Our annual X-factor performance leap increases performance per dollar while driving down computing costs for our customers. The long useful life of NVIDIA's CUDA GPUs is a significant TCO advantage over accelerators. CUDA's compatibility and our massive installed base extend the life of NVIDIA systems well beyond their original estimated useful life. For more than two decades, we have optimized the CUDA ecosystem, improving existing workloads, accelerating new ones, and increasing throughput with every software release.\nColette Kress\nEVP and CFO\n\n0:12:09",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 10
  },
  {
    "text": "e than two decades, we have optimized the CUDA ecosystem, improving existing workloads, accelerating new ones, and increasing throughput with every software release.\nColette Kress\nEVP and CFO\n\n0:12:09\n\nMost accelerators without CUDA and NVIDIA's time-tested and versatile architecture became obsolete within a few years as model technologies evolve. Thanks to CUDA, the A100 GPUs we shipped six years ago are still running at full utilization today, powered by vastly improved software stack. We have evolved over the past 25 years from a gaming GPU company to now an AI data center infrastructure company. Our ability to innovate across the CPU, the GPU, networking, and software, and ultimately drive down cost per token, is unmatched across the industry. Our networking business, purpose-built for AI and now the largest in the world, generated revenue of $8.2 billion, up 162% year over year, with NVLink, InfiniBand, and Spectrum X Ethernet all contributing to growth. We are winning in data center networking as the majority of AI deployments now include our switches with Ethernet GPU attach rates roughly on par with InfiniBand.\nColette Kress\nEVP and CFO\n\n0:13:31",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 11
  },
  {
    "text": "h. We are winning in data center networking as the majority of AI deployments now include our switches with Ethernet GPU attach rates roughly on par with InfiniBand.\nColette Kress\nEVP and CFO\n\n0:13:31\n\nMeta, Microsoft, Oracle, and xAI are building gigawatt AI factories with Spectrum X Ethernet switches, and each will run its operating system of choice, highlighting the flexibility and openness of our platform. We recently introduced Spectrum XGS, a scale-across technology that enables gigascale AI factories. NVIDIA is the only company with AI scale-up, scale-out, and scale-across platforms, reinforcing our unique position in the market as the AI infrastructure provider. Customer interest in NVLink Fusion continues to grow. We announced a strategic collaboration with Fujitsu in October, where we will integrate Fujitsu's CPUs and NVIDIA GPUs via NVLink Fusion, connecting our large ecosystems. We also announced a collaboration with Intel to develop multiple generations of custom data center and PC products, connecting NVIDIA and Intel's ecosystems using NVLink.\nColette Kress\nEVP and CFO\n\n0:14:42",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 12
  },
  {
    "text": "so announced a collaboration with Intel to develop multiple generations of custom data center and PC products, connecting NVIDIA and Intel's ecosystems using NVLink.\nColette Kress\nEVP and CFO\n\n0:14:42\n\nThis week at Supercomputing 25, Arm announced that it will be integrating NVLink IP for customers to build CPU SoCs that connect with NVIDIA. Currently on its fifth generation, NVLink is the only proven scale-up technology available on the market today. In the latest MLPerf training results, Blackwell Ultra delivered 5x faster time to train than Hopper. NVIDIA swept every benchmark. Notably, NVIDIA is the only training platform to leverage bridge FP4 while meeting MLPerf's strict accuracy standards. In semi-analysis inference max benchmark, Blackwell achieved the highest performance and lowest total cost of ownership across every model and use case. Particularly important is Blackwell's NVLink's performance on a mixture of experts, the architecture for the world's most popular reasoning models. On DeepSeek R1, Blackwell delivered 10x higher performance per watt and 10x lower cost per token versus H200, a huge generational leap fueled by our extreme code design approach.\nColette Kress\nEVP and CFO\n\n0:16:08",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 13
  },
  {
    "text": "1, Blackwell delivered 10x higher performance per watt and 10x lower cost per token versus H200, a huge generational leap fueled by our extreme code design approach.\nColette Kress\nEVP and CFO\n\n0:16:08\n\nNVIDIA Dynamo, an open-source, low-latency modular inference framework, has now been adopted by every major cloud service provider. Leveraging Dynamo's enablement and disaggregated inference, the resulting increase in performance of complex AI models such as MOE models, AWS, Google Cloud, Microsoft Azure, and OCI have boosted AI inference performance for enterprise cloud customers. We are working on a strategic partnership with OpenAI focused on helping them build and deploy at least 10 gigawatts of AI data centers. In addition, we have the opportunity to invest in the company. We serve OpenAI through their cloud partners, Microsoft Azure, OCI, and CoreWeave. We will continue to do so for the foreseeable future. As they continue to scale, we are delighted to support the company to add self-build infrastructure, and we are working toward a definitive agreement and are excited to support OpenAI's growth. Yesterday, we celebrated an announcement with Anthropic.\nColette Kress\nEVP and CFO\n\n0:17:25",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 14
  },
  {
    "text": "infrastructure, and we are working toward a definitive agreement and are excited to support OpenAI's growth. Yesterday, we celebrated an announcement with Anthropic.\nColette Kress\nEVP and CFO\n\n0:17:25\n\nFor the first time, Anthropic is adopting NVIDIA, and we are establishing a deep technology partnership to support Anthropic's fast growth. We will collaborate to optimize Anthropic models for CUDA and deliver the best possible performance, efficiency, and TCO. We will also optimize future NVIDIA architectures for Anthropic workloads. Anthropic's compute commitment is initially including up to 1 gigawatt of compute capacity with Grace Blackwell and Vera Rubin systems. Our strategic investments in Anthropic, Mistral, OpenAI, Reflection, Thinking Machines, and others represent partnerships that grow the NVIDIA CUDA AI ecosystem and enable every model to run optimally on NVIDIA's everywhere. We will continue to invest strategically while preserving our disciplined approach to cash flow management. Physical AI is already a multi-billion dollar business addressing a multi-trillion dollar opportunity and the next leg of growth for NVIDIA. Leading U.S.\nColette Kress\nEVP and CFO\n\n0:18:39",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 15
  },
  {
    "text": "management. Physical AI is already a multi-billion dollar business addressing a multi-trillion dollar opportunity and the next leg of growth for NVIDIA. Leading U.S.\nColette Kress\nEVP and CFO\n\n0:18:39\n\nManufacturers and robotics innovators are leveraging NVIDIA's three-computer architecture to train on NVIDIA, test on Omniverse computer, and deploy real-world AI on Jetson robotic computers. PTC and Siemens introduced new services that bring Omniverse-powered digital twin workflows to their extensive installed base of customers. Companies including Belden, Caterpillar, Foxconn, Lucid Motors, Toyota, TSMC, and Wistron are building Omniverse digital twin factories to accelerate AI-driven manufacturing and automation. Agility Robotics, Amazon Robotics, Figure, and Skilled at AI are building our platform, tapping offerings such as NVIDIA Cosmos World Foundation models for development, Omniverse for simulation and validation, and Jetson to power next-generation intelligent robots. We remain focused on building resiliency and redundancy in our global supply chain. Last month, in partnership with TSMC, we celebrated the first Blackwell wafer produced on U.S. soil.\nColette Kress\nEVP and CFO\n\n0:19:55",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 16
  },
  {
    "text": "n building resiliency and redundancy in our global supply chain. Last month, in partnership with TSMC, we celebrated the first Blackwell wafer produced on U.S. soil.\nColette Kress\nEVP and CFO\n\n0:19:55\n\nWe will continue to work with Foxconn, Wistron, Amcor, Spill, and others to grow our presence in the U.S. over the next four years. Gaming revenue was $4.3 billion, up 30% year-on-year, driven by strong demand as Blackwell momentum continued. End-market sell-through remains robust, and channel inventories are at normal levels heading into the holiday season. Steam recently broke its concurrent user record with 42 million gamers, while thousands of fans packed the GeForce Gamer Festival in South Korea to celebrate 25 years of GeForce. NVIDIA Pro Visualization has evolved into computers for engineers and developers, whether for graphics or for AI. Professional visualization revenue was $760 million, up 56% year-over-year, was another record. Growth was driven by DGX Spark, the world's smallest AI supercomputer built on a small configuration of Grace Blackwell. Automotive revenue was $592 million, up 32% year-over-year, primarily driven by self-driving solutions.\nColette Kress\nEVP and CFO\n\n0:21:18",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 17
  },
  {
    "text": "ercomputer built on a small configuration of Grace Blackwell. Automotive revenue was $592 million, up 32% year-over-year, primarily driven by self-driving solutions.\nColette Kress\nEVP and CFO\n\n0:21:18\n\nWe are partnering with Uber to scale the world's largest Level 4 ready autonomous fleet, built on the new NVIDIA Hyperion L4 Robotaxi reference architecture. Moving to the rest of the P&L, GAAP gross margins were 73.4%, and non-GAAP gross margins were 73.6%, exceeding our outlook. Gross margins increased sequentially due to our data center mix, improved cycle time, and cost structure. GAAP operating expenses were up 8% sequentially and up 11% on a non-GAAP basis. The growth was driven by infrastructure compute as well as higher compensation and benefits in engineering development costs. Non-GAAP effective tax rate for the third quarter was just over 17%, higher than our guidance of 16.5% due to the strong U.S. revenue. On our balance sheet, inventory grew 32% quarter over quarter, while supply commitments increased 63% sequentially.\nColette Kress\nEVP and CFO\n\n0:22:29",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 18
  },
  {
    "text": "guidance of 16.5% due to the strong U.S. revenue. On our balance sheet, inventory grew 32% quarter over quarter, while supply commitments increased 63% sequentially.\nColette Kress\nEVP and CFO\n\n0:22:29\n\nWe are preparing for significant growth ahead and feel good about our ability to execute against our opportunity set. Okay, let me turn to the outlook for the fourth quarter. Total revenue is expected to be $65 billion, plus or minus 2%. At the midpoint, our outlook implies 14% sequential growth driven by continued momentum in the Blackwell architecture. Consistent with last quarter, we are not assuming any data center compute revenue from China. GAAP and non-GAAP gross margins are expected to be 74.8% and 75% respectively, plus or minus 50 basis points. Looking ahead to fiscal year 2027, input costs are on the rise, but we are working to hold gross margins in the mid-70s. GAAP and non-GAAP operating expenses are expected to be approximately $6.7 billion and $5 billion respectively.\nColette Kress\nEVP and CFO\n\n0:23:41\n\nGAAP and non-GAAP other income and expenses are expected to be an income of approximately $500 million, excluding gains and losses from non-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. At this time, let me turn the call over to Jensen for him to say a few words.\nJensen Huang\nPresident and CEO\n\n0:24:09",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 19
  },
  {
    "text": " tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. At this time, let me turn the call over to Jensen for him to say a few words.\nJensen Huang\nPresident and CEO\n\n0:24:09\n\nThanks, Colette. There has been a lot of talk about an AI bubble. From our vantage point, we see something very different. As a reminder, NVIDIA is unlike any other accelerator. We excel at every phase of AI, from pre-training and post-training to inference. With our two-decade investment in CUDA X acceleration libraries, we are also exceptional at science and engineering simulations, computer graphics, structured data processing to classical machine learning.\nJensen Huang\nPresident and CEO\n\n0:24:54",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 20
  },
  {
    "text": "leration libraries, we are also exceptional at science and engineering simulations, computer graphics, structured data processing to classical machine learning.\nJensen Huang\nPresident and CEO\n\n0:24:54\n\nThe world is undergoing three massive platform shifts at once, the first time since the dawn of Moore's Law. NVIDIA is uniquely addressing each of the three transformations. The first transition is from CPU general-purpose computing to GPU accelerated computing as Moore's Law slows. The world has a massive investment in non-AI software, from data processing to science and engineering simulations, representing hundreds of billions of dollars in compute cloud computing spend each year. Many of these applications, which ran once exclusively on CPUs, are now rapidly shifting to CUDA GPUs. Accelerated computing has reached a tipping point. Secondly, AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones. For existing applications, generative AI is replacing classical machine learning in search ranking, recommender systems, ad targeting, click-through prediction to content moderation, the very foundations of hyperscale infrastructure.\nJensen Huang\nPresident and CEO\n\n0:26:26",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 21
  },
  {
    "text": "earning in search ranking, recommender systems, ad targeting, click-through prediction to content moderation, the very foundations of hyperscale infrastructure.\nJensen Huang\nPresident and CEO\n\n0:26:26\n\nMeta's Gem, a foundation model for ad recommendations trained on large-scale GPU clusters, exemplifies this shift. In Q2, Meta reported over a 5% increase in ad conversions on Instagram and 3% gain on Facebook feed, driven by generative AI-based Gem. Transitioning to generative AI represents substantial revenue gains for hyperscalers. Now, a new wave is rising: agentic AI systems capable of reasoning, planning, and using tools. From coding assistants like Cursor and Claude Code to radiology tools like iDoc, legal assistants like Harvey, and AI chauffeurs like Tesla FSD and Waymo, these systems mark the next frontier of computing. The fastest-growing companies in the world today—OpenAI, Anthropic, xAI, Google, Cursor, Lovable, Replet, Cognition AI, Open Evidence, Abridge, Tesla—are pioneering agentic AI. There are three massive platform shifts. The transition to accelerated computing is foundational and necessary, essential in a post-Moore's Law era.\nJensen Huang\nPresident and CEO\n\n0:27:57",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 22
  },
  {
    "text": "agentic AI. There are three massive platform shifts. The transition to accelerated computing is foundational and necessary, essential in a post-Moore's Law era.\nJensen Huang\nPresident and CEO\n\n0:27:57\n\nThe transition to generative AI is transformational and necessary, supercharging existing applications and business models. The transition to agentic and physical AI will be revolutionary, giving rise to new applications, companies, products, and services. As you consider infrastructure investments, consider these three fundamental dynamics. Each will contribute to infrastructure growth in the coming years. NVIDIA is chosen because our singular architecture enables all three transitions, and thus so for any form and modality of AI across all industries, across every phase of AI, across all of the diverse computing needs in a cloud, and also from cloud to enterprise to robots. One architecture. Toshiya, back to you.\nToshiya Hari\nVP of Investor Relations\n\n0:29:02\n\nWe will now open the call for questions.\nOperator\n\n0:29:06\n\nOperator, would you please pull for questions? Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\nOperator\n\n0:29:19\n\nWe'll pause for just a moment to compile the Q&A roster. As a reminder, please limit yourself to one question. Thank you. Your first question comes from Joseph Moore with Morgan Stanley. Your line is open.\nJoseph Moore\nSemiconductor Industry Analyst\n\n0:29:34",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 23
  },
  {
    "text": "s a reminder, please limit yourself to one question. Thank you. Your first question comes from Joseph Moore with Morgan Stanley. Your line is open.\nJoseph Moore\nSemiconductor Industry Analyst\n\n0:29:34\n\nGreat. Thank you. I wonder if you could update us. You talked about the $500 billion of revenue for Blackwell plus Rubin in 2025 and 2026 at GTC. At that time, you had talked about $150 billion of that already having been shipped. As the quarter's wrapped up, are those still kind of the general parameters that there's $350 billion in the next kind of 14 months or so? I would assume over that time, you haven't seen all the demand, but there is any possibility of upside to those numbers as we move forward.\nColette Kress\nEVP and CFO\n\n0:30:05\n\nYeah. Thanks, Joe. I'll start first with a response here on that. Yes, that's correct.\nColette Kress\nEVP and CFO\n\n0:30:12\n\nWe are working into our $500 billion forecast, and we are on track for that as we have finished some of the quarters. We have several quarters now in front of us to take us through the end of calendar year 2026. The number will grow, and we will achieve, I'm sure, additional needs for compute that will be shippable by fiscal year 2026. We shipped $50 billion this quarter, but we would be not finished if we did not say that we will probably be taking more orders. For example, just even today, our announcements with KSA and that agreement in itself is 400,000-600,000 more GPUs over three years. Anthropic is also not new. There is definitely an opportunity for us to have more on top of the $500 billion that we announced.\nOperator\n\n0:31:09\n\nThe next question comes from C.J. Muse with Cantor Fitzgerald.\nOperator\n\n0:31:20",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 24
  },
  {
    "text": "There is definitely an opportunity for us to have more on top of the $500 billion that we announced.\nOperator\n\n0:31:09\n\nThe next question comes from C.J. Muse with Cantor Fitzgerald.\nOperator\n\n0:31:20\n\nYour line is open.\nCJ Muse\nSenior Managing Director\n\n0:31:21\n\nYeah. Good afternoon. Thank you for taking the question. There's clearly a great deal of consternation around the magnitude of AI infrastructure buildouts and the ability to fund such plans and the ROI. Yet at the same time, you're talking about being sold out. Every stood-up GPU is taken. The AI world hasn't seen the enormous benefit yet from B300, never mind Rubin. Gemini 3 just announced Grok 5 coming soon. The question is this: when you look at that as the backdrop, do you see a realistic path for supply to catch up with demand over the next 12 to 18 months, or do you think it can extend beyond that timeframe?\nJensen Huang\nPresident and CEO\n\n0:31:59\n\nAs you know, we've done a really good job planning our supply chain. NVIDIA's supply chain basically includes every technology company in the world.\nJensen Huang\nPresident and CEO\n\n0:32:15",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 25
  },
  {
    "text": "O\n\n0:31:59\n\nAs you know, we've done a really good job planning our supply chain. NVIDIA's supply chain basically includes every technology company in the world.\nJensen Huang\nPresident and CEO\n\n0:32:15\n\nTSMC and their packaging and our memory vendors and memory partners and all of our system ODMs have done a really good job planning with us. We were planning for a big year. We've seen for some time the three transitions that I spoke about just a second ago: accelerated computing from general-purpose computing. It's really important to recognize that AI is not just agentic AI, but generative AI is transforming the way that hyperscalers did the work that they used to do on CPUs. Generative AI made it possible for them to move search and recommender systems and add recommendations and targeting. All of that has been moved to generative AI and is still transitioning.\nJensen Huang\nPresident and CEO\n\n0:33:05\n\nWhether you installed NVIDIA GPUs for data processing, or you did it for generative AI for your recommender system, or you're building it for agentic chatbots and the type of AIs that most people see when they think about AI, all of those applications are accelerated by NVIDIA. When you look at the totality of the spend, it's really important to think about each one of those layers. They're all growing. They're related, but not the same. The wonderful thing is that they all run on NVIDIA GPUs. Simultaneously, because the quality of the AI models are improving so incredibly, the adoption of it in the different use cases, whether it's in code assistance, which NVIDIA uses fairly exhaustively, and we're not the only one.\nJensen Huang\nPresident and CEO\n\n0:33:59",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 26
  },
  {
    "text": "o incredibly, the adoption of it in the different use cases, whether it's in code assistance, which NVIDIA uses fairly exhaustively, and we're not the only one.\nJensen Huang\nPresident and CEO\n\n0:33:59\n\nI mean, the fastest-growing application in history, a combination of Cursor and Claude Code and OpenAI's Codex and GitHub Copilot, these applications are the fastest-growing in history. It's not just used for software engineers. It's used because of vibe coding. It's used by engineers and marketeers all over companies, supply chain planners all over companies. I think that that's just one example, and the list goes on, whether it's open evidence and the work that they do in healthcare or the work that's being done in digital video editing, runway. I mean, the number of really, really exciting startups that are taking advantage of generative AI and agentic AI is growing quite rapidly. Not to mention, we're all using it a lot more.\nJensen Huang\nPresident and CEO\n\n0:34:51\n\nAll of these exponentials, not to mention, just today, I was reading a text from Demis, and he was saying that pre-training and post-training are fully intact. Gemini 3 takes advantage of the scaling laws and got a received a huge jump in quality performance and model performance. We are seeing all of these exponentials kind of running at the same time. I just always go back to first principles and think about what's happening from each one of the dynamics that I mentioned before: general-purpose computing to accelerated computing, generative AI replacing classical machine learning, and of course, agentic AI, which is a brand new category.\nOperator\n\n0:35:35",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 27
  },
  {
    "text": " I mentioned before: general-purpose computing to accelerated computing, generative AI replacing classical machine learning, and of course, agentic AI, which is a brand new category.\nOperator\n\n0:35:35\n\nThe next question comes from Vivek Aria with Bank of America Securities. Your line is open.\nVivek Arya\nManaging Director and Senior Analyst\n\n0:35:47\n\nThanks for taking my question. I'm curious, what assumptions are you making on NVIDIA content per gigawatt in that $500 billion number?\nVivek Arya\nManaging Director and Senior Analyst\n\n0:35:57\n\nBecause we have heard numbers as low as $25 billion per gigawatt of content to as high as $30 or $40 billion per gigawatt. I am curious what power and what dollar per gigawatt assumptions you are making as part of that $500 billion number. Longer-term, Jensen, the $3 to $4 trillion in data center by 2030 was mentioned. How much of that do you think will require vendor financing, and how much of that can be supported by cash flows of your large customers or governments or enterprises? Thank you.\nJensen Huang\nPresident and CEO\n\n0:36:31\n\nIn each generation, from Ampere to Hopper, from Hopper to Blackwell, Blackwell to Rubin, our part of the data center increases. Hopper generation was probably something along the lines of 20-somewhat, 20-25. Blackwell generation, Grace Blackwell particularly, is probably 30-30, say 30 plus or minus.\nJensen Huang\nPresident and CEO\n\n0:37:05",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 28
  },
  {
    "text": "ation was probably something along the lines of 20-somewhat, 20-25. Blackwell generation, Grace Blackwell particularly, is probably 30-30, say 30 plus or minus.\nJensen Huang\nPresident and CEO\n\n0:37:05\n\nRubin is probably higher than that. In each one of these generations, the speedup is X factors. Therefore, their TCO, the customer TCO, improves by X factors. The most important thing is, in the end, you still only have one gigawatt of power, one gigawatt data centers, one gigawatt of power. Therefore, performance per watt, the efficiency of your architecture, is incredibly important. The efficiency of your architecture can't be brute forced. There is no brute forcing about it. That one gigawatt translates directly, your performance per watt translates directly, absolutely directly to your revenues, which is the reason why choosing the right architecture matters so much now. The world doesn't have an excess of anything to squander.\nJensen Huang\nPresident and CEO\n\n0:38:00",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 29
  },
  {
    "text": "irectly to your revenues, which is the reason why choosing the right architecture matters so much now. The world doesn't have an excess of anything to squander.\nJensen Huang\nPresident and CEO\n\n0:38:00\n\nWe have to be really, really—we use this concept called co-design across our entire stack, across the frameworks and models, across the entire data center, even power and cooling optimized across the entire supply chain in our ecosystem. Each generation, our economic contribution will be greater. Our value delivered will be greater. The most important thing is our energy efficiency per watt is going to be extraordinary every single generation. With respect to growing into continuing to grow, our customers' financing is up to them. We see the opportunity to grow for quite some time. Remember, today, most of the focus has been on the hyperscalers. One of the areas that is really misunderstood about the hyperscalers is that the investment on NVIDIA GPUs not only improves their scale, speed, and cost from general-purpose computing—that is number one—because Moore's Law scaling has really slowed.\nJensen Huang\nPresident and CEO\n\n0:39:16",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 30
  },
  {
    "text": "t on NVIDIA GPUs not only improves their scale, speed, and cost from general-purpose computing—that is number one—because Moore's Law scaling has really slowed.\nJensen Huang\nPresident and CEO\n\n0:39:16\n\nMoore's Law is about driving cost down. It's about deflationary cost, the incredible deflationary cost of computing over time. That has slowed. Therefore, a new approach is necessary for them to keep driving the cost down. Going to NVIDIA GPU computing is really the best way to do so. The second is revenue boosting in their current business models. Recommender systems drive the world's hyperscalers every single, whether it's watching short-form videos or recommending books or recommending the next item in your basket to recommending ads to recommending news to—it's all about recommenders. The internet has trillions of pieces of content. How could they possibly figure out what to put in front of you and your little tiny screen unless they have really sophisticated recommender systems to do so? That has gone generative AI.\nJensen Huang\nPresident and CEO\n\n0:40:13",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 31
  },
  {
    "text": "ure out what to put in front of you and your little tiny screen unless they have really sophisticated recommender systems to do so? That has gone generative AI.\nJensen Huang\nPresident and CEO\n\n0:40:13\n\nThe first two things that I've just said, hundreds of billions of dollars of CapEx is going to have to be invested, is fully cash flow funded. What is above it, therefore, is agentic AI. This is net new, net new consumption, but it's also net new applications. And some of the applications I mentioned before, but these new applications are also the fastest-growing applications in history. Okay? I think that you're going to see that once people start to appreciate what is actually happening under the water, if you will, from the simplistic view of what's happening to CapEx investment, recognizing there's these three dynamics. Lastly, remember, we were just talking about the American CSPs. Each country will fund their own infrastructure. You have multiple countries. You have multiple industries.\nJensen Huang\nPresident and CEO\n\n0:41:10",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 32
  },
  {
    "text": "ember, we were just talking about the American CSPs. Each country will fund their own infrastructure. You have multiple countries. You have multiple industries.\nJensen Huang\nPresident and CEO\n\n0:41:10\n\nMost of the world's industries haven't really engaged agentic AI yet, and they're about to. All the names of companies that you know we're working with, whether it's autonomous vehicle companies or digital twins for physical AI for factories and the number of factories and warehouses being built around the world, just the number of digital biology startups that are being funded so that we could accelerate drug discovery. All of those different industries are now getting engaged, and they're going to do their own fundraising. Do not just look at the hyperscalers as a way to build out for the future. You got to look at the world. You got to look at all the different industries. Enterprise computing is going to fund their own industry.\nOperator\n\n0:41:55\n\nThe next question comes from Ben Ritzes with Melius. Your line is open.\nBen Reitzes\nManaging Director and Head of Technology Research\n\n0:42:05\n\nHey, thanks a lot.\nBen Reitzes\nManaging Director and Head of Technology Research\n\n0:42:09\n\nJensen, I wanted to ask you about cash. Speaking of half a trillion, you may generate about half a trillion in free cash flow over the next couple of years. What are your plans for that cash? How much goes to buyback versus investing in the ecosystem? How do you look at investing in the ecosystem? I think there's just a lot of confusion out there about how these deals work and your criteria for doing those, like the Anthropic, the OpenAIs, etc. Thanks a lot.\nJensen Huang\nPresident and CEO\n\n0:42:38",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 33
  },
  {
    "text": "hink there's just a lot of confusion out there about how these deals work and your criteria for doing those, like the Anthropic, the OpenAIs, etc. Thanks a lot.\nJensen Huang\nPresident and CEO\n\n0:42:38\n\nYeah, appreciate the question. Of course, using cash to fund our growth, no company has grown at the scale that we're talking about and have the connection and the depth and the breadth of supply chain that NVIDIA has.\nJensen Huang\nPresident and CEO\n\n0:43:00\n\nThe reason why our entire customer base can rely on us is because we've secured a really resilient supply chain, and we have the balance sheet to support them. When we make purchases, our suppliers can take it to the bank. When we make forecasts and we plan with them, they take us seriously because of our balance sheet. We're not making up the offtake. We know what our offtake is. Because they've been planning with us for so many years, our reputation and our credibility is incredible. It takes really strong balance sheet to do that, to support the level of growth and the rate of growth and the magnitude associated with that. That's number one. The second thing, of course, we're going to continue to do stock buybacks. We're going to continue to do that.\nJensen Huang\nPresident and CEO\n\n0:44:00",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 34
  },
  {
    "text": "agnitude associated with that. That's number one. The second thing, of course, we're going to continue to do stock buybacks. We're going to continue to do that.\nJensen Huang\nPresident and CEO\n\n0:44:00\n\nWith respect to the investments, this is really, really important work that we do. All of the investments that we've done so far, well, all the period, is associated with expanding the reach of CUDA, expanding the ecosystem. If you look at the work, the investments that we did with OpenAI, it's, of course, that relationship we've had since 2016. I delivered the first AI supercomputer ever made to OpenAI. We have had a close and wonderful relationship with OpenAI since then. Everything that OpenAI does runs on NVIDIA today. All the clouds that they deploy in, whether it's training and inference, runs NVIDIA, and we love working with them. The partnership that we have with them is one so that we could work even deeper from a technical perspective so that we could support their accelerated growth. This is a company that's growing incredibly fast.\nJensen Huang\nPresident and CEO\n\n0:45:02",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 35
  },
  {
    "text": "that we could work even deeper from a technical perspective so that we could support their accelerated growth. This is a company that's growing incredibly fast.\nJensen Huang\nPresident and CEO\n\n0:45:02\n\nDo not just look at what is said in the press. Look at all the ecosystem partners and all the developers that are connected to OpenAI. They are all driving consumption of it. The quality of the AI that is being produced is a huge step up since a year ago. The quality of response is extraordinary. We invest in OpenAI for a deep partnership in co-development to expand our ecosystem and to support their growth. Of course, rather than giving up a share of our company, we get a share of their company. We invested in them in one of the most consequential once-in-a-generation companies, once-in-a-generation company that we have a share of. I fully expect that investment to translate to extraordinary returns. Now, in the case of Anthropic, this is the first time that Anthropic will be on NVIDIA's architecture.\nJensen Huang\nPresident and CEO\n\n0:45:55",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 36
  },
  {
    "text": "ct that investment to translate to extraordinary returns. Now, in the case of Anthropic, this is the first time that Anthropic will be on NVIDIA's architecture.\nJensen Huang\nPresident and CEO\n\n0:45:55\n\nThe first time Anthropic will be on NVIDIA's architecture is the second most successful AI in the world in terms of total number of users. In enterprise, they're doing incredibly well. Claude Code is doing incredibly well. Claude is doing incredibly well all over the world's enterprise. Now we have the opportunity to have a deep partnership with them and bringing Claude onto the NVIDIA platform. What do we have now? NVIDIA's architecture, taking a step back, NVIDIA's architecture, NVIDIA's platform is the singular platform in the world that runs every AI model. We run OpenAI. We run Anthropic. We run xAI because of our deep partnership with Elon and xAI. We were able to bring that opportunity to Saudi Arabia, to the KSA, so that Humane could also be hosting opportunity for xAI. We run xAI. We run Gemini. We run Thinking Machines.\nJensen Huang\nPresident and CEO\n\n0:47:04",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 37
  },
  {
    "text": "ing that opportunity to Saudi Arabia, to the KSA, so that Humane could also be hosting opportunity for xAI. We run xAI. We run Gemini. We run Thinking Machines.\nJensen Huang\nPresident and CEO\n\n0:47:04\n\nLet's see, what else do we run? We run them all. Not to mention, we run the science models, the biology models, DNA models, gene models, chemical models, and all the different fields around the world. It's not just cognitive AI that the world uses. AI is impacting every single industry. We have the ability, through the ecosystem investments that we make, to partner with, deeply partner on a technical basis with some of the best companies, most brilliant companies in the world. We are expanding the reach of our ecosystem, and we're getting a share and investment in what will be a very successful company, oftentimes once-in-a-generation company. That's our investment thesis.\nOperator\n\n0:47:52\n\nThe next question comes from Jim Schneider with Goldman Sachs. Your line is open.\nJim Schneider\nSenior Equity Analyst\n\n0:48:02\n\nGood afternoon. Thanks for taking my question.\nJim Schneider\nSenior Equity Analyst\n\n0:48:06\n\nIn the past, you've talked about roughly 40% of your shipments tied to AI inference. I'm wondering, as you look forward into next year, where do you expect that percentage could go in, say, a year's time? Can you maybe address the Rubin CPX product you expect to introduce next year and contextualize that? How big of the overall TAM you expect that can take and maybe talk about some of the target customer applications for that specific product? Thank you.\nJensen Huang\nPresident and CEO\n\n0:48:34",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 38
  },
  {
    "text": "ze that? How big of the overall TAM you expect that can take and maybe talk about some of the target customer applications for that specific product? Thank you.\nJensen Huang\nPresident and CEO\n\n0:48:34\n\nCPX is designed for long-context type of workload generation. Long-context, basically, before you start generating answers, you have to read a lot, basically long-context. It could be a bunch of PDFs. It could be watching a bunch of videos, studying 3D images, so on and so forth. You have to absorb the context. CPX is designed for long-context type of workloads.\nJensen Huang\nPresident and CEO\n\n0:49:11\n\nIt's perf per dollars. Its perf per dollar is excellent. Its perf per watt is excellent. Which made me forget the first part of the question. Inferencing. Oh, inference. Yeah. There are three scaling laws that are scaling at the same time. The first scaling law called pretraining continues to be very effective. And the second is post-training. Post-training basically has found incredible algorithms for improving an AI's ability to break a problem down and solve a problem step by step. And post-training is scaling exponentially. Basically, the more compute you apply to a model, the smarter it is, the more intelligent it is. And then the third is inference. Inference, because of chain of thought, because of reasoning capabilities, AIs are essentially reading, thinking before it answers. The amount of computation necessary as a result of those three things has gone completely exponential.\nJensen Huang\nPresident and CEO\n\n0:50:23",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 39
  },
  {
    "text": " AIs are essentially reading, thinking before it answers. The amount of computation necessary as a result of those three things has gone completely exponential.\nJensen Huang\nPresident and CEO\n\n0:50:23\n\nI think that it's hard to know exactly what the percentage will be at any given point in time and who. Of course, our hope is that inference is a very large part of the market. If inference is large, then what it suggests is that people are using it in more applications, and they're using it more frequently. We should all hope for inference to be very large. This is where Grace Blackwell is just an order of magnitude better, more advanced than anything in the world. The second best platform is H200. It's very clear now that GB300, GB200, and GB300, because of NVLink 72, the scale-up network that we have achieved, and you saw and Colette talked about in the semi-analysis benchmark, it's the largest single inference benchmark ever done.\nJensen Huang\nPresident and CEO\n\n0:51:20",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 40
  },
  {
    "text": "le-up network that we have achieved, and you saw and Colette talked about in the semi-analysis benchmark, it's the largest single inference benchmark ever done.\nJensen Huang\nPresident and CEO\n\n0:51:20\n\nGB200, NVLink 72, is 10 times, 10-15 times higher performance. That is a big step up. It is going to take a long time before somebody is able to take that on. Our leadership there is surely multi-year. Yeah. I think I am hoping that inference becomes a very big deal. Our leadership in inference is extraordinary. The next question comes from Timothy Arcury with UBS. Your line is open. Thanks a lot. Jensen, many of your customers are pursuing behind-the-meter power. What is the single biggest bottleneck that worries you that could constrain your growth? Is it power, or maybe it is financing, or maybe it is something else like memory or even foundry? Thanks a lot. These are all issues, and they are all constraints.\nJensen Huang\nPresident and CEO\n\n0:52:15",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 41
  },
  {
    "text": "it power, or maybe it is financing, or maybe it is something else like memory or even foundry? Thanks a lot. These are all issues, and they are all constraints.\nJensen Huang\nPresident and CEO\n\n0:52:15\n\nThe reason for that, when you're growing at the rate that we are and the scale that we are, how could anything be easy? What NVIDIA is doing, obviously, has never been done before. We have created a whole new industry. On the one hand, we are transitioning computing from general-purpose and classical or traditional computing to accelerated computing and AI. That's on the one hand. On the other hand, we created a whole new industry called AI factories. The idea that in order for software to run, you need these factories to generate it, generate every single token instead of retrieving information that was pre-created. I think this whole transition requires extraordinary scale. All the way from the supply chain, of course, the supply chain, we have much better visibility and control over it because, obviously, we're incredibly good at managing our supply chain.\nJensen Huang\nPresident and CEO\n\n0:53:17",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 42
  },
  {
    "text": "y chain, of course, the supply chain, we have much better visibility and control over it because, obviously, we're incredibly good at managing our supply chain.\nJensen Huang\nPresident and CEO\n\n0:53:17\n\nWe have great partners that we've worked with for 33 years. The supply chain part of it, we're quite confident. Now, looking down our supply chain, we've now established partnerships with so many players in land and power and shell and, of course, financing. None of these things are easy, but they're all tractable, and they're all solvable things. The most important thing that we have to do is do a good job planning. We plan up the supply chain, down the supply chain. We have established a whole lot of partners. We have a lot of routes to market. Very importantly, our architecture has to deliver the best value to the customers that we have. At this point, I'm very confident that NVIDIA's architecture is the best performance per TCO.\nJensen Huang\nPresident and CEO\n\n0:54:16\n\nIt is the best performance per watt, and therefore, for any amount of energy that is delivered, our architecture will drive the most revenues. I think the increasing rate of our success, I think that we're more successful this year at this point than we were last year at this point. The number of customers coming to us and the number of platforms coming to us after they've explored others is increasing, not decreasing. I think all of that is just all the things that I've been telling you over the years are really coming true and are becoming evident.\nOperator\n\n0:54:58\n\nThe next question comes from Stacey Raskin with Bernstein Research. Your line is open.\nStacy Rasgon\nSenior Analyst\n\n0:55:09",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 43
  },
  {
    "text": "he years are really coming true and are becoming evident.\nOperator\n\n0:54:58\n\nThe next question comes from Stacey Raskin with Bernstein Research. Your line is open.\nStacy Rasgon\nSenior Analyst\n\n0:55:09\n\nQuestions. Colette, I have some questions on margins. You said for next year, you're working to hold them in the mid-70s. I guess, first of all, what are the biggest cost increases?\nStacy Rasgon\nSenior Analyst\n\n0:55:26\n\nIs it just memory, or is it something else? What are you doing to work toward that? How much is cost optimizations versus pre-buys versus pricing? Also, how should we think about OpEx growth next year, given the revenues seem likely to grow materially from where we're running right now?\nColette Kress\nEVP and CFO\n\n0:55:44\n\nThanks, Stacey. Let me see if I can start with remembering where we were with the current fiscal year that we're in. Remember, earlier this year, we indicated that through cost improvements and mix that we would exit the year and our gross margins in the mid-70s. We've achieved that and getting ready to also execute that in Q4. Now it's time for us to communicate where are we working right now in terms of next year. Next year, there are input prices that are well-known in the industries that we need to work through.\nColette Kress\nEVP and CFO\n\n0:56:23",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 44
  },
  {
    "text": "mmunicate where are we working right now in terms of next year. Next year, there are input prices that are well-known in the industries that we need to work through.\nColette Kress\nEVP and CFO\n\n0:56:23\n\nOur systems are by no means very easy to work with. There are a tremendous amount of components, many different parts of it, as we think about that. We are taking all of that into account. We do believe, as we look at working again on cost improvements, cycle time, and mix, that we will work to try and hold at our gross margins in the mid-70%. That is our overall plan for gross margin. Your second question is around OpEx. Right now, our goal in terms of OpEx is to really make sure that we are innovating with our engineering teams, with all of our business teams to create more and more systems for this market. As you know, right now, we have a new architecture coming out. That means they are quite busy in order to meet that goal.\nColette Kress\nEVP and CFO\n\n0:57:17\n\nWe're going to continue to see our investments on innovating more and more, both our software, both our systems, and our hardware to do so. I'll leave it turned to Jensen if he wants to add any couple more comments.\nJensen Huang\nPresident and CEO\n\n0:57:29",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 45
  },
  {
    "text": "ating more and more, both our software, both our systems, and our hardware to do so. I'll leave it turned to Jensen if he wants to add any couple more comments.\nJensen Huang\nPresident and CEO\n\n0:57:29\n\nYeah. That's spot on. I think the only thing that I would add is remember that we plan, we forecast, we plan, and we negotiate with our supply chain well in advance. Our supply chain has known for quite a long time our requirements. And they've known for quite a long time our demand. We've been working with them and negotiating with them for quite a long time. I think the recent surge, obviously, quite significant. Remember, our supply chain has been working with us for a very long time.\nJensen Huang\nPresident and CEO\n\n0:58:05\n\nIn many cases, we've secured a lot of supply for ourselves because, obviously, they're working with the largest company in the world in doing so. We've also been working closely with them on the financial aspects of it and securing forecasts and plans and so on and so forth. I think all of that has worked out well for us.\nOperator\n\n0:58:25\n\nYour final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\nAaron Rakers\nWall Street Analyst\n\n0:58:35\n\nYeah. Thanks for taking the question. Jensen, the question for you, as you think about the Anthropic deal that was announced and just the overall breadth of your customers, I'm curious if your thoughts around the role that AI ASICs or dedicated XPUs play in these architecture buildouts has changed at all. Have you seen?\nAaron Rakers\nWall Street Analyst\n\n0:58:58",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 46
  },
  {
    "text": "ustomers, I'm curious if your thoughts around the role that AI ASICs or dedicated XPUs play in these architecture buildouts has changed at all. Have you seen?\nAaron Rakers\nWall Street Analyst\n\n0:58:58\n\nI think you've been fairly adamant in the past that some of these programs never really see deployments. I'm curious if we're at a point where maybe that's even changed more in favor of just GPU architecture. Thank you.\nJensen Huang\nPresident and CEO\n\n0:59:10\n\nThank you very much. I really appreciate the question. First of all, you're not competing against teams. Excuse me. Again, as a company, you're competing against teams. There just aren't that many teams in the world who are extraordinary at building these incredibly complicated things. Back in the Hopper day and the Ampere days, we would build one GPU. That's the definition of an accelerated AI system. Today, we've got to build entire racks, entire three different types of switches: a scale-up, a scale-out, and a scale-across switch. It takes a lot more than one chip to build a compute node anymore.\nJensen Huang\nPresident and CEO\n\n0:59:55",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 47
  },
  {
    "text": "entire three different types of switches: a scale-up, a scale-out, and a scale-across switch. It takes a lot more than one chip to build a compute node anymore.\nJensen Huang\nPresident and CEO\n\n0:59:55\n\nEverything about that computing system, because AI needs to have memory, AI didn't used to have memory at all. Now it has to remember things. The amount of memory and context it has is gigantic. The memory architecture implication is incredible. The diversity of models from a mixture of experts to dense models to diffusion models to autoregressive, not to mention biological models that obey the laws of physics. The list of different types of models has exploded in the last several years. The challenge is the complexity of the problem is much higher. The diversity of AI models is incredibly large. This is where, if I will say, the five things that make us special, if you will. The first thing I would say that makes us special is that we accelerate every phase of that transition. That's the first phase.\nJensen Huang\nPresident and CEO\n\n1:00:54",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 48
  },
  {
    "text": "at make us special, if you will. The first thing I would say that makes us special is that we accelerate every phase of that transition. That's the first phase.\nJensen Huang\nPresident and CEO\n\n1:00:54\n\nThat CUDA allows us to have CUDA X for transitioning from general-purpose to accelerated computing. We are incredibly good at generative AI. We're incredibly good at agentic AI. Every single phase of that, every single layer of that transition, we are excellent at. You can invest in one architecture, use it across the board. You can use one architecture and not worry about the changes in the workload across those three phases. That's number one. Number two, we're excellent at every phase of AI. Everybody's always known that we're incredibly good at pretraining. We're obviously very good at post-training. And we're incredibly good, as it turns out, at inference because inference is really, really hard. How could thinking be easy? People think that inference is one shot, and therefore, it's easy. Anybody could approach the market that way.\nJensen Huang\nPresident and CEO\n\n1:01:49",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 49
  },
  {
    "text": " really, really hard. How could thinking be easy? People think that inference is one shot, and therefore, it's easy. Anybody could approach the market that way.\nJensen Huang\nPresident and CEO\n\n1:01:49\n\nBut it turns out to be the hardest of all because thinking, as it turns out, is quite hard. We're great at every phase of AI, the second thing. The third thing is we're now the only architecture in the world that runs every AI model, every frontier AI model. We run open-source AI models incredibly well. We run science models, biology models, robotics models. We run every single model. We're the only architecture in the world that can claim that. It doesn't matter whether you're autoregressive or diffusion-based. We run everything. We run it for every major platform, as I just mentioned. We run every model. The fourth thing I would say is that we're in every cloud. The reason why developers love us is because we're literally everywhere. We're in every cloud.\nJensen Huang\nPresident and CEO\n\n1:02:38",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 50
  },
  {
    "text": "model. The fourth thing I would say is that we're in every cloud. The reason why developers love us is because we're literally everywhere. We're in every cloud.\nJensen Huang\nPresident and CEO\n\n1:02:38\n\nWe're in every—we could even make you a little tiny cloud called DGX Spark. We're in every computer. We're everywhere, from cloud to on-prem to robotic systems, edge devices, PCs, you name it. One architecture, things just work. It's incredible. The last thing, and this is probably the most important thing, the fifth thing, is if you are a cloud service provider, if you're a new company like Humane, if you're a new company like CoreWeave or NSCALE or Nevius, or OCI for that matter, the reason why NVIDIA is the best platform for you is because our offtake is so diverse. We can help you with offtake. It's not about just putting a random ASIC into a data center. Where's the offtake coming from? Where's the diversity coming from? Where's the resilience coming from?\nJensen Huang\nPresident and CEO\n\n1:03:31\n\nThe versatility of the architecture coming from, the diversity of capability coming from. NVIDIA has such incredibly good offtake because our ecosystem is so large. So these five things, every phase of acceleration and transition, every phase of AI, every model, every cloud to on-prem, and of course, finally, it all leads to offtake.\nOperator\n\n1:03:51\n\nThank you. I will now turn the call to Toshiya Hari for closing remarks.\nToshiya Hari\nVP of Investor Relations\n\n1:04:00",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 51
  },
  {
    "text": " to on-prem, and of course, finally, it all leads to offtake.\nOperator\n\n1:03:51\n\nThank you. I will now turn the call to Toshiya Hari for closing remarks.\nToshiya Hari\nVP of Investor Relations\n\n1:04:00\n\nIn closing, please note we will be at the UBS Global Technology and AI Conference on December 2nd. And our earnings call to discuss the results of our fourth quarter of fiscal 2026 is scheduled for February 25th. Thank you for joining us today. Operator, please go ahead and close the call.\nOperator\n\n1:04:19\n\nThank you. This concludes today's conference call. You may now disconnect.\nCopyright © 2025 Yahoo. All rights reserved.\nWhat's trending\nDow Jones\nDAX Index\nNvidia\nTesla\nDJT\nTariffs\nExplore more\nMortgages\nCredit Cards\nSectors\nCrypto Heatmap\nFinancial News\nAbout\nData Disclaimer\nHelp\nFeedback\nSitemap\nLicensing\nWhat's New\nAbout Our Ads\nPremium Plans",
    "doc_path": "data\\raw\\NVDA\\NVDA_2025_Q3.txt",
    "chunk_id": 52
  }
]